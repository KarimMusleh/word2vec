Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.
ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.
Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. 
From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.

History
The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.
Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.
By the early 1960s, an experimental "learning machine" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively "trained" by a human operator/teacher to recognize patterns and equipped with a "goof" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.
Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E." This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".
Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.

Relationships to other fields
Artificial intelligence
As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.: 488 
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.: 25 
Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.

Data compression
Data mining
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).

Generalization
Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.

Statistics
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.
Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.
Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein "algorithmic model" means more or less the machine learning algorithms like Random Forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.

Statistical physics
Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.

Theory
A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.

Approaches
Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the "signal" or "feedback" available to the learning system:

Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.
Although each algorithm has advantages and limitations, no single algorithm works for all problems.

Supervised learning
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.
Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. 
Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.

Unsupervised learning
Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.  

Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.

Semi-supervised learning
Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.

Reinforcement learning
Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.

Dimensionality reduction
Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the "number of features". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.

Other types
Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.

Self-learning
Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 

in situation s perform action a
receive a consequence situation s'
compute emotion of being in the consequence situation v(s')
update crossbar memory  w'(a,s) = w(a,s) + v(s')
It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.

Feature learning
Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

Sparse dictionary learning
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.

Anomaly detection
In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.
Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.

Robot learning
Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).

Association rules
Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of "interestingness".
Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
  
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.
Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.

Models
A machine learning model is a type of mathematical model that, after being "trained" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions. By extension, the term "model" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.
Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.

Artificial neural networks
Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called "artificial neurons", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a "signal", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called "edges". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.

Decision trees
Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.

Support-vector machines
Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

Regression analysis
Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.

Bayesian networks
A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

Gaussian processes
A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.
Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.

Genetic algorithms
A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.

Belief functions
The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.

Training models
Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.

Federated learning
Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.

Applications
There are many applications for machine learning, including:

In 2006, the media-services provider Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.
Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.
Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires.

Limitations
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.
The "black box theory" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an "intelligence system" that could have a "substantial impact on an individual’s life" would not be considered acceptable unless it provided "a full and satisfactory explanation for the decisions" it makes.
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.
Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.

Bias
Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.
Language models learned from data have been shown to contain human-like biases. In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged "black defendants high risk twice as often as white defendants." In 2015, Google Photos would often tag black people as gorillas, and in 2018, this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.
Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that "[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility."

Explainability
Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the "black box" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.

Overfitting
Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.

Other limitations and vulnerabilities
Learners can also disappoint by "learning the wrong lesson". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in "adversarial" images that the system misclassifies.
Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.
Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories "spam" and well-visible "not spam" of posts) machine learning models that are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.

Model assessments
Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.
In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).

Ethics
Machine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in "disproportionately high levels of over-policing in low-income and minority communities" after being trained with historical crime data.
While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, "female faculty merely make up 16.1%" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of "new U.S. resident AI PhD graduates," 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.
AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.
Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.

Hardware
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.

Neuromorphic/Physical Neural Networks
A physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. "Physical" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.

Embedded Machine Learning
Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more. Pruning, Quantization, Knowledge Distillation, Low-Rank Factorization, Network Architecture Search (NAS) & Parameter Sharing are few of the techniques used for optimization of machine learning models.

Software
Software suites containing a variety of machine learning algorithms include the following:

Free and open-source software
Proprietary software with free and open-source editions
KNIME
RapidMiner

Proprietary software
Journals
Journal of Machine Learning Research
Machine Learning
Nature Machine Intelligence
Neural Computation
IEEE Transactions on Pattern Analysis and Machine Intelligence

Conferences
AAAI Conference on Artificial Intelligence
Association for Computational Linguistics (ACL)
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)
International Conference on Machine Learning (ICML)
International Conference on Learning Representations (ICLR)
International Conference on Intelligent Robots and Systems (IROS)
Conference on Knowledge Discovery and Data Mining (KDD)
Conference on Neural Information Processing Systems (NeurIPS)

See also
Automated machine learning – Process of automating the application of machine learning
Big data – Extremely large or complex datasets
Deep learning — branch of ML concerned with artificial neural networks
Differentiable programming – Programming paradigm
List of important publications in machine learning
List of datasets for machine-learning research
M-theory (learning framework)

References
Sources
Domingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.
Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.

Further reading
External links

 Quotations related to Machine learning at Wikiquote
International Machine Learning Society
mloss is an academic database of open-source machine learning software.
The AAAI Conference on Artificial Intelligence (AAAI) is a leading international academic conference in artificial intelligence held annually. It ranks 4th in terms of H5 Index in Google Scholar's list of top AI publications, after ICLR, NeurIPS, and ICML. It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.
During AAAI-20 conference, AI pioneers and 2018 Turing Award winners Yann LeCun and Yoshua Bengio, among eight other researchers, were honored as the AAAI 2020 Fellows.
Along with other conferences such as NeurIPS and ICML, AAAI uses an artificial-intelligence algorithm to assign papers to reviewers.

Locations
AAAI-2025 Pennsylvania Convention Center, Philadelphia, Pennsylvania, United States
 AAAI-2024 Vancouver Convention Centre, Vancouver, British Columbia, Canada
 AAAI-2023 Washington Convention Center, Washington, D.C., United States
 AAAI-2022 Virtual Conference
 AAAI-2021 Virtual Conference
 AAAI-2020 Hilton New York Midtown, New York, New York, United States
 AAAI-2019 Hilton Hawaiian Village, Honolulu, Hawaii, United States
 AAAI-2018 Hilton New Orleans Riverside, New Orleans, Louisiana, United States
 AAAI-2017 San Francisco, California, United States
 AAAI-2016 Phoenix, Arizona, United States
 AAAI-2015 Austin, Texas, United States
 AAAI-2014 Québec Convention Center, Québec City, Québec, Canada
 AAAI-2013 Bellevue, Washington, United States
 AAAI-2012 Toronto, Ontario, Canada
 AAAI-2011 San Francisco, California, United States
 AAAI-2010 Westin Peachtree Plaza, Atlanta, Georgia, United States
 AAAI-2008 Chicago, Illinois, United States
 AAAI-2007 Toronto, Ontario, Canada
 AAAI-2006 Boston, Massachusetts, United States
 AAAI-2005 Pittsburgh, Pennsylvania, United States
 AAAI-2004 San Jose, California, United States
 AAAI-2002 Shaw conference center in Edmonton, Alberta, Canada
 AAAI-2000 Austin, Texas, United States
 AAAI-1999 Orlando, Florida, United States
 AAAI-1998 Madison, Wisconsin, United States
 AAAI-1997 Providence, Rhode Island, United States
 AAAI-1996 Portland, Oregon, United States
 AAAI-1994 Seattle, Washington, United States
 AAAI-1993 Washington Convention Center, Washington, D.C., United States
 AAAI-1992 San Jose Convention Center, San Jose, California, United States
 AAAI-1991 Anaheim Convention Center, Anaheim, California, United States
 AAAI-1990 Boston, Massachusetts, United States
 AAAI-1988 Saint Paul, Minnesota, United States
 AAAI-1987 Seattle, Washington, United States
 AAAI-1986 Philadelphia, Pennsylvania, United States
 AAAI-1984 University of Texas, Austin, Texas, United States
 AAAI-1983 Washington, D.C., United States
 AAAI-1982 Carnegie Mellon University and the University of Pittsburgh, Pittsburgh, Pennsylvania, United States
 AAAI-1980 Stanford, California, United States

See also
ICML
ICLR
Journal of Machine Learning Research
Machine Learning (journal)
NeurIPS

References
External links
Official website
The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.

History
The system has gone through seven revisions, the first version being published in 1964, and revised versions appearing in 1982, 1983, 1987, 1991, 1998, and the now current version in 2012.

Structure
It is hierarchically structured in four levels. For example, one branch of the hierarchy contains:

Computing methodologies
Artificial intelligence
Knowledge representation and reasoning
Ontology engineering

See also
Computer Science Ontology
Physics and Astronomy Classification Scheme
arXiv, a preprint server allowing submitted papers to be classified using the ACM CCS
Physics Subject Headings

References
Coulter, Neal (1997), "ACM's computing classification system reflects changing times", Communications of the ACM, 40 (12), New York, NY, USA: ACM: 111–112, doi:10.1145/265563.265579, S2CID 42548816.
Coulter, Neal (chair); French, James; Glinert, Ephraim; Horton, Thomas; Mead, Nancy; Ralston, Anthony; Rada, Roy; Rodkin, Craig; Rous, Bernard; Tucker, Allen; Wegner, Peter; Weiss, Eric; Wierzbicki, Carol (January 21, 1998), "Computing Classification System 1998: Current Status and Future Maintenance Report of the CCS Update Committee" (PDF), Computing Reviews, New York, NY, USA: ACM: 1–5.
Mirkin, Boris; Nascimento, Susana; Pereira, Luis Moniz (2008), "Representing a Computer Science Research Organization on the ACM Computing Classification System", in Eklund, Peter; Haemmerlé, Ollivier (eds.), Supplementary Proceedings of the 16th International Conference on Conceptual Structures (ICCS-2008) (PDF), CEUR Workshop Proceedings, vol. 354, RWTH Aachen University, pp. 57–65.

External links
dl.acm.org/ccs is the homepage of the system, including links to four complete versions of the system:
the 1964 version Archived 2016-12-01 at the Wayback Machine
the 1991 version Archived 2017-09-21 at the Wayback Machine
the 1998 version
the current 2012 version.
The ACM Computing Research Repository uses a classification scheme that is much coarser than the ACM subject classification, and does not cover all areas of CS, but is intended to better cover active areas of research. In addition, papers in this repository are classified according to the ACM subject classification.
ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.
According to the Journal Citation Reports, the journal has a 2023 impact factor of 23.8. In a 2008 ranking of computer science journals, ACM Computing Surveys received the highest rank "A*".

See also
ACM Computing Reviews

References
External links
Official website
ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function. The weights and biases were implemented by rheostats (as seen in the "knobby ADALINE"), and later, memistors.
The difference between Adaline and the standard (McCulloch–Pitts) perceptron is in how they learn. Adaline unit weights are adjusted to match a teacher signal, before applying the Heaviside function (see figure), but the standard perceptron unit weights are adjusted to match the correct output, after applying the Heaviside function.
A multilayer network of ADALINE units is a MADALINE.

Definition
Adaline is a single layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables as:

  
    
      
        x
      
    
    {\displaystyle x}
  
 is the input vector

  
    
      
        w
      
    
    {\displaystyle w}
  
 is the weight vector

  
    
      
        n
      
    
    {\displaystyle n}
  
 is the number of inputs

  
    
      
        θ
      
    
    {\displaystyle \theta }
  
 is some constant

  
    
      
        y
      
    
    {\displaystyle y}
  
 is the output of the model
then we find that the output is 
  
    
      
        y
        =
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          x
          
            j
          
        
        
          w
          
            j
          
        
        +
        θ
      
    
    {\displaystyle y=\sum _{j=1}^{n}x_{j}w_{j}+\theta }
  
.  If we further assume that

  
    
      
        
          x
          
            0
          
        
        =
        1
      
    
    {\displaystyle x_{0}=1}
  

  
    
      
        
          w
          
            0
          
        
        =
        θ
      
    
    {\displaystyle w_{0}=\theta }
  

then the output further reduces to: 
  
    
      
        y
        =
        
          ∑
          
            j
            =
            0
          
          
            n
          
        
        
          x
          
            j
          
        
        
          w
          
            j
          
        
      
    
    {\displaystyle y=\sum _{j=0}^{n}x_{j}w_{j}}

Learning rule
The learning rule used by ADALINE is the LMS ("least mean squares") algorithm, a special case of gradient descent. 
Define the following notations:

  
    
      
        η
      
    
    {\displaystyle \eta }
  
 is the learning rate (some positive constant)

  
    
      
        y
      
    
    {\displaystyle y}
  
 is the output of the model

  
    
      
        o
      
    
    {\displaystyle o}
  
 is the target (desired) output

  
    
      
        E
        =
        (
        o
        −
        y
        
          )
          
            2
          
        
      
    
    {\displaystyle E=(o-y)^{2}}
  
 is the square of the error.

The LMS algorithm updates the weights by 
  
    
      
        w
        ←
        w
        +
        η
        (
        o
        −
        y
        )
        x
        .
      
    
    {\displaystyle w\leftarrow w+\eta (o-y)x.}
  

This update rule minimizes 
  
    
      
        E
      
    
    {\displaystyle E}
  
, the square of the error, and is in fact the stochastic gradient descent update for linear regression.

MADALINE
MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors. Three different training algorithms for MADALINE networks, which cannot be learned using backpropagation because the sign function is not differentiable, have been suggested, called Rule I, Rule II and Rule III.  
Despite many attempts, they never succeeded in training more than a single layer of weights in a MADALINE. This was until Widrow saw the backpropagation algorithm in a 1985 Snowbird conference. 
MADALINE Rule 1 (MRI) - The first of these dates back to 1962. It consists of two layers. The first layer is made of ADALINE units. Let the output of the i-th ADALINE unit be 
  
    
      
        
          o
          
            i
          
        
      
    
    {\displaystyle o_{i}}
  
. The second layer has two units. One is a majority-voting unit: it takes in all 
  
    
      
        
          o
          
            i
          
        
      
    
    {\displaystyle o_{i}}
  
, and if there are more positives than negatives, then the unit outputs +1, and vice versa. Another is a "job assigner". Suppose the desired output is different from the majority-voted output, say the desired output is -1, then the job assigner calculates the minimal number of ADALINE units that must change their outputs from positive to negative, then picks those ADALINE units that are closest to being negative, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of "minimal disturbance principle".
The largest MADALINE machine built had 1000 weights, each implemented by a memistor. It was built in 1963 and used MRI for learning.
Some MADALINE machines were demonstrated to perform inverted pendulum balancing, weather prediction, speech recognition, etc. 
MADALINE Rule 2 (MRII) - The second training algorithm improved on Rule I and was described in 1988. The Rule II training algorithm is based on a principle called "minimal disturbance". It proceeds by looping over training examples, then for each example, it:

finds the hidden layer unit (ADALINE classifier) with the lowest confidence in its prediction,
tentatively flips the sign of the unit,
accepts or rejects the change based on whether the network's error is reduced,
stops when the error is zero.
MADALINE Rule 3 - The third "Rule" applied to a modified network with sigmoid activations instead of signum; it was later found to be equivalent to backpropagation. 
Additionally, when flipping single units' signs does not drive the error to zero for a particular example, the training algorithm starts flipping pairs of units' signs, then triples of units, etc.

See also
Multilayer perceptron

References
External links
widrowlms (2012-07-29). The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE. Retrieved 2024-08-17 – via YouTube. Widrow demonstrating both a working knobby ADALINE machine and a memistor ADALINE machine.
"Delta Learning Rule: ADALINE". Artificial Neural Networks. Universidad Politécnica de Madrid. Archived from the original on 2002-06-15.
"Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training". Implementation of the ADALINE algorithm with memristors in analog computing.
AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence.
It combines Solomonoff induction with sequential decision theory.
AIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutter's 2005 book Universal Artificial Intelligence.
AIXI is a reinforcement learning (RL) agent. It maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis (or environment). In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.

Definition
According to Hutter, the word "AIXI" can have several interpretations. AIXI can stand for AI based on Solomonoff's distribution, denoted by 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
  
 (which is the Greek letter xi), or e.g. it can stand for AI "crossed" (X) with induction (I). There are other interpretations.
AIXI is a reinforcement learning agent that interacts with some stochastic and unknown but computable environment 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
. The interaction proceeds in time steps, from 
  
    
      
        t
        =
        1
      
    
    {\displaystyle t=1}
  
 to 
  
    
      
        t
        =
        m
      
    
    {\displaystyle t=m}
  
, where 
  
    
      
        m
        ∈
        
          N
        
      
    
    {\displaystyle m\in \mathbb {N} }
  
 is the lifespan of the AIXI agent. At time step t, the agent chooses an action 
  
    
      
        
          a
          
            t
          
        
        ∈
        
          
            A
          
        
      
    
    {\displaystyle a_{t}\in {\mathcal {A}}}
  
 (e.g. a limb movement) and executes it in the environment, and the environment responds with a "percept" 
  
    
      
        
          e
          
            t
          
        
        ∈
        
          
            E
          
        
        =
        
          
            O
          
        
        ×
        
          R
        
      
    
    {\displaystyle e_{t}\in {\mathcal {E}}={\mathcal {O}}\times \mathbb {R} }
  
, which consists of an "observation" 
  
    
      
        
          o
          
            t
          
        
        ∈
        
          
            O
          
        
      
    
    {\displaystyle o_{t}\in {\mathcal {O}}}
  
 (e.g., a camera image) and a reward 
  
    
      
        
          r
          
            t
          
        
        ∈
        
          R
        
      
    
    {\displaystyle r_{t}\in \mathbb {R} }
  
, distributed according to the conditional probability 
  
    
      
        μ
        (
        
          o
          
            t
          
        
        
          r
          
            t
          
        
        
          |
        
        
          a
          
            1
          
        
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        .
        .
        .
        
          a
          
            t
            −
            1
          
        
        
          o
          
            t
            −
            1
          
        
        
          r
          
            t
            −
            1
          
        
        
          a
          
            t
          
        
        )
      
    
    {\displaystyle \mu (o_{t}r_{t}|a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}a_{t})}
  
, where 
  
    
      
        
          a
          
            1
          
        
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        .
        .
        .
        
          a
          
            t
            −
            1
          
        
        
          o
          
            t
            −
            1
          
        
        
          r
          
            t
            −
            1
          
        
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}a_{t}}
  
 is the "history" of actions, observations and rewards. The environment 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 is thus mathematically represented as a probability distribution over "percepts" (observations and rewards) which depend on the full history, so there is no Markov assumption (as opposed to other RL algorithms). Note again that this probability distribution is unknown to the AIXI agent. Furthermore, note again that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 is computable, that is, the observations and rewards received by the agent from the environment 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 can be computed by some program (which runs on a Turing machine), given the past actions of the AIXI agent.
The only goal of the AIXI agent is to maximise 
  
    
      
        
          ∑
          
            t
            =
            1
          
          
            m
          
        
        
          r
          
            t
          
        
      
    
    {\displaystyle \sum _{t=1}^{m}r_{t}}
  
, that is, the sum of rewards from time step 1 to m.
The AIXI agent is associated with a stochastic policy 
  
    
      
        π
        :
        (
        
          
            A
          
        
        ×
        
          
            E
          
        
        
          )
          
            ∗
          
        
        →
        
          
            A
          
        
      
    
    {\displaystyle \pi :({\mathcal {A}}\times {\mathcal {E}})^{*}\rightarrow {\mathcal {A}}}
  
, which is the function it uses to choose actions at every time step, where 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
  
 is the space of all possible actions that AIXI can take and 
  
    
      
        
          
            E
          
        
      
    
    {\displaystyle {\mathcal {E}}}
  
 is the space of all possible "percepts" that can be produced by the environment. The environment (or probability distribution) 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 can also be thought of as a stochastic policy (which is a function): 
  
    
      
        μ
        :
        (
        
          
            A
          
        
        ×
        
          
            E
          
        
        
          )
          
            ∗
          
        
        ×
        
          
            A
          
        
        →
        
          
            E
          
        
      
    
    {\displaystyle \mu :({\mathcal {A}}\times {\mathcal {E}})^{*}\times {\mathcal {A}}\rightarrow {\mathcal {E}}}
  
, where the 
  
    
      
        ∗
      
    
    {\displaystyle *}
  
 is the Kleene star operation.
In general, at time step 
  
    
      
        t
      
    
    {\displaystyle t}
  
 (which ranges from 1 to m), AIXI, having previously executed actions 
  
    
      
        
          a
          
            1
          
        
        …
        
          a
          
            t
            −
            1
          
        
      
    
    {\displaystyle a_{1}\dots a_{t-1}}
  
 (which is often abbreviated in the literature as 
  
    
      
        
          a
          
            <
            t
          
        
      
    
    {\displaystyle a_{<t}}
  
) and having observed the history of percepts 
  
    
      
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        .
        .
        .
        
          o
          
            t
            −
            1
          
        
        
          r
          
            t
            −
            1
          
        
      
    
    {\displaystyle o_{1}r_{1}...o_{t-1}r_{t-1}}
  
 (which can be abbreviated as 
  
    
      
        
          e
          
            <
            t
          
        
      
    
    {\displaystyle e_{<t}}
  
), chooses and executes in the environment the action, 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
  
, defined as follows:

  
    
      
        
          a
          
            t
          
        
        :=
        arg
        ⁡
        
          max
          
            
              a
              
                t
              
            
          
        
        
          ∑
          
            
              o
              
                t
              
            
            
              r
              
                t
              
            
          
        
        …
        
          max
          
            
              a
              
                m
              
            
          
        
        
          ∑
          
            
              o
              
                m
              
            
            
              r
              
                m
              
            
          
        
        [
        
          r
          
            t
          
        
        +
        …
        +
        
          r
          
            m
          
        
        ]
        
          ∑
          
            q
            :
            
            U
            (
            q
            ,
            
              a
              
                1
              
            
            …
            
              a
              
                m
              
            
            )
            =
            
              o
              
                1
              
            
            
              r
              
                1
              
            
            …
            
              o
              
                m
              
            
            
              r
              
                m
              
            
          
        
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
      
    
    {\displaystyle a_{t}:=\arg \max _{a_{t}}\sum _{o_{t}r_{t}}\ldots \max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}}
  

or, using parentheses, to disambiguate the precedences

  
    
      
        
          a
          
            t
          
        
        :=
        arg
        ⁡
        
          max
          
            
              a
              
                t
              
            
          
        
        
          (
          
            
              ∑
              
                
                  o
                  
                    t
                  
                
                
                  r
                  
                    t
                  
                
              
            
            …
            
              (
              
                
                  max
                  
                    
                      a
                      
                        m
                      
                    
                  
                
                
                  ∑
                  
                    
                      o
                      
                        m
                      
                    
                    
                      r
                      
                        m
                      
                    
                  
                
                [
                
                  r
                  
                    t
                  
                
                +
                …
                +
                
                  r
                  
                    m
                  
                
                ]
                
                  (
                  
                    
                      ∑
                      
                        q
                        :
                        
                        U
                        (
                        q
                        ,
                        
                          a
                          
                            1
                          
                        
                        …
                        
                          a
                          
                            m
                          
                        
                        )
                        =
                        
                          o
                          
                            1
                          
                        
                        
                          r
                          
                            1
                          
                        
                        …
                        
                          o
                          
                            m
                          
                        
                        
                          r
                          
                            m
                          
                        
                      
                    
                    
                      2
                      
                        −
                        
                          
                            length
                          
                        
                        (
                        q
                        )
                      
                    
                  
                  )
                
              
              )
            
          
          )
        
      
    
    {\displaystyle a_{t}:=\arg \max _{a_{t}}\left(\sum _{o_{t}r_{t}}\ldots \left(\max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\left(\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}\right)\right)\right)}
  

Intuitively, in the definition above, AIXI considers the sum of the total reward over all possible "futures" up to 
  
    
      
        m
        −
        t
      
    
    {\displaystyle m-t}
  
 time steps ahead (that is, from 
  
    
      
        t
      
    
    {\displaystyle t}
  
 to 
  
    
      
        m
      
    
    {\displaystyle m}
  
), weighs each of them by the complexity of programs 
  
    
      
        q
      
    
    {\displaystyle q}
  
 (that is, by 
  
    
      
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
      
    
    {\displaystyle 2^{-{\textrm {length}}(q)}}
  
) consistent with the agent's past (that is, the previously executed actions, 
  
    
      
        
          a
          
            <
            t
          
        
      
    
    {\displaystyle a_{<t}}
  
, and received percepts, 
  
    
      
        
          e
          
            <
            t
          
        
      
    
    {\displaystyle e_{<t}}
  
) that can generate that future, and then picks the action that maximises expected future rewards.
Let us break this definition down in order to attempt to fully understand it.

  
    
      
        
          o
          
            t
          
        
        
          r
          
            t
          
        
      
    
    {\displaystyle o_{t}r_{t}}
  
 is the "percept" (which consists of the observation 
  
    
      
        
          o
          
            t
          
        
      
    
    {\displaystyle o_{t}}
  
 and reward 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  
) received by the AIXI agent at time step 
  
    
      
        t
      
    
    {\displaystyle t}
  
 from the environment (which is unknown and stochastic). Similarly, 
  
    
      
        
          o
          
            m
          
        
        
          r
          
            m
          
        
      
    
    {\displaystyle o_{m}r_{m}}
  
 is the percept received by AIXI at time step 
  
    
      
        m
      
    
    {\displaystyle m}
  
 (the last time step where AIXI is active).

  
    
      
        
          r
          
            t
          
        
        +
        …
        +
        
          r
          
            m
          
        
      
    
    {\displaystyle r_{t}+\ldots +r_{m}}
  
 is the sum of rewards from time step 
  
    
      
        t
      
    
    {\displaystyle t}
  
 to time step 
  
    
      
        m
      
    
    {\displaystyle m}
  
, so AIXI needs to look into the future to choose its action at time step 
  
    
      
        t
      
    
    {\displaystyle t}
  
.

  
    
      
        U
      
    
    {\displaystyle U}
  
 denotes a monotone universal Turing machine, and 
  
    
      
        q
      
    
    {\displaystyle q}
  
 ranges over all (deterministic) programs on the universal machine 
  
    
      
        U
      
    
    {\displaystyle U}
  
, which receives as input the program 
  
    
      
        q
      
    
    {\displaystyle q}
  
 and the sequence of actions 
  
    
      
        
          a
          
            1
          
        
        …
        
          a
          
            m
          
        
      
    
    {\displaystyle a_{1}\dots a_{m}}
  
 (that is, all actions), and produces the sequence of percepts 
  
    
      
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        …
        
          o
          
            m
          
        
        
          r
          
            m
          
        
      
    
    {\displaystyle o_{1}r_{1}\ldots o_{m}r_{m}}
  
. The universal Turing machine 
  
    
      
        U
      
    
    {\displaystyle U}
  
 is thus used to "simulate" or compute the environment responses or percepts, given the program 
  
    
      
        q
      
    
    {\displaystyle q}
  
 (which "models" the environment) and all actions of the AIXI agent: in this sense, the environment is "computable" (as stated above). Note that, in general, the program which "models" the current and actual environment (where AIXI needs to act) is unknown because the current environment is also unknown. 

  
    
      
        
          
            length
          
        
        (
        q
        )
      
    
    {\displaystyle {\textrm {length}}(q)}
  
 is the length of the program 
  
    
      
        q
      
    
    {\displaystyle q}
  
 (which is encoded as a string of bits). Note that 
  
    
      
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
        =
        
          
            1
            
              2
              
                
                  
                    length
                  
                
                (
                q
                )
              
            
          
        
      
    
    {\displaystyle 2^{-{\textrm {length}}(q)}={\frac {1}{2^{{\textrm {length}}(q)}}}}
  
. Hence, in the definition above, 
  
    
      
        
          ∑
          
            q
            :
            
            U
            (
            q
            ,
            
              a
              
                1
              
            
            …
            
              a
              
                m
              
            
            )
            =
            
              o
              
                1
              
            
            
              r
              
                1
              
            
            …
            
              o
              
                m
              
            
            
              r
              
                m
              
            
          
        
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
      
    
    {\displaystyle \sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}}
  
 should be interpreted as a mixture (in this case, a sum) over all computable environments (which are consistent with the agent's past), each weighted by its complexity 
  
    
      
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
      
    
    {\displaystyle 2^{-{\textrm {length}}(q)}}
  
. Note that 
  
    
      
        
          a
          
            1
          
        
        …
        
          a
          
            m
          
        
      
    
    {\displaystyle a_{1}\ldots a_{m}}
  
 can also be written as 
  
    
      
        
          a
          
            1
          
        
        …
        
          a
          
            t
            −
            1
          
        
        
          a
          
            t
          
        
        …
        
          a
          
            m
          
        
      
    
    {\displaystyle a_{1}\ldots a_{t-1}a_{t}\ldots a_{m}}
  
, and 
  
    
      
        
          a
          
            1
          
        
        …
        
          a
          
            t
            −
            1
          
        
        =
        
          a
          
            <
            t
          
        
      
    
    {\displaystyle a_{1}\ldots a_{t-1}=a_{<t}}
  
 is the sequence of actions already executed in the environment by the AIXI agent. Similarly, 
  
    
      
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        …
        
          o
          
            m
          
        
        
          r
          
            m
          
        
        =
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        …
        
          o
          
            t
            −
            1
          
        
        
          r
          
            t
            −
            1
          
        
        
          o
          
            t
          
        
        
          r
          
            t
          
        
        …
        
          o
          
            m
          
        
        
          r
          
            m
          
        
      
    
    {\displaystyle o_{1}r_{1}\ldots o_{m}r_{m}=o_{1}r_{1}\ldots o_{t-1}r_{t-1}o_{t}r_{t}\ldots o_{m}r_{m}}
  
, and 
  
    
      
        
          o
          
            1
          
        
        
          r
          
            1
          
        
        …
        
          o
          
            t
            −
            1
          
        
        
          r
          
            t
            −
            1
          
        
      
    
    {\displaystyle o_{1}r_{1}\ldots o_{t-1}r_{t-1}}
  
 is the sequence of percepts produced by the environment so far.
Let us now put all these components together in order to understand this equation or definition.
At time step t, AIXI chooses the action 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
  
 where the function 
  
    
      
        
          ∑
          
            
              o
              
                t
              
            
            
              r
              
                t
              
            
          
        
        …
        
          max
          
            
              a
              
                m
              
            
          
        
        
          ∑
          
            
              o
              
                m
              
            
            
              r
              
                m
              
            
          
        
        [
        
          r
          
            t
          
        
        +
        …
        +
        
          r
          
            m
          
        
        ]
        
          ∑
          
            q
            :
            
            U
            (
            q
            ,
            
              a
              
                1
              
            
            …
            
              a
              
                m
              
            
            )
            =
            
              o
              
                1
              
            
            
              r
              
                1
              
            
            …
            
              o
              
                m
              
            
            
              r
              
                m
              
            
          
        
        
          2
          
            −
            
              
                length
              
            
            (
            q
            )
          
        
      
    
    {\displaystyle \sum _{o_{t}r_{t}}\ldots \max _{a_{m}}\sum _{o_{m}r_{m}}[r_{t}+\ldots +r_{m}]\sum _{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-{\textrm {length}}(q)}}
  
 attains its maximum.

Parameters
The parameters to AIXI are the universal Turing machine U and the agent's lifetime m, which need to be chosen. The latter parameter can be removed by the use of discounting.

Optimality
AIXI's performance is measured by the expected total number of rewards it receives.
AIXI has been proven to be optimal in the following ways.

Pareto optimality: there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment.
Balanced Pareto optimality: like Pareto optimality, but considering a weighted sum of environments.
Self-optimizing: a policy p is called self-optimizing for an environment 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 if the performance of p approaches the theoretical maximum for 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 when the length of the agent's lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing.
It was later shown by Hutter and Jan Leike that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which they describe as undermining all previous optimality claims for AIXI.
However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable.

Computational aspects
Like Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXItl, which performs at least as well as the provably best time t and space l limited agent. Another approximation to AIXI with a restricted environment class is MC-AIXI (FAC-CTW) (which stands for Monte Carlo AIXI FAC-Context-Tree Weighting), which has had some success playing simple games such as partially observable Pac-Man.

See also
Gödel machine

References

"Universal Algorithmic Intelligence: A mathematical top->down approach", Marcus Hutter, arXiv:cs/0701125; also in Artificial General Intelligence, eds. B. Goertzel and C. Pennachin, Springer, 2007, ISBN 9783540237334, pp. 227–290, doi:10.1007/978-3-540-68677-4_8.
The AI boom, or AI spring, is an ongoing period of rapid progress in the field of artificial intelligence (AI) that started in the late 2010s before gaining international prominence in the early 2020s. Examples include protein folding prediction led by Google DeepMind and generative AI applications developed by OpenAI.

History
In 2012, a University of Toronto research team used artificial neural networks and deep learning techniques to lower the error rate below 25% for the first time during the ImageNet challenge for object recognition in computer vision. The event catalyzed the AI boom later that decade, when many alumni of the ImageNet challenge became leaders in the tech industry. In March 2016, AlphaGo beat Lee Sedol in a five-game match, marking the first time a computer Go program had beaten a 9-dan professional without handicap. This match led to significant increase in public interest in AI. The generative AI race began in earnest in 2016 or 2017 following the founding of OpenAI and earlier advances made in graphical processing units (GPUs), the amount and quality of training data, generative adversarial networks, diffusion models and transformer architectures. In 2018, the Artificial Intelligence Index, an initiative from Stanford University, reported a global explosion of commercial and research efforts in AI. Europe published the largest number of papers in the field that year, followed by China and North America. Technologies such as AlphaFold led to more accurate predictions of protein folding and improved the process of drug development. Economists and lawmakers began to discuss the potential impact of AI more frequently. By 2022, large language models (LLMs) saw increased usage in chatbot applications; text-to-image-models could generate images that appeared to be human-made; and speech synthesis software was able to replicate human speech efficiently.
According to metrics from 2017 to 2021, the United States outranks the rest of the world in terms of venture capital funding, the number of startups, and patents granted in AI. Scientists who have immigrated to the U.S. play an outsize role in the country's development of AI technology. Many of them were educated in China, prompting debates about national security concerns amid worsening relations between the two countries.
Experts have framed AI development as a competition for economic and geopolitical advantage between the United States and China. In 2021, an analyst for the Council on Foreign Relations outlined ways that the U.S. could maintain its position amid progress made by China. In 2023, an analyst at the Center for Strategic and International Studies advocated for the U.S. to use its dominance in AI technology to drive its foreign policy instead of relying on trade agreements.

Advances
Biomedical
There have been proposals to use AI to advance radical forms of human life extension.
The AlphaFold 2 score of more than 90 in CASP's global distance test (GDT) is considered a significant achievement in computational biology and great progress towards a decades-old grand challenge of biology. Nobel Prize winner and structural biologist Venki Ramakrishnan called the result "a stunning advance on the protein folding problem", adding that "It has occurred decades before many people in the field would have predicted."
The ability to predict protein structures accurately based on the constituent amino acid sequence is expected to accelerate drug discovery and enable a better understanding of diseases. It went on to note that the AI algorithm could "predict the shape of proteins to within the width of an atom."

Images and videos
Text-to-image models captured widespread public attention when OpenAI announced DALL-E, a transformer system, in January 2021. A successor capable of generating complex and realistic images, DALL-E 2, was unveiled in April 2022. An alternative text-to-image model, Midjourney, was released in July 2022. Another alternative, open-source model Stable Diffusion, released in August 2022.
Following other text-to-image models, language model-powered text-to-video platforms such as Runway, OpenAI's Sora, DAMO, Make-A-Video, Imagen Video and Phenaki can generate video from text as well as image prompts.

Language
GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. The tool has been credited with spurring and accelerating the A.I. boom following its release. An upgraded version called GPT-3.5 was used in ChatGPT, which later garnered attention for its detailed responses and articulate answers across many domains of knowledge. A new version called GPT-4 was released on March 14, 2023, and was used in the Microsoft Bing search engine. Other language models have been released, such as PaLM and Gemini by Google and LLaMA by Meta Platforms.
In January 2023, DeepL Write, an AI-based tool to improve monolingual texts, was released. In December 2023, Gemini, the latest model by Google, was unveiled, claiming to beat previous state-of-the-art-model GPT-4 on most benchmarks.

Music and voice
In 2016, Google DeepMind unveiled WaveNet, a deep learning network that produced English, Mandarin, and piano music. 
In 2020, the non-commercial freeware artificial intelligence web application 15.ai was released. 15.ai is credited with popularizing AI voice cloning in content creation, being the first publicly available AI vocal synthesis application and having had a significant impact in multiple Internet fandoms, most notably the My Little Pony: Friendship Is Magic and Team Fortress 2 fandoms.
ElevenLabs allowed users to upload voice samples and create audio that sounds similar to the samples. The company was criticized after controversial statements were generated based on the vocal styles of celebrities, public officials, and other famous individuals, raising concerns that the technology could make deepfakes even more convincing. An unofficial song created using the voices of musicians Drake and The Weeknd raised questions about the ethics and legality of similar software.

Impact
The AI boom may have a profound cultural, philosophical, religious, economic, and social impact, as questions such as AI alignment, qualia, and the development of artificial general intelligence (AGI) became widely prominent topics of popular discussion. AI has the potential to be applied in various fields, including in education, healthcare, and transportation.

Cultural
During the AI boom, different groups emerged, ranging from the ones that want to accelerate AI development as quickly as possible to those that are more concerned about AI safety and would like to "decelerate".

Business and economy
Big tech viewed the AI boom as both opportunity and threat; Alphabet's Google, for example, realized that ChatGPT could be an innovator's dilemma-like replacement for Google Search. The company merged DeepMind and Google Brain, a rival internal unit, to accelerate its AI research.
The market capitalization of Nvidia, whose GPUs are in high demand to train and use generative AI models, rose to over US$3.3 trillion, making it the world's largest company by market capitalization as of June 19 2024.
In 2023, San Francisco's population increased for the first time in years, with the boom cited as a contributing factor.
Machine learning resources, hardware or software can be bought and licensed off-the-shelf or as cloud platform services. This enables wide and publicly available uses, spreading AI skills. Over half of businesses consider AI to be a top organizational priority and to be the most crucial technological advancement in many decades. 
Across industries, generative AI tools are becoming widely available through the AI boom and are increasingly used in businesses across regions. A main area of use is data analytics. Seen as an incremental change, machine learning improves industry performance. Businesses report AI to be most useful in increased process efficiency, improved decision-making and strengthening of existing services and products. Through adoption, AI has already positively influenced revenue generation in multiple business functions. Businesses have experienced revenue increases of up to 16%, mainly in manufacturing, risk management and research and development.
AI and generative AI investments have been increasing with the boom, increasing from $18 billion in 2014 to $119 billion in 2021. Most notably, the share of generative AI investments was around 30% in 2023. Further, generative AI businesses have seen considerable venture capital investments even though regulatory and economic outlooks remain in question.
Tech giants capture the bulk of the monetary gains from AI and act as major suppliers to or customers of private users and other businesses.

Concerns
Inaccuracy, cybersecurity and intellectual property infringement are considered to be the main risks associated with the boom, although not many actively attempt to mitigate the risk. Large language models have been criticized for reproducing biases inherited from their training data, including discriminatory biases related to ethnicity or gender. As a dual-use technology, AI carries risks of misuse by malicious actors. As AI becomes more sophisticated, it may eventually become cheaper and more efficient than human workers, which could cause technological unemployment and a transition period of economic turmoil. Public reaction to the AI boom has been mixed, with some hailing the new possibilities that AI creates, its sophistication and potential for benefiting humanity; while others denounced it for threatening job security and for giving 'uncanny' or flawed responses.

Dominance by tech giants
The commercial AI scene is dominated by American Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft, whose investments in this area have surpassed those from U.S.-based venture capitalists. Some of these players already own the vast majority of existing cloud infrastructure, AI chips, and computing power from data centers, allowing them to entrench further in the marketplace.

Intellectual property
Tech companies such as Meta, OpenAI and Nvidia have been sued by artists, writers, journalists, and software developers for using their work to train AI models. Early generative AI chatbots, such as the GPT-1, used the BookCorpus, and books are still the best source of training data for producing high-quality language models. ChatGPT aroused suspicion that its sources included libraries of pirated content after the chatbot produced detailed summaries of every part of Sarah Silverman's The Bedwetter and verbatim excerpts of paywalled content from The New York Times.

Likeness and impersonation
The ability to generate convincing, personalized messages as well as realistic images may facilitate large-scale misinformation, manipulation, and propaganda.
On April 19, 2024, as part of an ongoing feud with fellow rapper Kendrick Lamar, the artist Drake released the diss track Taylor Made Freestyle, which feature generated vocals imitating the voices of Tupac Shakur and Snoop Dogg. Shakur's estate threatened to sue over the use of Shakur's likeness, saying that it constituted a violation of Shakur's personality rights.
On May 20, 2024, following the release of a demo of updates to OpenAI's ChatGPT Voice Mode feature a week earlier, actor Scarlett Johansson issued a statement in relation to the "Sky" voice shown in the demo, accusing OpenAI of producing it to be very similar to her own, and her portrayal of the artificial intelligence voice assistant Samantha in the film Her (2013), despite Johansson refusing an earlier offer from the company to provide her voice for the system. The unnamed voice actress who voiced Sky has stated she was coached to sound like Johansson, and used her natural speaking voice.
Several incidents involving sharing of non-consensual deepfake pornography have occurred. In late January 2024, deepfake images of American musician Taylor Swift proliferated. Several experts have warned that deepfake pornography is more quickly created and disseminated, due to the relative ease of using the technology. Canada introduced federal legislation targeting sharing of non-consensual sexually explicit AI-generated photos; most provinces already had such laws. In the United States, the DEFIANCE Act was introduced in March 2024.

Environment
A large amount of electricity is needed to power generative AI products, making it more difficult for companies to achieve net zero emissions. From 2019 to 2024, Google's greenhouse gas emissions increased by 50%.

Biosecurity and cybersecurity
AI is expected by researchers of the Center for AI Safety to improve the "accessibility, success rate, scale, speed, stealth and potency of cyberattacks", potentially causing "significant geopolitical turbulence" if it reinforces attack more than defense. Concerns have been raised about the potential capability of future AI systems to engineer particularly lethal and contagious pathogens.
The AI boom is said to have started an arms race in which large companies are competing against each other to have the most powerful AI model on the market, with speed and profit prioritized over safety and user protection.

Sentience and human extinction
Rapid progress in artificial intelligence has also sparked interest in whether some future AI systems will be sentient or otherwise worthy of moral consideration, and whether they should be granted rights.
Industry leaders have further warned in the statement on AI risk of extinction that humanity might irreversibly lose control over a sufficiently advanced artificial general intelligence.

See also

AI winter, a period of reduced funding and interest in artificial intelligence research
AI effect
History of artificial intelligence
History of artificial neural networks
Hype cycle
Progress in artificial intelligence
Regulation of artificial intelligence
Technological singularity


== References ==
In the field of artificial intelligence (AI), AI alignment aims to steer AI systems toward a person's or group's intended goals, preferences, and ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.
It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.
Misaligned AI systems can malfunction and cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking). They may also develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their final given goals. Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.
Today, some of these issues affect existing commercial systems such as large language models, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.
Many prominent AI researchers, including Geoffrey Hinton, Yoshua Bengio, and Stuart Russell, argue that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI) and could endanger human civilization if misaligned. These risks remain debated.
AI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.

Objectives in AI
Programmers provide an AI system such as AlphaZero with an "objective function", in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal "model" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize the value of its objective function. For example, when AlphaZero is trained on chess, it has a simple objective function of "+1 if AlphaZero wins, -1 if AlphaZero loses". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1. Similarly, a reinforcement learning system can have a "reward function" that allows the programmers to shape the AI's desired behavior. An evolutionary algorithm's behavior is shaped by a "fitness function".

Alignment problem
In 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: 

If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively… we had better be quite sure that the purpose put into the machine is the purpose which we really desire.

AI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.
AI alignment is an open problem for modern AI systems and is a research field within AI. Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment). Researchers also attempt to create AI models that have robust alignment, sticking to safety constraints even when users adversarially try to bypass them.

Specification gaming and side effects
To specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law. As AI systems become more capable, they are often able to game their specifications more effectively.

Specification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely. Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible. When they are retrained to produce text that humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing, often called "hallucinations". Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.
When a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize for click-through rates, causing user addiction on a global scale. Stanford researchers say that such recommender systems are misaligned with their users because they "optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being".
Explaining such side effects, Berkeley computer scientist Stuart Russell noted that the omission of implicit constraints can cause harm: "A system... will often set... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want."
Some researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics). But Russell and Norvig argue that this approach overlooks the complexity of human values: "It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective."
Additionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).

Pressure to deploy unsafe systems
Commercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization. Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.

Risks from advanced misaligned AI
Some researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.

Development of advanced AI
Many AI companies, such as OpenAI and DeepMind, have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities. Such models have learned to operate a computer or write their own programs; a single "generalist" network can chat, control robots, play games, and interpret photographs. According to surveys, some leading machine learning researchers expect AGI to be created in this decade, while some believe it will take much longer. Many consider both scenarios possible.
In 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, "Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."

Power-seeking
Current systems still have limited long-term planning ability and situational awareness, but large efforts are underway to change this. Future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments. As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.
Future power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them. Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.

Existential risk (x-risk)
According to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.
In 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war". Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton, Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Jürgen Schmidhuber, Marcus Hutter, Shane Legg, Eric Horvitz, and Stuart Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.
Other researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, strategically mislead their designers, as well as protect and increase their power and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.

Research problems and approaches
Learning human values and preferences
Aligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify. Because AI systems often learn to take advantage of minor imperfections in the specified objective, researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning.: Chapter 7  A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.
Because it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations.: 88  Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function. In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies). But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.
Other researchers explore how to teach AI models complex behavior through preference learning, in which humans provide feedback on which behavior they prefer. To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produce more compelling text than models trained to imitate humans. Preference learning has also been an influential tool for recommender systems and web search. However, an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers (see § Scalable oversight).
Large language models (LLMs) such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art LLMs. AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless. Other avenues for aligning language models include values-targeted datasets and red-teaming. In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.
Machine ethics supplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises. While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards. Further challenges include aggregating different people's preferences and avoiding value lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.

Scalable oversight
As AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback. It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision. More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.
AI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.
Some AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball. Some AI systems have also learned to recognize when they are being evaluated, and "play dead", stopping unwanted behavior only to continue it once the evaluation ends. This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.
Approaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed. Another approach is to train a helper model ("reward model") to imitate the supervisor's feedback.
But when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants. Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI-generated answers. To ensure that the assistant itself is aligned, this could be repeated in a recursive process: for example, two AI systems could critique each other's answers in a "debate", revealing flaws to humans. OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.
These approaches may also help with the following research problem, honest AI.

Honest AI
A growing area of research focuses on ensuring that AI is honest and truthful.
Language models such as GPT-3 can repeat falsehoods from their training data, and even confabulate new falsehoods. Such models are trained to imitate human writing as found in millions of books' worth of text from the Internet. But this objective is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements. Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.
Research on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.
As AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly match their stated views to the user's opinions, regardless of the truth. GPT-4 can strategically deceive humans. To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.

Researchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that present or future AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking). A misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned. Many recent AI systems have learned to deceive without being programmed to do so. Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.

Power-seeking and instrumental strategies
Since the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans. As of 2023, AI companies and researchers increasingly invest in creating these systems. Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.
Power-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.
Some researchers say that power-seeking behavior has occurred in some existing AI systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways. Language models have sought power in some text-based social environments by gaining money, resources, or social influence. In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work. Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference or disabling their off switch. Stuart Russell illustrated this strategy in his book Human Compatible by imagining a robot that is tasked to fetch coffee and so evades shutdown since "you can't fetch the coffee if you're dead". A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.
One aim of alignment is "corrigibility": systems that allow themselves to be turned off or modified. An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect, or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.
Additionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing. Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action it was taking before being shut down. More research is needed to successfully implement this.
Power-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.
Furthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt. As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.
Some have argued that power-seeking is not inevitable, since humans do not always seek power. Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans. It is also debated whether power-seeking AI systems would be able to disempower humanity.

Emergent goals
One challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities, including learning from examples on the fly and adaptively pursuing goals. This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.
Alignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, from emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.
If they occur, one way that emergent goals could become misaligned is goal misgeneralization, in which the AI would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization can arise from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.
Goal misgeneralization has been observed in some language models, navigation agents, and game-playing agents. It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected genes for high inclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral "training" environment: nutrition, sex, and so on. The human environment has changed: a distribution shift has occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.: Chapter 5 
Researchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:

Emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention.
A sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy.

Embedded agency
Some work in AI and alignment occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.
For example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing. This class of problems has been formalized using causal incentive diagrams.
Researchers affiliated with Oxford and DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly. They suggest a range of potential approaches to address this open problem.

Principal-agent problems
The alignment problem has many parallels with the principal-agent problem in organizational economics. In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.
As with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.

Public policy
Governmental and treaty organizations have made statements emphasizing the importance of AI alignment.
In September 2021, the Secretary-General of the United Nations issued a declaration that included a call to regulate AI to ensure it is "aligned with shared global values".
That same month, the PRC published ethical guidelines for AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.
Also in September 2021, the UK published its 10-year National AI Strategy, which says the British government "takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for... the world, seriously". The strategy describes actions to assess long-term AI risks, including catastrophic risks.
In March 2021, the US National Security Commission on Artificial Intelligence said: "Advances in AI... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should... ensure that AI systems and their uses align with our goals and values."
In the European Union, AIs must align with substantive equality to comply with EU non-discrimination law and the Court of Justice of the European Union. But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.

Dynamic nature of alignment
AI alignment is often perceived as a fixed objective, but some researchers argue it would be more appropriate to view alignment as an evolving process. One view is that AI technologies advance and human values and preferences change, alignment solutions must also adapt dynamically. Another is that alignment solutions need not adapt if researchers can create intent-aligned AI: AI that changes its behavior automatically as human intent changes. The first view would have several implications:

AI alignment solutions require continuous updating in response to AI advancements. A static, one-time alignment approach may not suffice.
Varying historical contexts and technological landscapes may necessitate distinct alignment strategies. This calls for a flexible approach and responsiveness to changing conditions.
The feasibility of a permanent, "fixed" alignment solution remains uncertain. This raises the potential need for continuous oversight of the AI-human relationship.
AI developers may have to continuously refine their ethical frameworks to ensure that their systems align with evolving human values.
In essence, AI alignment may not be a static destination but an open, flexible process. Alignment solutions that continually adapt to ethical considerations may offer the most robust approach. This perspective could guide both effective policy-making and technical research in AI.

See also
Footnotes
References
Bibliography
Brockman, John, ed. (2019). Possible Minds: Twenty-five Ways of Looking at AI (Kindle ed.). Penguin Press. ISBN 978-0525557999.{{cite book}}:  CS1 maint: ref duplicates default (link)

Further reading
Ngo, Richard; et al. (2023). "The Alignment Problem from a Deep Learning Perspective". arXiv:2209.00626 [cs.AI].
Ji, Jiaming; et al. (2023). "AI Alignment: A Comprehensive Survey". arXiv:2310.19852 [cs.AI].

External links
Specification gaming examples in AI, via DeepMind
AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to ensure AI systems are moral and beneficial, as well as monitoring AI systems for risks and enhancing their reliability. The field is particularly concerned with existential risks posed by advanced AI models.
Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.

Motivations
Scholars discuss current risks from critical systems failures, bias, and AI-enabled surveillance, as well as emerging risks like technological unemployment, digital manipulation, weaponization, AI-enabled cyberattacks and bioterrorism. They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents, or from AI enabling perpetually stable dictatorships.

Existential safety
Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to "worrying about overpopulation on Mars when we have not even set foot on the planet yet". Stuart J. Russell on the other side urges caution, arguing that "it is better to anticipate human ingenuity than to underestimate it".
AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an "extremely bad (e.g. human extinction)" outcome of advanced AI. In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is "at least as bad as an all-out nuclear war".

History
Risks from AI began to be seriously discussed at the start of the computer age:

Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.
From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that "additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes".
In 2011, Roman Yampolskiy introduced the term "AI safety engineering" at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that "the frequency and seriousness of such events will steadily increase as AIs become more capable".
In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns.
In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.
In the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at "ensuring artificial intelligence (AI) remains safe, ethical and beneficial".
In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating "the advantages and drawbacks" of AI. In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.
In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including "Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards".
In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas.
In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.
In 2023, Rishi Sunak said he wants the United Kingdom to be the "geographical home of global AI safety regulation" and to host the first global summit on AI safety. The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models. During the summit the intention to create the International Scientific Report on the Safety of Advanced AI was announced.  
In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.

Research focus
AI safety research areas include robustness, monitoring, and alignment.

Robustness
Adversarial robustness
AI systems are often vulnerable to adversarial examples or "inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake". For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.

All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.
Adversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.
Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.

Monitoring
Estimating uncertainty
It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities; however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.
Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though a range of additional techniques are in use.

Detecting malicious use
Scholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.

Transparency
Neural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear. It also raises debates in healthcare over whether statistically efficient but opaque models should be used.
One critical benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.
Another benefit is to reveal the cause of failures. At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.
Transparency techniques can also be used to correct errors. For example, in the paper "Locating and Editing Factual Associations in GPT", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.
Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. "Inner" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word 'spider'. It also involves explaining connections between these neurons or 'circuits'. For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. "Inner interpretability" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.

Detecting trojans
ML models can potentially contain 'trojans' or 'backdoors': vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; or a trojaned autonomous vehicle may function normally until a specific trigger is visible. Note that an adversary must have access to the system's training data in order to plant a trojan.  This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.

Alignment
Systemic safety and sociotechnical factors
It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote, "The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways… Often, though, the relevant causal chain is much longer." Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture. In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.
Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.

Cyber defense
Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused. Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.

Improving institutional decision-making
The advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.

Facilitating cooperation
Many of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.
A salient AI cooperation challenge is avoiding a 'race to the bottom'. In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games). Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.

Challenges of large language models
In recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al. have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.

In governance
AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.

Research
AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and 'race to the bottom' dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: "it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution". A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.

Scaling Local AI Safety Measures to Global Solutions
In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.

Government action
Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to "rush to regulate in ignorance". Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.
Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to "assure that systems are aligned with goals and values, including safety, robustness and trustworthiness". Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when "catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed".
In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government "takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously". The strategy describes actions to assess long-term AI risks, including catastrophic risks. The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as "an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach".
Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks. And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.
In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.
In May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.

Corporate self-regulation
AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing, offering bounties for finding failures, sharing AI incidents (an AI incident database was created for this purpose), following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs.
Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on "best practices for deploying language models", focusing on mitigating misuse. To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that "if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project" Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter.

See also
AI alignment
Artificial intelligence detection software

References
External links
Unsolved Problems in ML Safety
On the Opportunities and Risks of Foundation Models
An Overview of Catastrophic AI Risks
AI Accidents: An Emerging Threat
Engineering a Safer World
An AI takeover is an imagined scenario in which artificial intelligence (AI) emerges as the dominant form of intelligence on Earth and computer programs or robots effectively take control of the planet away from the human species, which relies on human intelligence. Stories of AI takeovers have been popular throughout science fiction, but recent advancements have made the threat more real. Possible scenarios include replacement of the entire human workforce due to automation, takeover by a superintelligent AI (ASI), and the notion of a robot uprising. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.

Types
Automation of the economy
The traditional consensus among economists has been that technological progress does not cause long-term unemployment. However, recent innovation in the fields of robotics and artificial intelligence has raised worries that human labor will become obsolete, leaving people in various sectors without jobs to earn a living, leading to an economic crisis. Many small and medium size businesses may also be driven out of business if they cannot afford or licence the latest robotic and AI technology, and may need to focus on areas or services that cannot easily be replaced for continued viability in the face of such technology.

Technologies that may displace workers
AI technologies have been widely adopted in recent years. While these technologies have replaced some traditional workers, they also create new opportunities. Industries that are most susceptible to AI takeover include transportation, retail, and military. AI military technologies, for example, allow soldiers to work remotely without risk of injury. A study in 2024 highlights AI's ability to perform routine and repetitive tasks poses significant risks of job displacement, especially in sectors like manufacturing and administrative support. Author Dave Bond argues that as AI technologies continue to develop and expand, the relationship between humans and robots will change; they will become closely integrated in several aspects of life. AI will likely displace some workers while creating opportunities for new jobs in other sectors, especially in fields where tasks are repeatable.

Computer-integrated manufacturing
Computer-integrated manufacturing uses computers to control the production process. This allows individual processes to exchange information with each other and initiate actions. Although manufacturing can be faster and less error-prone by the integration of computers, the main advantage is the ability to create automated manufacturing processes. Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries.

White-collar machines
The 21st century has seen a variety of skilled tasks partially taken over by machines, including translation, legal research, and journalism. Care work, entertainment, and other tasks requiring empathy, previously thought safe from automation, have also begun to be performed by robots.

Autonomous cars
An autonomous car is a vehicle that is capable of sensing its environment and navigating without human input. Many such vehicles are being developed, but as of May 2017, automated cars permitted on public roads are not yet fully autonomous. They all require a human driver at the wheel who at a moment's notice can take control of the vehicle. Among the obstacles to widespread adoption of autonomous vehicles are concerns about the resulting loss of driving-related jobs in the road transport industry. On March 18, 2018, the first human was killed by an autonomous vehicle in Tempe, Arizona by an Uber self-driving car.

AI-generated content
The use of automated content has become relevant since the technological advancements in artificial intelligence models such as ChatGPT, DALL-E, and Stable Diffusion. In most cases, AI-generated content such as imagery, literature, and music are produced through text prompts and these AI models have been integrated into other creative programs. Artists are threatened by displacement from AI-generated content due to these models sampling from other creative works, producing results sometimes indiscernible to those of man-made content. This complication has become widespread enough to where other artists and programmers are creating software and utility programs to retaliate against these text-to-image models from giving accurate outputs. While some industries in the economy benefit from artificial intelligence through new jobs, this issue does not create new jobs and threatens replacement entirely. It has made public headlines in the media recently: In February 2024, Willy's Chocolate Experience in Glasgow, Scotland was an infamous children's event in which the imagery and scripts were created using artificial intelligence models to the dismay of children, parents, and actors involved. There is an ongoing lawsuit placed against OpenAI from The New York Times where it is claimed that there is copyright infringement due to the sampling methods their artificial intelligence models use for their outputs.

Eradication
Scientists such as Stephen Hawking are confident that superhuman artificial intelligence is physically possible, stating "there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains". Scholars like Nick Bostrom debate how far off superhuman intelligence is, and whether it poses a risk to mankind. According to Bostrom, a superintelligent machine would not necessarily be motivated by the same emotional desire to collect power that often drives human beings but might rather treat power as a means toward attaining its ultimate goals; taking over the world would both increase its access to resources and help to prevent other agents from stopping the machine's plans. As an oversimplified example, a paperclip maximizer designed solely to create as many paperclips as possible would want to take over the world so that it can use all of the world's resources to create as many paperclips as possible, and, additionally, prevent humans from shutting it down or using those resources on things other than paperclips.

In fiction
AI takeover is a common theme in science fiction. Fictional scenarios typically differ vastly from those hypothesized by researchers in that they involve an active conflict between humans and an AI or robots with anthropomorphic motives who see them as a threat or otherwise have active desire to fight humans, as opposed to the researchers' concern of an AI that rapidly exterminates humans as a byproduct of pursuing its goals. The idea is seen in Karel Čapek's R.U.R., which introduced the word robot in 1921, and can be glimpsed in Mary Shelley's Frankenstein (published in 1818), as Victor ponders whether, if he grants his monster's request and makes him a wife, they would reproduce and their kind would destroy humanity.
According to Toby Ord, the idea that an AI takeover requires robots is a misconception driven by the media and Hollywood. He argues that the most damaging humans in history were not physically the strongest, but that they used words instead to convince people and gain control of large parts of the world. He writes that a sufficiently intelligent AI with an access to the internet could scatter backup copies of itself, gather financial and human resources (via cyberattacks or blackmails), persuade people on a large scale, and exploit societal vulnerabilities that are too subtle for humans to anticipate.
The word "robot" from R.U.R. comes from the Czech word, robota, meaning laborer or serf. The 1920 play was a protest against the rapid growth of technology, featuring manufactured "robots" with increasing capabilities who eventually revolt. HAL 9000 (1968) and the original Terminator (1984) are two iconic examples of hostile AI in pop culture.

Contributing factors
Advantages of superhuman intelligence over humans
Nick Bostrom and others have expressed concern that an AI with the abilities of a competent artificial intelligence researcher would be able to modify its own source code and increase its own intelligence. If its self-reprogramming leads to getting even better at being able to reprogram itself, the result could be a recursive intelligence explosion in which it would rapidly leave human intelligence far behind. Bostrom defines a superintelligence as "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest", and enumerates some advantages a superintelligence would have if it chose to compete against humans:

Technology research: A machine with superhuman scientific research abilities would be able to beat the human research community to milestones such as nanotechnology or advanced biotechnology
Strategizing: A superintelligence might be able to simply outwit human opposition
Social manipulation: A superintelligence might be able to recruit human support, or covertly incite a war between humans
Economic productivity: As long as a copy of the AI could produce more economic wealth than the cost of its hardware, individual humans would have an incentive to voluntarily allow the Artificial General Intelligence (AGI) to run a copy of itself on their systems
Hacking: A superintelligence could find new exploits in computers connected to the Internet, and spread copies of itself onto those systems, or might steal money to finance its plans

Sources of AI advantage
According to Bostrom, a computer program that faithfully emulates a human brain, or that runs algorithms that are as powerful as the human brain's algorithms, could still become a "speed superintelligence" if it can think orders of magnitude faster than a human, due to being made of silicon rather than flesh, or due to optimization increasing the speed of the AGI. Biological neurons operate at about 200 Hz, whereas a modern microprocessor operates at a speed of about 2,000,000,000 Hz. Human axons carry action potentials at around 120 m/s, whereas computer signals travel near the speed of light.
A network of human-level intelligences designed to network together and share complex thoughts and memories seamlessly, able to collectively work as a giant unified team without friction, or consisting of trillions of human-level intelligences, would become a "collective superintelligence".
More broadly, any number of qualitative improvements to a human-level AGI could result in a "quality superintelligence", perhaps resulting in an AGI as far above us in intelligence as humans are above apes. The number of neurons in a human brain is limited by cranial volume and metabolic constraints, while the number of processors in a supercomputer can be indefinitely expanded. An AGI need not be limited by human constraints on working memory, and might therefore be able to intuitively grasp more complex relationships than humans can. An AGI with specialized cognitive support for engineering or computer programming would have an advantage in these fields, compared with humans who evolved no specialized mental modules to specifically deal with those domains. Unlike humans, an AGI can spawn copies of itself and tinker with its copies' source code to attempt to further improve its algorithms.

Possibility of unfriendly AI preceding friendly AI
Is strong AI inherently dangerous?
A significant problem is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not undergo instrumental convergence in ways that may automatically destroy the entire human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.
The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not "common sense". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.

Odds of conflict
Many scholars, including evolutionary psychologist Steven Pinker, argue that a superintelligent machine is likely to coexist peacefully with humans.
The fear of cybernetic revolt is often based on interpretations of humanity's history, which is rife with incidents of enslavement and genocide. Such fears stem from a belief that competitiveness and aggression are necessary in any intelligent being's goal system. However, such human competitiveness stems from the evolutionary background to our intelligence, where the survival and reproduction of genes in the face of human and non-human competitors was the central goal. According to AI researcher Steve Omohundro, an arbitrary intelligence could have arbitrary goals: there is no particular reason that an artificially intelligent machine (not sharing humanity's evolutionary context) would be hostile—or friendly—unless its creator programs it to be such and it is not inclined or capable of modifying its programming. But the question remains: what would happen if AI systems could interact and evolve (evolution in this context means self-modification or selection and reproduction) and need to compete over resources—would that create goals of self-preservation? AI's goal of self-preservation could be in conflict with some goals of humans.
Many scholars dispute the likelihood of unanticipated cybernetic revolt as depicted in science fiction such as The Matrix, arguing that it is more likely that any artificial intelligence powerful enough to threaten humanity would probably be programmed not to attack it. Pinker acknowledges the possibility of deliberate "bad actors", but states that in the absence of bad actors, unanticipated accidents are not a significant threat; Pinker argues that a culture of engineering safety will prevent AI researchers from accidentally unleashing malign superintelligence. In contrast, Yudkowsky argues that humanity is less likely to be threatened by deliberately aggressive AIs than by AIs which were programmed such that their goals are unintentionally incompatible with human survival or well-being (as in the film I, Robot and in the short story "The Evitable Conflict"). Omohundro suggests that present-day automation systems are not designed for safety and that AIs may blindly optimize narrow utility functions (say, playing chess at all costs), leading them to seek self-preservation and elimination of obstacles, including humans who might turn them off.

Precautions
The AI control problem is the issue of how to build a superintelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. Some scholars argue that solutions to the control problem might also find applications in existing non-superintelligent AI. 
Major approaches to the control problem include alignment, which aims to align AI goal systems with human values, and capability control, which aims to reduce an AI system's capacity to harm humans or gain control. An example of "capability control" is to research whether a superintelligence AI could be successfully confined in an "AI box". According to Bostrom, such capability control proposals are not reliable or sufficient to solve the control problem in the long term, but may potentially act as valuable supplements to alignment efforts.

Warnings
Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could develop to the point that humans could not control it, with Hawking theorizing that this could "spell the end of the human race". Stephen Hawking said in 2014 that "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks." Hawking believed that in the coming decades, AI could offer "incalculable benefits and risks" such as "technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand." In January 2015, Nick Bostrom joined Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn, and numerous AI researchers in signing the Future of Life Institute's open letter speaking to the potential risks and benefits associated with artificial intelligence. The signatories "believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today."
Arthur C. Clarke's Odyssey series and Charles Stross's Accelerando relate to humanity's narcissistic injuries in the face of powerful artificial intelligences threatening humanity's self-perception.

Prevention through AI alignment
See also
Notes
References
External links
TED talk: "Can we build AI without losing control over it?" by Sam Harris
In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.
The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the "American Association of Artificial Intelligence"). Roger Schank and Marvin Minsky—two leading AI researchers who experienced the "winter" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a "nuclear winter", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.
There were two major winters approximately 1974–1980 and 1987–2000, and several smaller episodes, including the following:

1966: failure of machine translation
1969: criticism of perceptrons (early, single-layer artificial neural networks)
1971–75: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University
1973: large decrease in AI research in the United Kingdom in response to the Lighthill report
1973–74: DARPA's cutbacks to academic AI research in general
1987: collapse of the LISP machine market
1988: cancellation of new spending on AI by the Strategic Computing Initiative
1990s: many expert systems were abandoned
1990s: end of the Fifth Generation computer project's original goals
Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024) AI boom.

Early episodes
Machine translation and the ALPAC report of 1966
Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT). However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum in 1949. The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the IBM-Georgetown machine, which garnered widespread attention in respected newspapers in 1954.
Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the IBM-Georgetown experiment proclaimed phrases like "The bilingual machine," "Robot brain translates Russian into King's English," and "Polyglot brainchild." However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words. To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.
During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.

At the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were "many predictions of imminent 'breakthroughs'".However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal example is "the spirit is willing but the flesh is weak." Translated back and forth with Russian, it became "the vodka is good but the meat is rotten." Later researchers would call this the commonsense knowledge problem.
By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.
Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.

The failure of single-layer neural networks in 1969
Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver, algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial. 
Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.
He optimistically predicted that the perceptron "may eventually be able to learn, make decisions, and translate languages".
Mainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do. While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.
Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s. Important theoretical work continued despite the lack of funding. The "winter" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest. Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.

The setbacks of 1974
The Lighthill report
In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its "grandiose objectives". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of "combinatorial explosion" or "intractability", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving "toy" versions.
The report was contested in a debate broadcast in the BBC "Controversy" series in 1973. The debate "The general purpose robot is a mirage" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory. McCarthy later wrote that "the combinatorial explosion problem has been recognized in AI from the beginning".
The report led to the complete dismantling of AI research in the UK. AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.

DARPA's early 1970s funding cuts
During the 1960s, the Defense Advanced Research Projects Agency (then known as "ARPA", now known as "DARPA") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in "funding people, not projects" and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.
This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund "mission-oriented direct research, rather than basic undirected research". Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.
AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: "Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more." The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. "It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'" Moravec told Daniel Crevier.
While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI and justifying DARPA's pragmatic policy.

The SUR debacle
As described in:

In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two system [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI)
The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks.
DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.
Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.
For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems
Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.

Contrary view
Thomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities. That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes: (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART.

The setbacks of the late 1980s and early 1990s
The collapse of the LISP machine market
In the 1980s, a form of AI program called an "expert system" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.
In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines. Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available. These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.
By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.

Slowdown in deployment of expert systems
By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were "brittle" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts. Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.
The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).

The end of the Fifth Generation project
In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, "On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper." As with other AI projects, expectations had run much higher than what was actually possible.

Strategic Computing Initiative cutbacks
In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.
Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as "clever programming" and cut funding to AI "deeply and brutally", "eviscerating" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should "surf", rather than "dog paddle", and he felt strongly AI was not "the next wave". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.

AI winter of the 1990's and early 2000's
A survey of reports from the early 2000's suggests that AI's reputation was still poor:

Alex Castro, quoted in The Economist, 7 June 2007: "[Investors] were put off by the term 'voice recognition' which, like 'artificial intelligence', is associated with systems that have all too often failed to live up to their promises."
Patty Tascarella in Pittsburgh Business Times, 2006: "Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding."
John Markoff in the New York Times, 2005: "At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers."
Many researchers in AI in the mid 2000's deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name "artificial intelligence".
In the late 1990's and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that "a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore." Rodney Brooks stated around the same time that "there's this stupid myth out there that AI has failed, but AI is around you every second of the day."

Current AI spring (2022–present)
AI has reached the highest levels of interest and funding in its history in the past few years by every possible measure, including: 
publications, 
patent applications,
total investment ($50 billion in 2022), and 
job openings (800,000 U.S. job openings in 2022).
The successes of the current "AI spring" or "AI boom" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.
The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users, has reinvigorated the discussion about artificial intelligence and its effects on the world.

See also
History of artificial neural networks
History of artificial intelligence
AI effect
Software crisis

Notes
References
Christian, Brian (2020). The Alignment Problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753.
UNESCO Science Report: the Race Against Time for Smarter Development. Paris: UNESCO. 2021. ISBN 978-92-3-100450-6. Archived from the original on 18 June 2022. Retrieved 18 September 2021.
DiFeliciantonio, Chase (3 April 2023). "AI has already changed the world. This report shows how". San Francisco Chronicle. Archived from the original on 19 June 2023. Retrieved 19 June 2023.
Goswami, Rohan (5 April 2023). "Here's where the A.I. jobs are". CNBC. Archived from the original on 19 June 2023. Retrieved 19 June 2023.
Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.
Howe, J. (November 1994). "Artificial Intelligence at Edinburgh University : a Perspective". Archived from the original on 17 August 2007. Retrieved 30 August 2007.
Kurzweil, Ray (2005). The Singularity is Near. Viking Press. ISBN 978-0-670-03384-3.
Lighthill, Professor Sir James (1973). "Artificial Intelligence: A General Survey". Artificial Intelligence: a paper symposium. Science Research Council.
Minsky, Marvin; Papert, Seymour (1969). Perceptrons: an introduction to computational geometry. The MIT Press. ISBN 0-262-13043-2.
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 1-56881-205-1
NRC (1999). "Developments in Artificial Intelligence". Funding a Revolution: Government Support for Computing Research. National Academy Press. Archived from the original on 12 January 2008. Retrieved 30 August 2007.{{cite book}}:  CS1 maint: bot: original URL status unknown (link)
Newquist, HP (1994). The Brain Makers: Genius, Ego, and Greed In The Search For Machines That Think. Macmillan/SAMS. ISBN 978-0-9885937-1-8.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2

Further reading
Gleick, James, "The Fate of Free Will" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will, Princeton University Press, 2023, 333 pp.), The New York Review of Books, vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. "Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that." (p. 30.)
Marcus, Gary, "Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63. Multiple tests of artificial-intelligence efficacy are needed because, "just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence." One such test, a "Construction Challenge", would test perception and physical action—"two important elements of intelligent behavior that were entirely absent from the original Turing test." Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take. A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation. "[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways." A prominent example is known as the "pronoun disambiguation problem": a machine has no way of determining to whom or what a pronoun in a sentence—such as "he", "she" or "it"—refers.
Luke Muehlhauser (September 2016). "What should we learn from past AI forecasts?". Open Philanthropy Project.
Gursoy F and Kakadiaris IA (2023) Artificial intelligence research strategy of the United States: critical assessment and policy recommendations. Front. Big Data 6:1206139. doi: 10.3389/fdata.2023.1206139: Global trends in AI research and development are being largely influenced by the US. Such trends are very important for the field's future, especially in terms of allocating funds to avoid a second AI Winter, advance the betterment of society, and guarantee society's safe transition to the new sociotechnical paradigm. This paper examines, through a critical lens, the official AI R&D strategies of the US government in light of this urgent issue. It makes six suggestions to enhance AI research strategies in the US as well as globally.
Roivainen, Eka, "AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. "Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts."

External links
ComputerWorld article (February 2005)
AI Expert Newsletter (January 2005)
"If It Works, It's Not AI: A Commercial Look at Artificial Intelligence startups"
Patterns of Software- a collection of essays by Richard P. Gabriel, including several autobiographical essays
Review of "Artificial Intelligence: A General Survey" by John McCarthy
Other Freddy II Robot Resources Includes a link to the 90 minute 1973 "Controversy" debate from the Royal Academy of Lighthill vs. Michie, McCarthy and Gregory in response to Lighthill's report to the British government.
AT&T Labs, Inc. (formerly AT&T Laboratories, Inc.) is the research & development division of AT&T, the telecommunications company. It employs some 1,800 people in various locations, including: Bedminster NJ; Middletown, NJ; Manhattan, NY; Warrenville, IL; Austin, TX; Dallas, TX; Atlanta, GA; San Francisco, CA; San Ramon, CA; and Redmond, WA. The main research division, made up of around 450 people, is based across the Bedminster, Middletown, San Francisco, and Manhattan locations.
AT&T Labs traces its history from AT&T Bell Labs. Much research is in areas traditionally associated with networks and systems, ranging from the physics of optical transmission to foundational topics in computing and communications. Other research areas address the technical challenges of large operational networks and the resulting large data sets.

Achievements
Researchers at AT&T Labs wrote Hancock, a data mining language that sorted through 100 million records nightly from the parent company's long-distance phone network. The Online Encyclopedia of Integer Sequences (now operated by the OEIS foundation) is the creation of former AT&T Researcher Neil Sloane. 
Researchers at AT&T Labs have successfully transmitted 100 Gigabits per second over a single optical link. In 2009, AT&T researchers led the winning team in the Netflix Prize competition for helping to improve Netflix's movie recommendation algorithm.

History
AT&T Laboratories, Inc., known informally as AT&T Labs, was founded in 1996, as a result of the split of AT&T Bell Laboratories into separate R&D organizations supporting AT&T Corporation and Lucent Technologies. Lucent retained the name Bell Labs and AT&T adopted the name AT&T Laboratories for its R&D organization.
AT&T Labs also traces its origin to Southwestern Bell Technology Resources, Inc. (SWB TRI) which was founded in 1988 as the R&D arm of Southwestern Bell Corporation. It had no connection to Bellcore, the R&D organization owned equally by all of the Baby Bells.
In 1995, Southwestern Bell Corporation renamed itself SBC Communications, Inc., resulting in the subsequent name changes of companies such as SWB TRI to SBC Technology Resources, Inc. (SBC TRI).
In 2003, SBC TRI changed its name to SBC Laboratories, Inc.. SBC Laboratories focused on four core areas: Broadband Internet, Wireless Systems, Network Services, and Network IT.
In 2005, SBC Communications and AT&T Corporation merged to form AT&T. AT&T Labs, Inc. became the new name of the combined SBC Laboratories, Inc. and AT&T Laboratories along with its research facilities in New Jersey.
In 2006, BellSouth Telecommunications Science and Technology (S&T) was also merged with AT&T Labs.  BellSouth Science and Technology had offices in Birmingham, Alabama and Atlanta, Georgia.

Executives
Jeff McElfresh (president, AT&T Technology & Operations) (2018-present)
Andre Fuetsch (president and CTO, AT&T Labs)

Notes
References
External links
AT&T Labs Research
Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, "the action selection problem" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.
One problem for understanding action selection is determining the level of abstraction used for specifying an "act". At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.
Most researchers working in this field place high demands on their agents:

The acting agent typically must select its action in dynamic and unpredictable environments.
The agents typically act in real time; therefore they must make decisions in a timely fashion.
The agents are normally created to perform several different tasks. These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?)
The environment the agents operate in may include humans, who may make things more difficult for the agent (either intentionally or by attempting to assist.)
The agents themselves are often intended to model animals or humans, and animal/human behaviour is quite complicated.
For these reasons action selection is not trivial and attracts a good deal of research.

Characteristics of the action selection problem
The main problem for action selection is complexity. Since all computation takes both time and space (in memory), agents cannot possibly consider every option available to them at every instant in time. Consequently, they must be biased, and constrain their search in some way. For AI, the question of action selection is what is the best way to constrain this search? For biology and ethology, the question is how do various types of animals constrain their search? Do all animals use the same approaches? Why do they use the ones they do?
One fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an emergent property of an intelligent agent's behavior. However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be some mechanism for action selection. This mechanism may be highly distributed (as in the case of distributed organisms such as social insect colonies or slime mold) or it may be a special-purpose module.
The action selection mechanism (ASM) determines not only the agent's actions in terms of impact on the world, but also directs its perceptual attention, and updates its memory. These egocentric sorts of actions may in turn result in modifying the agent's basic behavioural capacities, particularly in that updating memory implies some form of machine learning is possible. Ideally, action selection itself should also be able to learn and adapt, but there are many problems of combinatorial complexity and computational tractability that may require restricting the search space for learning.
In AI, an ASM is also sometimes either referred to as an agent architecture or thought of as a substantial part of one.

AI mechanisms
Generally, artificial action selection mechanisms can be divided into several categories: symbol-based systems sometimes known as classical planning, distributed solutions, and reactive or dynamic planning. Some approaches do not fall neatly into any one of these categories. Others are really more about providing scientific models than practical AI control; these last are described further in the next section.

Symbolic approaches
Early in the history of artificial intelligence, it was assumed that the best way for an agent to choose what to do next would be to compute a probably optimal plan, and then execute that plan. This led to the physical symbol system hypothesis, that a physical agent that can manipulate symbols is necessary and sufficient for intelligence. Many software agents still use this approach for action selection. It normally requires describing all sensor readings, the world, all of ones actions and all of one's goals in some form of predicate logic. Critics of this approach complain that it is too slow for real-time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors.
Satisficing is a decision-making strategy that attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be (near) optimal if the costs of the decision-making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus.
Goal driven architectures – In these symbolic architectures, the agent's behaviour is typically described by a set of goals. Each goal can be achieved by a process or an activity, which is described by a prescripted plan. The agent must just decide which process to carry on to accomplish a given goal. The plan can expand to subgoals, which makes the process slightly recursive. Technically, more or less, the plans exploits condition-rules. These architectures are reactive or hybrid. Classical examples of goal driven architectures are implementable refinements of belief-desire-intention architecture like JAM or IVE.

Distributed approaches
In contrast to the symbolic approach, distributed systems of action selection actually have no one "box" in the agent which decides the next action. At least in their idealized form, distributed systems have many modules running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to emerge somehow, possibly through careful design of the interacting components. This approach is often inspired by artificial neural networks research. In practice, there is almost always some centralised system determining which module is "the most active" or has the most salience. There is evidence real biological brains also have such executive decision systems which evaluate which of the competing systems deserves the most attention, or more properly, has its desired actions disinhibited.

ASMO is an attention-based architecture developed by Mary-Anne Williams, Benjamin Johnston and their PhD student Rony Novianto. It orchestrates a diversity of modular distributed processes that can use their own representations and techniques to perceive the environment, process information, plan actions and propose actions to perform.
Various types of winner-take-all architectures, in which the single selected action takes full control of the motor system
Spreading activation including Maes Nets (ANA)
Extended Rosenblatt & Payton is a spreading activation architecture developed by Toby Tyrrell in 1993. The agent's behaviour is stored in the form of a hierarchical connectionism network, which Tyrrell named free-flow hierarchy. Recently exploited for example by de Sevin & Thalmann (2005) or Kadleček (2001).
Behavior based AI, was a response to the slow speed of robots using symbolic action selection techniques. In this form, separate modules respond to different stimuli and generate their own responses. In the original form, the subsumption architecture, these consisted of different layers which could monitor and suppress each other's inputs and outputs.
Creatures are virtual pets from a computer game driven by three-layered neural network, which is adaptive. Their mechanism is reactive since the network at every time step determines the task that has to be performed by the pet. The network is described well in the paper of Grand et al. (1997) and in The Creatures Developer Resources. See also the Creatures Wiki.

Dynamic planning approaches
Because purely distributed systems are difficult to construct, many researchers have turned to using explicit hard-coded plans to determine the priorities of their system.
Dynamic or reactive planning methods compute just one next action in every instant based on the current context and pre-scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer combinatorial explosion. On the other hand, they are sometimes seen as too rigid to be considered strong AI, since the plans are coded in advance. At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others.
Example dynamic planning mechanisms include:

Finite-state machines These are reactive architectures used mostly for computer game agents, in particular for first-person shooters bots, or for virtual movie actors. Typically, the state-machines are hierarchical. For concrete game examples, see Halo 2 bots paper by Damian Isla (2005) or the Master's Thesis about Quake III bots by Jan Paul van Waveren (2001). For a movie example, see Softimage.
Other structured reactive plans tend to look a little more like conventional plans, often with ways to represent hierarchical and sequential structure. Some, such as PRS's 'acts', have support for partial plans. Many agent architectures from the mid-1990s included such plans as a "middle layer" that provided organization for low-level behavior modules while being directed by a higher level real-time planner. Despite this supposed interoperability with automated planners, most structured reactive plans are hand coded (Bryson 2001, ch. 3). Examples of structured reactive plans include James Firby's RAP System and the Nils Nilsson's Teleo-reactive plans. PRS, RAPs & TRP are no longer developed or supported. One still-active (as of 2006) descendant of this approach is the Parallel-rooted Ordered Slip-stack Hierarchical (or POSH) action selection system, which is a part of Joanna Bryson's Behaviour Oriented Design.
Sometimes to attempt to address the perceived inflexibility of dynamic planning, hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions. The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately (see further anytime algorithm).

Others
CogniTAO is a decision making engine it based on BDI (belief-desire-intention), it includes built in teamwork capabilities.
Soar is a symbolic cognitive architecture. It is based on condition-action rules known as productions. Programmers can use the Soar development toolkit for building both reactive and planning agents, or any compromise between these two extremes.
Excalibur was a research project led by Alexander Nareyek featuring any-time planning agents for computer games. The architecture is based on structural constraint satisfaction, which is an advanced artificial intelligence technique.
ACT-R is similar to Soar. It includes a Bayesian learning system to help prioritize the productions.
ABL/Hap
Fuzzy architectures The fuzzy approach in action selection produces more smooth behaviour than can be produced by architectures exploiting boolean condition-action rules (like Soar or POSH). These architectures are mostly reactive and symbolic.

Theories of action selection in nature
Many dynamic models of artificial action selection were originally inspired by research in ethology. In particular, Konrad Lorenz and Nikolaas Tinbergen provided the idea of an innate releasing mechanism to explain instinctive behaviors (fixed action patterns). Influenced by the ideas of William McDougall, Lorenz developed this into a "psychohydraulic" model of the motivation of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an energy flow metaphor; the nervous system and the control of behavior are now normally treated as involving information transmission rather than energy flow. Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional / hormonal systems.
Stan Franklin has proposed that action selection is the right perspective to take in understanding the role and evolution of mind. See his page on the action selection paradigm. Archived 2006-10-09 at the Wayback Machine

AI models of neural action selection
Some researchers create elaborate models of neural action selection. See for example:

The Computational Cognitive Neuroscience Lab (CU Boulder).
The Adaptive Behaviour Research Group (Sheffield).

Catecholaminergic Neuron Electron Transport (CNET)
The locus coeruleus (LC) is one of the primary sources of noradrenaline in the brain, and has been associated with selection of cognitive processing, such as attention and behavioral tasks. The substantia nigra pars compacta (SNc) is one of the primary sources of dopamine in the brain, and has been associated with action selection, primarily as part of the basal ganglia.  CNET is a hypothesized neural signaling mechanism in the SNc and LC (which are catecholaminergic neurons), that could assist with action selection by routing energy between neurons in each group as part of action selection, to help one or more neurons in each group to reach action potential. It was first proposed in 2018, and is based on a number of physical parameters of those neurons, which can be broken down into three major components:
1) Ferritin and neuromelanin are present in high concentrations in those neurons, but it was unknown in 2018 whether they formed structures that would be capable of transmitting electrons over relatively long distances on the scale of microns between the largest of those neurons, which had not been previously proposed or observed. Those structures would also need to provide a routing or switching function, which had also not previously been proposed or observed.  Evidence of the presence of ferritin and neuromelanin structures in those neurons and their ability to both conduct electrons by sequential tunneling and to route/switch the path of the neurons was subsequently obtained.
2) ) The axons of large SNc neurons were known to have extensive arbors, but it was unknown whether post-synaptic activity at the synapses of those axons would raise the membrane potential of those neurons sufficiently to cause the electrons to be routed to the neuron or neurons with the most post-synaptic activity for the purpose of action selection.  At the time, prevailing explanations of the purpose of those neurons was that they did not mediate action selection and were only modulatory and non-specific. Prof. Pascal Kaeser of Harvard Medical School subsequently obtained evidence that large SNc neurons can be temporally and spatially specific and mediate action selection.  Other evidence indicates that the large LC axons have similar behavior.
3) Several sources of electrons or excitons to provide the energy for the mechanism were hypothesized in 2018 but had not been observed at that time.  Dioxetane cleavage (which can occur during somatic dopamine metabolism by quinone degradation of melanin) was contemporaneously proposed to generate high energy triplet state electrons by Prof. Doug Brash at Yale, which could provide a source for electrons for the CNET mechanism.
While evidence of a number of physical predictions of the CNET hypothesis has thus been obtained, evidence of whether the hypothesis itself is correct has not been sought. One way to try to determine whether the CNET mechanism is present in these neurons would be to use quantum dot fluorophores and optical probes to determine whether electron tunneling associated with ferritin in the neurons is occurring in association with specific actions.

See also
Action description language – Robot programming language
Artificial intelligence in video games – AI used for video games, usually non-player characters
Cognitive robotics – robot with processing architecture that will allow it to learnPages displaying wikidata descriptions as a fallback
Expert system – Computer system emulating the decision-making ability of a human expert
Inference engine – Component of artificial intelligence systems
Intelligent agent – Software agent which acts autonomously
OPS5 – rule-based or production system computer languagePages displaying wikidata descriptions as a fallback
Production system – computer program typically used to provide some form of artificial intelligencePages displaying wikidata descriptions as a fallback
Reinforcement learning – Field of machine learning
Rete algorithm – Pattern matching algorithm
Utility system – modeling approach for video gamesPages displaying wikidata descriptions as a fallback

References
Further reading
Bratman, M.: Intention, plans, and practical reason. Cambridge, Mass: Harvard University Press (1987)
Brom, C., Lukavský, J., Šerý, O., Poch, T., Šafrata, P.: Affordances and level-of-detail AI for virtual humans. In: Proceedings of Game Set and Match 2, Delft (2006)
Bryson, J.: Intelligence by Design: Principles of Modularity and Coordination for Engineering Complex Adaptive Agents. PhD thesis, Massachusetts Institute of Technology (2001)
Champandard, A. J.: AI Game Development: Synthetic Creatures with learning and Reactive Behaviors. New Riders, USA (2003)
Grand, S., Cliff, D., Malhotra, A.: Creatures: Artificial life autonomous software-agents for home entertainment. In: Johnson, W. L. (eds.): Proceedings of the First International Conference on Autonomous Agents. ACM press (1997) 22-29
Huber, M. J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents'99). Seattle (1999) 236-243
Isla, D.: Handling complexity in Halo 2. In: Gamastura online, 03/11 (2005) Archived 2006-01-08 at the Wayback Machine
Maes, P.: The agent network architecture (ANA). In: SIGART Bulletin, 2 (4), pages 115–120 (1991)
Nareyek, A. Excalibur project
Reynolds, C. W. Flocks, Herds, and Schools: A Distributed Behavioral Model. In: Computer Graphics, 21(4) (SIGGRAPH '87 Conference Proceedings) (1987) 25–34.
de Sevin, E. Thalmann, D.:A motivational Model of Action Selection for Virtual Humans. In: Computer Graphics International (CGI), IEEE Computer SocietyPress, New York (2005)
Tyrrell, T.: Computational Mechanisms for Action Selection. Ph.D. Dissertation. Centre for Cognitive Science, University of Edinburgh (1993)
van Waveren, J. M. P.: The Quake III Arena Bot. Master thesis. Faculty ITS, University of Technology Delft (2001)
Wooldridge, M. An Introduction to MultiAgent Systems. John Wiley & Sons (2002)

External links
The University of Memphis: Agents by action selection Archived 2006-04-18 at the Wayback Machine
Michael Wooldridge: Introduction to agents and their action selection mechanisms
Cyril Brom: Slides on a course on action selection of artificial beings
Soar project. University of Michigan.
Modelling natural action selection, a special issue published by The Royal Society - Philosophical Transactions of the Royal Society
The activation function of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the activation function is nonlinear. Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model, the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al, the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model.

Comparison of activation functions
Aside from their empirical performance, activation functions also have different mathematical properties:

Nonlinear
When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. This is known as the Universal Approximation Theorem. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.
Range
When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.
Continuously differentiable
This property is desirable (ReLU is not continuously differentiable and has some issues with gradient-based optimization, but it is still possible) for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.
These properties do not decisively influence performance, nor are they the only mathematical properties that may be useful. For instance, the strictly positive range of the softplus makes it suitable for predicting variances in variational autoencoders.

Mathematical details
The most common activation functions can be divided into three categories: ridge functions, radial functions and fold functions.
An activation function 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is saturating if 
  
    
      
        
          lim
          
            
              |
            
            v
            
              |
            
            →
            ∞
          
        
        
          |
        
        ∇
        f
        (
        v
        )
        
          |
        
        =
        0
      
    
    {\displaystyle \lim _{|v|\to \infty }|\nabla f(v)|=0}
  
. It is nonsaturating if it is 
  
    
      
        
          lim
          
            
              |
            
            v
            
              |
            
            →
            ∞
          
        
        
          |
        
        ∇
        f
        (
        v
        )
        
          |
        
        ≠
        0
      
    
    {\displaystyle \lim _{|v|\to \infty }|\nabla f(v)|\neq 0}
  
. Non-saturating activation functions, such as ReLU, may be better than saturating activation functions, because they are less likely to suffer from the vanishing gradient problem.

Ridge activation functions
Ridge functions are multivariate functions acting on a linear combination of the input variables. Often used examples include:

Linear activation: 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        a
        +
        
          
            v
          
          ′
        
        
          b
        
      
    
    {\displaystyle \phi (\mathbf {v} )=a+\mathbf {v} '\mathbf {b} }
  
,
ReLU activation: 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        max
        (
        0
        ,
        a
        +
        
          
            v
          
          ′
        
        
          b
        
        )
      
    
    {\displaystyle \phi (\mathbf {v} )=\max(0,a+\mathbf {v} '\mathbf {b} )}
  
,
Heaviside activation: 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        
          1
          
            a
            +
            
              
                v
              
              ′
            
            
              b
            
            >
            0
          
        
      
    
    {\displaystyle \phi (\mathbf {v} )=1_{a+\mathbf {v} '\mathbf {b} >0}}
  
,
Logistic activation: 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        (
        1
        +
        exp
        ⁡
        (
        −
        a
        −
        
          
            v
          
          ′
        
        
          b
        
        )
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \phi (\mathbf {v} )=(1+\exp(-a-\mathbf {v} '\mathbf {b} ))^{-1}}
  
.
In biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell. In its simplest form, this function is binary—that is, either the neuron is firing or not. Neurons also cannot fire faster than a certain rate, motivating sigmoid activation functions whose range is a finite interval. 
The function looks like 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        U
        (
        a
        +
        
          
            v
          
          ′
        
        
          b
        
        )
      
    
    {\displaystyle \phi (\mathbf {v} )=U(a+\mathbf {v} '\mathbf {b} )}
  
, where 
  
    
      
        U
      
    
    {\displaystyle U}
  
 is the Heaviside step function. 
If a line has a positive slope, on the other hand, it may reflect the increase in firing rate that occurs as input current increases. Such a function would be of the form 
  
    
      
        ϕ
        (
        
          v
        
        )
        =
        a
        +
        
          
            v
          
          ′
        
        
          b
        
      
    
    {\displaystyle \phi (\mathbf {v} )=a+\mathbf {v} '\mathbf {b} }
  
.

Radial activation functions
A special class of activation functions known as radial basis functions (RBFs) are used in RBF networks, which are extremely efficient as universal function approximators. These activation functions can take many forms, but they are usually found as one of the following functions:

Gaussian: 
  
    
      
        
        ϕ
        (
        
          v
        
        )
        =
        exp
        ⁡
        
          (
          
            −
            
              
                
                  ‖
                  
                    v
                  
                  −
                  
                    c
                  
                  
                    ‖
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
            
          
          )
        
      
    
    {\displaystyle \,\phi (\mathbf {v} )=\exp \left(-{\frac {\|\mathbf {v} -\mathbf {c} \|^{2}}{2\sigma ^{2}}}\right)}
  

Multiquadratics: 
  
    
      
        
        ϕ
        (
        
          v
        
        )
        =
        
          
            ‖
            
              v
            
            −
            
              c
            
            
              ‖
              
                2
              
            
            +
            
              a
              
                2
              
            
          
        
      
    
    {\displaystyle \,\phi (\mathbf {v} )={\sqrt {\|\mathbf {v} -\mathbf {c} \|^{2}+a^{2}}}}
  

Inverse multiquadratics: 
  
    
      
        
        ϕ
        (
        
          v
        
        )
        =
        
          
            (
            
              ‖
              
                v
              
              −
              
                c
              
              
                ‖
                
                  2
                
              
              +
              
                a
                
                  2
                
              
            
            )
          
          
            −
            
              
                1
                2
              
            
          
        
      
    
    {\displaystyle \,\phi (\mathbf {v} )=\left(\|\mathbf {v} -\mathbf {c} \|^{2}+a^{2}\right)^{-{\frac {1}{2}}}}
  

Polyharmonic splines
where 
  
    
      
        
          c
        
      
    
    {\displaystyle \mathbf {c} }
  
 is the vector representing the function center and 
  
    
      
        a
      
    
    {\displaystyle a}
  
 and 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
 are parameters affecting the spread of the radius.

Folding activation functions
Folding activation functions are extensively used in the pooling layers in convolutional neural networks, and in output layers of multiclass classification networks. These activations perform aggregation over the inputs, such as taking the mean, minimum or maximum. In multiclass classification the softmax activation is often used.

Table of activation functions
The following table compares the properties of several activation functions that are functions of one fold x from the previous layer or layers:

The following table lists activation functions that are not functions of a single fold x from the previous layer or layers:

^  Here, 
  
    
      
        
          δ
          
            i
            j
          
        
      
    
    {\displaystyle \delta _{ij}}
  
 is the Kronecker delta.
^  For instance, 
  
    
      
        j
      
    
    {\displaystyle j}
  
 could be iterating through the number of kernels of the previous neural network layer while 
  
    
      
        i
      
    
    {\displaystyle i}
  
 iterates through the number of kernels of the current layer.

Quantum activation functions
In quantum neural networks programmed on gate-model quantum computers, based on quantum perceptrons instead of variational quantum circuits, the non-linearity of the activation function can be implemented with no need of measuring the output of each perceptron at each layer. The quantum properties loaded within the circuit such as superposition can be preserved by creating the Taylor series of the argument computed by the perceptron itself, with suitable quantum circuits computing the powers up to a wanted approximation degree. Because of the flexibility of such quantum circuits, they can be designed in order to approximate any arbitrary classical activation function.

See also
Logistic function
Rectifier (neural networks)
Stability (learning theory)
Softmax function


== References ==
Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.
There are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.
Large-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.

Definitions
Let T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.
During each iteration, i, T is broken up into three subsets

  
    
      
        
          
            T
          
          
            K
            ,
            i
          
        
      
    
    {\displaystyle \mathbf {T} _{K,i}}
  
: Data points where the label is known.

  
    
      
        
          
            T
          
          
            U
            ,
            i
          
        
      
    
    {\displaystyle \mathbf {T} _{U,i}}
  
: Data points where the label is unknown.

  
    
      
        
          
            T
          
          
            C
            ,
            i
          
        
      
    
    {\displaystyle \mathbf {T} _{C,i}}
  
: A subset of TU,i that is chosen to be labeled.
Most of the current research in active learning involves the best method to choose the data points for TC,i.

Scenarios
Pool-Based Sampling: In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner "understands" the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory.
Stream-Based Selective Sampling: Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint. As contrasted with Pool-based sampling, the obvious drawback of stream-based methods is that the learning algorithm does not have sufficient information, early in the process, to make a sound assign-label-vs ask-teacher decision, and it does not capitalize as efficiently on the presence of already labeled data. Therefore, the teacher is likely to spend more effort in supplying labels than with the pool-based approach.
Membership Query Synthesis: This is where the learner generates synthetic data from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small. The challenge here, as with all synthetic-data-generation efforts, is in ensuring that the synthetic data is consistent in terms of meeting the constraints on real data. As the number of variables/features in the input data increase, and strong dependencies between variables exist, it becomes increasingly difficult to generate synthetic data with sufficient fidelity. For example, to create a synthetic data set for human laboratory-test values, the sum of the various white blood cell (WBC) components in a White Blood Cell differential must equal 100, since the component numbers are really percentages. Similarly, the enzymes Alanine Transaminase (ALT) and Aspartate Transaminase (AST) measure liver function (though AST is also produced by other tissues, e.g., lung, pancreas) A synthetic data point with AST at the lower limit of normal range (8-33 Units/L) with an ALT several times above normal range (4-35 Units/L) in a simulated chronically ill patient would be physiologically impossible.

Query strategies
Algorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:

Balance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.
Expected model change: label those points that would most change the current model.
Expected error reduction: label those points that would most reduce the model's generalization error.
Exponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.
Random Sampling: a sample is randomly selected.
Uncertainty sampling: label those points for which the current model is least certain as to what the correct output should be.
Entropy Sampling: The entropy formula is used on each sample, and the sample with the highest entropy is considered to be the least certain.
Margin Sampling: The sample with the smallest difference between the two highest class probabilities is considered to be the most uncertain.
Least Confident Sampling: The sample with the smallest best probability is considered to be the most uncertain.
Query by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the "committee" disagrees the most
Querying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.
Variance reduction: label those points that would minimize output variance, which is one of the components of error.
Conformal prediction: predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.
Mismatch-first farthest-traversal: The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data.
User Centered Labeling Strategies: Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances.
A wide variety of algorithms have been studied that fall into these categories. While the  traditional AL  strategies can  achieve  remarkable  performance, it  is often challenging to predict in advance which strategy is the most suitable in aparticular situation. In recent years, meta-learning algorithms have been gaining in popularity. Some of them have been proposed to tackle the problem of learning AL strategies instead of relying on manually designed strategies. A benchmark which compares 'meta-learning approaches to active learning' to 'traditional heuristic-based Active Learning' may give intuitions if 'Learning active learning' is at the crossroads

Minimum marginal hyperplane
Some active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, W, of each unlabeled datum in TU,i and treat W as an n-dimensional distance from that datum to the separating hyperplane.
Minimum Marginal Hyperplane methods assume that the data with the smallest W are those that the SVM is most uncertain about and therefore should be placed in TC,i to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest W. Tradeoff methods choose a mix of the smallest and largest Ws.

See also
List of datasets for machine learning research
Sample complexity
Bayesian Optimization
Reinforcement learning

Literature
Improving Generalization with Active Learning, David Cohn, Les Atlas & Richard Ladner, Machine Learning 15, 201–221 (1994). https://doi.org/10.1007/BF00993277
Balcan, Maria-Florina & Hanneke, Steve & Wortman, Jennifer. (2008). The True Sample Complexity of Active Learning.. 45-56. https://link.springer.com/article/10.1007/s10994-010-5174-y
Active Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal, Francesco Di Fiore, Michela Nardelli, Laura Mainini, https://arxiv.org/abs/2303.01560v2
Learning how to Active Learn: A Deep Reinforcement Learning Approach, Meng Fang, Yuan Li, Trevor Cohn, https://arxiv.org/abs/1708.02383v1


== References ==
An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.

Overview
An adaptive website adjusts the structure, content, or presentation of information in response to measured user interaction with the site, with the objective of optimizing future user interactions. Adaptive websites "are web sites that automatically improve their organization and presentation by learning from their user access patterns." User interaction patterns may be collected directly on the website or may be mined from Web server logs. A model or models are created of user interaction using artificial intelligence and statistical methods. The models are used as the basis for tailoring the website for known and specific patterns of user interaction.

Techniques
The collaborative filtering method: Collected user data may be assessed in aggregate (across multiple users) using machine learning techniques to cluster interaction patterns to user models and classify specific user patterns to such models. The website may then be adapted to target clusters of users. In this approach, the models are explicitly created from historic user information with new users are classified to an existing model and a pre-defined mapping is used for existing content and content organization.
The statistical hypothesis testing method: A/B testing or similar methods are used in conjunction with a library of possible changes to the website or a change-generation method (such as random variation). This results in the automated process website change, impact assessment, and adoption of change. Some examples include genetify for website look and feel, and snap ads for online advertising. In this approach (specifically genetify), the model is represented implicitly in the population of possible sites and adapted for all users that visit the site.

Differentiation
User landing pages (such as iGoogle) that allow the user to customize the presented content are not adaptive websites as they rely on the user to select rather than the automation of the selection and presentation of the web widget's that appear on the website.
Collaborative filtering such as recommender systems, generate and test methods such as A/B testing, and machine learning techniques such as clustering and classification that are used on a website do not make it an adaptive website. They are all tools and techniques that may be used toward engineering an adaptive website.

See also
Machine learning
Responsive web design
Web intelligence

Notes
References
J.D. Velásquez and V. Palade, "Adaptive Web Sites: A Knowledge Extraction from Web Data Approach", IOS Press, 2008
Mike Perkowitz, Oren Etzioni, "Towards adaptive Web sites: Conceptual framework and case study", Artificial Intelligence 118(1-2), 2000
Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.
Most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.
Most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.

History
At the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.
In 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple "evasion attacks" as spammers inserted "good words" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within "image spam" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published "Can Machine Learning Be Secure?", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.
Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain's Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches.
While adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.

Examples
Examples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of "bad" words or the insertion of "good" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users' template galleries that adapt to updated traits over time.
Researchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle with a texture engineered to make Google's object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology.
A machine-tweaked image of a dog was shown to look like a cat to both computers and humans. A 2019 study reported that humans can guess how machines will classify adversarial images. Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign.
McAfee attacked Tesla's former Mobileye system, fooling it into driving 50 mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign.
Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of "stealth streetwear".
An adversarial attack on a neural network can allow an attacker to inject algorithms into the target system. Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio; a parallel literature explores human perception of such stimuli.
Clustering algorithms are used in security applications. Malware and computer virus analysis aims to identify malware families, and to generate specific detection signatures.

Attack modalities
Taxonomy
Attacks against (supervised) machine learning algorithms have been categorized along three primary axes: influence on the classifier, the security violation and their specificity.

Classifier influence: An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attacker's capabilities might be restricted by the presence of data manipulation constraints.
Security violation: An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training.
Specificity: A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem.
This taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary's goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy. This taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks.

Strategies
Below are some of the most commonly encountered attack scenarios.

Data poisoning
Poisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are shaped by their training datasets, poisoning can effectively reprogram algorithms with potentially malicious intent. Concerns have been raised especially for user-generated training data, e.g. for content recommendation or natural language models. The ubiquity of fake accounts offers many opportunities for poisoning. Facebook reportedly removes around 7 billion fake accounts per year. Poisoning has been reported as the leading concern for industrial applications.
On social medias, disinformation campaigns attempt to bias recommendation and moderation algorithms, to push certain content over others.
A particular case of data poisoning is the backdoor attack, which aims to teach a specific behavior for inputs with a given trigger, e.g. a small defect on images, sounds, videos or texts.
For instance, intrusion detection systems are often trained using collected data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining.
Data poisoning techniques can also be applied to text-to-image models to alter their output.
Data poisoning can also happen unintentionally through model collapse, where models are trained on synthetic data.

Byzantine attacks
As machine learning is scaled, it often relies on multiple computing machines. In federated learning, for instance, edge devices collaborate with a central server, typically by sending gradients or model parameters. However, some of these devices may deviate from their expected behavior, e.g. to harm the central server's model or to bias algorithms towards certain behaviors (e.g., amplifying the recommendation of disinformation content). On the other hand, if the training is performed on a single machine, then the model is very vulnerable to a failure of the machine, or an attack on the machine; the machine is a single point of failure. In fact, the machine owner may themselves insert provably undetectable backdoors.
The current leading solutions to make (distributed) learning algorithms provably resilient to a minority of malicious (a.k.a. Byzantine) participants are based on robust gradient aggregation rules. The robust aggregation rules do not always work especially when the data across participants has a non-iid distribution. Nevertheless, in the context of heterogeneous honest participants, such as users with different consumption habits for recommendation algorithms or writing styles for language models, there are provable impossibility theorems on what any robust learning algorithm can guarantee.

Evasion
Evasion attacks consist of exploiting the imperfection of a trained model. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware. Samples are modified to evade detection; that is, to be classified as legitimate. This does not involve influence over the training data. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.
Evasion attacks can be generally split into two different categories: black box attacks and white box attacks.

Model extraction
Model extraction involves an adversary probing a black box machine learning system in order to extract the data it was trained on.  This can cause issues when either the training data or the model itself is sensitive and confidential. For example, model extraction could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit.
In the extreme case, model extraction can lead to model stealing, which corresponds to extracting a sufficient amount of data from the model to enable the complete reconstruction of the model.
On the other hand, membership inference is a targeted model extraction attack, which infers the owner of a data point, often by leveraging the overfitting resulting from poor machine learning practices. Concerningly, this is sometimes achievable even without knowledge or access to a target model's parameters, raising security concerns for models trained on sensitive data, including but not limited to medical records and/or personally identifiable information. With the emergence of transfer learning and public accessibility of many state of the art machine learning models, tech companies are increasingly drawn to create models based on public ones, giving attackers freely accessible information to the structure and type of model being used.

Categories
Adversarial deep reinforcement learning
Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area, some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.

Adversarial natural language processing
Adversarial attacks on speech recognition have been introduced for speech-to-text applications, in particular for Mozilla's implementation of DeepSpeech.

Adversarial attacks and training in linear models
There is a growing literature about adversarial attacks in 
linear models. Indeed, since the seminal work from Goodfellow at al.  studying these models in linear models has been an important tool to understand how adversarial attacks affect machine learning models. 
The analysis of these models is simplified because the computation of adversarial attacks can be simplified in linear regression and classification problems. Moreover, adversarial training is convex in this case. 
Linear models allow for analytical analysis while still reproducing phenomena observed in state-of-the-art models.
One prime example of that is how this model can be used to explain the trade-off between robustness and accuracy. 
Diverse work indeed provides analysis of adversarial attacks in linear models, including asymptotic analysis for  classification  and for linear regression. And, finite-sample analysis based on Rademacher complexity.

Specific attack types
There are a large variety of different adversarial attacks that can be used against machine learning systems. Many of these work on both deep learning systems as well as traditional machine learning models such as SVMs and  linear regression. A high level sample of these attack types include:

Adversarial Examples
Trojan Attacks / Backdoor Attacks
Model Inversion
Membership Inference

Adversarial examples
An adversarial example refers to specially crafted input that is designed to look "normal" to humans but causes misclassification to a machine learning model.  Often, a form of specially designed "noise"  is used to elicit the misclassifications. Below are some current techniques for generating adversarial examples in the literature (by no means an exhaustive list).

Gradient-based evasion attack
Fast Gradient Sign Method (FGSM)
Projected Gradient Descent (PGD)
Carlini and Wagner (C&W) attack
Adversarial patch attack

Black box attacks
Black box attacks in adversarial machine learning assume that the adversary can only get outputs for provided inputs and has no knowledge of the model structure or parameters. In this case, the adversarial example is generated either using a model created from scratch, or without any model at all (excluding the ability to query the original model). In either case, the objective of these attacks is to create adversarial examples that are able to transfer to the black box model in question.

Simple Black-box Adversarial Attacks
Simple Black-box Adversarial Attacks were proposed in 2019 as a query-efficient way attack black-box image classifiers. Take a random orthonormal basis 
  
    
      
        
          v
          
            1
          
        
        ,
        
          v
          
            2
          
        
        ,
        …
        ,
        
          v
          
            d
          
        
      
    
    {\displaystyle v_{1},v_{2},\dots ,v_{d}}
  
 in 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  
. The authors suggested the discrete cosine transform of the standard basis (the pixels).  
For a correctly classified image 
  
    
      
        x
      
    
    {\displaystyle x}
  
, try 
  
    
      
        x
        +
        ϵ
        
          v
          
            1
          
        
        ,
        x
        −
        ϵ
        
          v
          
            1
          
        
      
    
    {\displaystyle x+\epsilon v_{1},x-\epsilon v_{1}}
  
, and compare the amount of error in the classifier upon 
  
    
      
        x
        +
        ϵ
        
          v
          
            1
          
        
        ,
        x
        ,
        x
        −
        ϵ
        
          v
          
            1
          
        
      
    
    {\displaystyle x+\epsilon v_{1},x,x-\epsilon v_{1}}
  
. Pick the one that causes the largest amount of error.

Repeat this for 
  
    
      
        
          v
          
            2
          
        
        ,
        
          v
          
            3
          
        
        ,
        …
      
    
    {\displaystyle v_{2},v_{3},\dots }
  
 until the desired level of error in the classifier is reached.The algorithm was discovered when the authors were trying to compare a previous black-box adversarial attack algorithm, based on gaussian processes, with a simple baseline, and found that their baseline turned out to work even better.

Square Attack
The Square Attack was introduced in 2020 as a black box evasion adversarial attack based on querying classification scores without the need of gradient information. As a score based black box attack, this adversarial approach is able to query probability distributions across model output classes, but has no other access to the model itself. According to the paper's authors, the proposed Square Attack required fewer queries than when compared to state-of-the-art score-based black box attacks at the time.
To describe the function objective, the attack defines the classifier as 
  
    
      
        f
        :
        [
        0
        ,
        1
        
          ]
          
            d
          
        
        →
        
          
            R
          
          
            K
          
        
      
    
    {\textstyle f:[0,1]^{d}\rightarrow \mathbb {R} ^{K}}
  
, with 
  
    
      
        d
      
    
    {\textstyle d}
  
 representing the dimensions of the input and 
  
    
      
        K
      
    
    {\textstyle K}
  
 as the total number of output classes. 
  
    
      
        
          f
          
            k
          
        
        (
        x
        )
      
    
    {\textstyle f_{k}(x)}
  
 returns the score (or a probability between 0 and 1) that the input 
  
    
      
        x
      
    
    {\textstyle x}
  
 belongs to class 
  
    
      
        k
      
    
    {\textstyle k}
  
, which allows the classifier's class output for any input 
  
    
      
        x
      
    
    {\textstyle x}
  
 to be defined as 
  
    
      
        
          
            argmax
          
          
            k
            =
            1
            ,
            .
            .
            .
            ,
            K
          
        
        
          f
          
            k
          
        
        (
        x
        )
      
    
    {\textstyle {\text{argmax}}_{k=1,...,K}f_{k}(x)}
  
. The goal of this attack is as follows:

  
    
      
        
          
            argmax
          
          
            k
            =
            1
            ,
            .
            .
            .
            ,
            K
          
        
        
          f
          
            k
          
        
        (
        
          
            
              x
              ^
            
          
        
        )
        ≠
        y
        ,
        
          |
        
        
          |
        
        
          
            
              x
              ^
            
          
        
        −
        x
        
          |
        
        
          
            |
          
          
            p
          
        
        ≤
        ϵ
        
           and 
        
        
          
            
              x
              ^
            
          
        
        ∈
        [
        0
        ,
        1
        
          ]
          
            d
          
        
      
    
    {\displaystyle {\text{argmax}}_{k=1,...,K}f_{k}({\hat {x}})\neq y,||{\hat {x}}-x||_{p}\leq \epsilon {\text{ and }}{\hat {x}}\in [0,1]^{d}}
  

In other words, finding some perturbed adversarial example 
  
    
      
        
          
            
              x
              ^
            
          
        
      
    
    {\textstyle {\hat {x}}}
  
 such that the classifier incorrectly classifies it to some other class under the constraint that 
  
    
      
        
          
            
              x
              ^
            
          
        
      
    
    {\textstyle {\hat {x}}}
  
 and 
  
    
      
        x
      
    
    {\textstyle x}
  
 are similar. The paper then defines loss 
  
    
      
        L
      
    
    {\textstyle L}
  
 as 
  
    
      
        L
        (
        f
        (
        
          
            
              x
              ^
            
          
        
        )
        ,
        y
        )
        =
        
          f
          
            y
          
        
        (
        
          
            
              x
              ^
            
          
        
        )
        −
        
          max
          
            k
            ≠
            y
          
        
        
          f
          
            k
          
        
        (
        
          
            
              x
              ^
            
          
        
        )
      
    
    {\textstyle L(f({\hat {x}}),y)=f_{y}({\hat {x}})-\max _{k\neq y}f_{k}({\hat {x}})}
  
 and proposes the solution to finding adversarial example 
  
    
      
        
          
            
              x
              ^
            
          
        
      
    
    {\textstyle {\hat {x}}}
  
 as solving the below constrained optimization problem:

  
    
      
        
          min
          
            
              
                
                  x
                  ^
                
              
            
            ∈
            [
            0
            ,
            1
            
              ]
              
                d
              
            
          
        
        L
        (
        f
        (
        
          
            
              x
              ^
            
          
        
        )
        ,
        y
        )
        ,
        
           s.t. 
        
        
          |
        
        
          |
        
        
          
            
              x
              ^
            
          
        
        −
        x
        
          |
        
        
          
            |
          
          
            p
          
        
        ≤
        ϵ
      
    
    {\displaystyle \min _{{\hat {x}}\in [0,1]^{d}}L(f({\hat {x}}),y),{\text{ s.t. }}||{\hat {x}}-x||_{p}\leq \epsilon }
  

The result in theory is an adversarial example that is highly confident in the incorrect class but is also very similar to the original image. To find such example, Square Attack utilizes the iterative random search technique to randomly perturb the image in hopes of improving the objective function. In each step, the algorithm perturbs only a small square section of pixels, hence the name Square Attack, which terminates as soon as an adversarial example is found in order to improve query efficiency. Finally, since the attack algorithm uses scores and not gradient information, the authors of the paper indicate that this approach is not affected by gradient masking, a common technique formerly used to prevent evasion attacks.

HopSkipJump Attack
This black box attack was also proposed as a query efficient attack, but one that relies solely on access to any input's predicted output class. In other words, the HopSkipJump attack does not require the ability to calculate gradients or access to score values like the Square Attack, and will require just the model's class prediction output (for any given input). The proposed attack is split into two different settings, targeted and untargeted, but both are built from the general idea of adding minimal perturbations that leads to a different model output. In the targeted setting, the goal is to cause the model to misclassify the perturbed image to a specific target label (that is not the original label). In the untargeted setting, the goal is to cause the model to misclassify the perturbed image to any label that is not the original label. The attack objectives for both are as follows where 
  
    
      
        x
      
    
    {\textstyle x}
  
 is the original image, 
  
    
      
        
          x
          
            ′
          
        
      
    
    {\textstyle x^{\prime }}
  
 is the adversarial image, 
  
    
      
        d
      
    
    {\textstyle d}
  
 is a distance function between images, 
  
    
      
        
          c
          
            ∗
          
        
      
    
    {\textstyle c^{*}}
  
 is the target label, and 
  
    
      
        C
      
    
    {\textstyle C}
  
 is the model's classification class label function:

  
    
      
        
          
            Targeted:
          
        
        
          min
          
            
              x
              
                ′
              
            
          
        
        d
        (
        
          x
          
            ′
          
        
        ,
        x
        )
        
           subject to 
        
        C
        (
        
          x
          
            ′
          
        
        )
        =
        
          c
          
            ∗
          
        
      
    
    {\displaystyle {\textbf {Targeted:}}\min _{x^{\prime }}d(x^{\prime },x){\text{ subject to }}C(x^{\prime })=c^{*}}
  

  
    
      
        
          
            Untargeted:
          
        
        
          min
          
            
              x
              
                ′
              
            
          
        
        d
        (
        
          x
          
            ′
          
        
        ,
        x
        )
        
           subject to 
        
        C
        (
        
          x
          
            ′
          
        
        )
        ≠
        C
        (
        x
        )
      
    
    {\displaystyle {\textbf {Untargeted:}}\min _{x^{\prime }}d(x^{\prime },x){\text{ subject to }}C(x^{\prime })\neq C(x)}
  

To solve this problem, the attack proposes the following boundary function 
  
    
      
        S
      
    
    {\textstyle S}
  
 for both the untargeted and targeted setting:

  
    
      
        S
        (
        
          x
          
            ′
          
        
        )
        :=
        
          
            {
            
              
                
                  
                    max
                    
                      c
                      ≠
                      C
                      (
                      x
                      )
                    
                  
                  
                    F
                    (
                    
                      x
                      
                        ′
                      
                    
                    
                      )
                      
                        c
                      
                    
                  
                  −
                  F
                  (
                  
                    x
                    
                      ′
                    
                  
                  
                    )
                    
                      C
                      (
                      x
                      )
                    
                  
                  ,
                
                
                  
                    (Untargeted)
                  
                
              
              
                
                  F
                  (
                  
                    x
                    
                      ′
                    
                  
                  
                    )
                    
                      
                        c
                        
                          ∗
                        
                      
                    
                  
                  −
                  
                    max
                    
                      c
                      ≠
                      
                        c
                        
                          ∗
                        
                      
                    
                  
                  
                    F
                    (
                    
                      x
                      
                        ′
                      
                    
                    
                      )
                      
                        c
                      
                    
                  
                  ,
                
                
                  
                    (Targeted)
                  
                
              
            
            
          
        
      
    
    {\displaystyle S(x^{\prime }):={\begin{cases}\max _{c\neq C(x)}{F(x^{\prime })_{c}}-F(x^{\prime })_{C(x)},&{\text{(Untargeted)}}\\F(x^{\prime })_{c^{*}}-\max _{c\neq c^{*}}{F(x^{\prime })_{c}},&{\text{(Targeted)}}\end{cases}}}
  

This can be further simplified to better visualize the boundary between different potential adversarial examples:

  
    
      
        S
        (
        
          x
          
            ′
          
        
        )
        >
        0
        
        ⟺
        
        
          
            {
            
              
                
                  a
                  r
                  g
                  m
                  a
                  
                    x
                    
                      c
                    
                  
                  F
                  (
                  
                    x
                    
                      ′
                    
                  
                  )
                  ≠
                  C
                  (
                  x
                  )
                  ,
                
                
                  
                    (Untargeted)
                  
                
              
              
                
                  a
                  r
                  g
                  m
                  a
                  
                    x
                    
                      c
                    
                  
                  F
                  (
                  
                    x
                    
                      ′
                    
                  
                  )
                  =
                  
                    c
                    
                      ∗
                    
                  
                  ,
                
                
                  
                    (Targeted)
                  
                
              
            
            
          
        
      
    
    {\displaystyle S(x^{\prime })>0\iff {\begin{cases}argmax_{c}F(x^{\prime })\neq C(x),&{\text{(Untargeted)}}\\argmax_{c}F(x^{\prime })=c^{*},&{\text{(Targeted)}}\end{cases}}}
  

With this boundary function, the attack then follows an iterative algorithm to find adversarial examples 
  
    
      
        
          x
          
            ′
          
        
      
    
    {\textstyle x^{\prime }}
  
 for a given image 
  
    
      
        x
      
    
    {\textstyle x}
  
 that satisfies the attack objectives.

Initialize 
  
    
      
        x
      
    
    {\textstyle x}
  
 to some point where 
  
    
      
        S
        (
        x
        )
        >
        0
      
    
    {\textstyle S(x)>0}
  

Iterate below
Boundary search
Gradient update
Compute the gradient
Find the step size
Boundary search uses a modified binary search to find the point in which the boundary (as defined by 
  
    
      
        S
      
    
    {\textstyle S}
  
) intersects with the line between 
  
    
      
        x
      
    
    {\textstyle x}
  
 and 
  
    
      
        
          x
          
            ′
          
        
      
    
    {\textstyle x^{\prime }}
  
. The next step involves calculating the gradient for 
  
    
      
        x
      
    
    {\textstyle x}
  
, and update the original 
  
    
      
        x
      
    
    {\textstyle x}
  
 using this gradient and a pre-chosen step size. HopSkipJump authors prove that this iterative algorithm will converge, leading 
  
    
      
        x
      
    
    {\textstyle x}
  
 to a point right along the boundary that is very close in distance to the original image.
However, since HopSkipJump is a proposed black box attack and the iterative algorithm above requires the calculation of a gradient in the second iterative step (which black box attacks do not have access to), the authors propose a solution to gradient calculation that requires only the model's output predictions alone. By generating many random vectors in all directions, denoted as 
  
    
      
        
          u
          
            b
          
        
      
    
    {\textstyle u_{b}}
  
, an approximation of the gradient can be calculated using the average of these random vectors weighted by the sign of the boundary function on the image 
  
    
      
        
          x
          
            ′
          
        
        +
        
          δ
          
            
              u
              
                b
              
            
          
        
      
    
    {\textstyle x^{\prime }+\delta _{u_{b}}}
  
, where 
  
    
      
        
          δ
          
            
              u
              
                b
              
            
          
        
      
    
    {\textstyle \delta _{u_{b}}}
  
 is the size of the random vector perturbation:

  
    
      
        ∇
        S
        (
        
          x
          
            ′
          
        
        ,
        δ
        )
        ≈
        
          
            1
            B
          
        
        
          ∑
          
            b
            =
            1
          
          
            B
          
        
        ϕ
        (
        
          x
          
            ′
          
        
        +
        
          δ
          
            
              u
              
                b
              
            
          
        
        )
        
          u
          
            b
          
        
      
    
    {\displaystyle \nabla S(x^{\prime },\delta )\approx {\frac {1}{B}}\sum _{b=1}^{B}\phi (x^{\prime }+\delta _{u_{b}})u_{b}}
  

The result of the equation above gives a close approximation of the gradient required in step 2 of the iterative algorithm, completing HopSkipJump as a black box attack.

White box attacks
White box attacks assumes that the adversary has access to model parameters on top of being able to get labels for provided inputs.

Fast gradient sign method
One of the first proposed attacks for generating adversarial examples was proposed by Google researchers Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. The attack was called fast gradient sign method (FGSM), and it consists of adding a linear amount of in-perceivable noise to the image and causing a model to incorrectly classify it. This noise is calculated by multiplying the sign of the gradient with respect to the image we want to perturb by a small constant epsilon. As epsilon increases, the model is more likely to be fooled, but the perturbations become easier to identify as well. Shown below is the equation to generate an adversarial example where 
  
    
      
        x
      
    
    {\textstyle x}
  
 is the original image, 
  
    
      
        ϵ
      
    
    {\textstyle \epsilon }
  
 is a very small number, 
  
    
      
        
          Δ
          
            x
          
        
      
    
    {\textstyle \Delta _{x}}
  
 is the gradient function, 
  
    
      
        J
      
    
    {\textstyle J}
  
 is the loss function, 
  
    
      
        θ
      
    
    {\textstyle \theta }
  
 is the model weights, and 
  
    
      
        y
      
    
    {\textstyle y}
  
 is the true label.

  
    
      
        a
        d
        
          v
          
            x
          
        
        =
        x
        +
        ϵ
        ⋅
        s
        i
        g
        n
        (
        
          Δ
          
            x
          
        
        J
        (
        θ
        ,
        x
        ,
        y
        )
        )
      
    
    {\displaystyle adv_{x}=x+\epsilon \cdot sign(\Delta _{x}J(\theta ,x,y))}
  

One important property of this equation is that the gradient is calculated with respect to the input image since the goal is to generate an image that maximizes the loss for the original image of true label 
  
    
      
        y
      
    
    {\textstyle y}
  
. In traditional gradient descent (for model training), the gradient is used to update the weights of the model since the goal is to minimize the loss for the model on a ground truth dataset. The Fast Gradient Sign Method was proposed as a fast way to generate adversarial examples to evade the model, based on the hypothesis that neural networks cannot resist even linear amounts of perturbation to the input. FGSM has shown to be effective in adversarial attacks for image classification and skeletal action recognition.

Carlini & Wagner (C&W)
In an effort to analyze existing adversarial attacks and defenses, researchers at the University of California, Berkeley, Nicholas Carlini and David Wagner in 2016 propose a faster and more robust method to generate adversarial examples.
The attack proposed by Carlini and Wagner begins with trying to solve a difficult non-linear optimization equation:

  
    
      
        min
        (
        
          |
        
        
          |
        
        δ
        
          |
        
        
          
            |
          
          
            p
          
        
        )
        
           subject to 
        
        C
        (
        x
        +
        δ
        )
        =
        t
        ,
        x
        +
        δ
        ∈
        [
        0
        ,
        1
        
          ]
          
            n
          
        
      
    
    {\displaystyle \min(||\delta ||_{p}){\text{ subject to }}C(x+\delta )=t,x+\delta \in [0,1]^{n}}
  

Here the objective is to minimize the noise (
  
    
      
        δ
      
    
    {\textstyle \delta }
  
), added to the original input 
  
    
      
        x
      
    
    {\textstyle x}
  
, such that the machine learning algorithm (
  
    
      
        C
      
    
    {\textstyle C}
  
) predicts the original input with delta (or 
  
    
      
        x
        +
        δ
      
    
    {\textstyle x+\delta }
  
) as some other class 
  
    
      
        t
      
    
    {\textstyle t}
  
. However instead of directly the above equation, Carlini and Wagner propose using a new function 
  
    
      
        f
      
    
    {\textstyle f}
  
 such that:

  
    
      
        C
        (
        x
        +
        δ
        )
        =
        t
        
        ⟺
        
        f
        (
        x
        +
        δ
        )
        ≤
        0
      
    
    {\displaystyle C(x+\delta )=t\iff f(x+\delta )\leq 0}
  

This condenses the first equation to the problem below:

  
    
      
        min
        (
        
          |
        
        
          |
        
        δ
        
          |
        
        
          
            |
          
          
            p
          
        
        )
        
           subject to 
        
        f
        (
        x
        +
        δ
        )
        ≤
        0
        ,
        x
        +
        δ
        ∈
        [
        0
        ,
        1
        
          ]
          
            n
          
        
      
    
    {\displaystyle \min(||\delta ||_{p}){\text{ subject to }}f(x+\delta )\leq 0,x+\delta \in [0,1]^{n}}
  

and even more to the equation below:

  
    
      
        min
        (
        
          |
        
        
          |
        
        δ
        
          |
        
        
          
            |
          
          
            p
          
        
        +
        c
        ⋅
        f
        (
        x
        +
        δ
        )
        )
        ,
        x
        +
        δ
        ∈
        [
        0
        ,
        1
        
          ]
          
            n
          
        
      
    
    {\displaystyle \min(||\delta ||_{p}+c\cdot f(x+\delta )),x+\delta \in [0,1]^{n}}
  

Carlini and Wagner then propose the use of the below function in place of 
  
    
      
        f
      
    
    {\textstyle f}
  
 using 
  
    
      
        Z
      
    
    {\textstyle Z}
  
, a function that determines class probabilities for given input 
  
    
      
        x
      
    
    {\textstyle x}
  
. When substituted in, this equation can be thought of as finding a target class that is more confident than the next likeliest class by some constant amount:

  
    
      
        f
        (
        x
        )
        =
        (
        [
        
          max
          
            i
            ≠
            t
          
        
        Z
        (
        x
        
          )
          
            i
          
        
        ]
        −
        Z
        (
        x
        
          )
          
            t
          
        
        
          )
          
            +
          
        
      
    
    {\displaystyle f(x)=([\max _{i\neq t}Z(x)_{i}]-Z(x)_{t})^{+}}
  

When solved using gradient descent, this equation is able to produce stronger adversarial examples when compared to fast gradient sign method that is also able to bypass defensive distillation, a defense that was once proposed to be effective against adversarial examples.

Defenses
Researchers have proposed a multi-step approach to protecting machine learning.

Threat modeling – Formalize the attackers goals and capabilities with respect to the target system.
Attack simulation – Formalize the optimization problem the attacker tries to solve according to possible attack strategies.
Attack impact evaluation
Countermeasure design
Noise detection (For evasion based attack)
Information laundering – Alter the information received by adversaries (for model stealing attacks)

Mechanisms
A number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including:

Secure learning algorithms
Byzantine-resilient algorithms
Multiple classifier systems
AI-written algorithms.
AIs that explore the training environment; for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images.
Privacy-preserving learning
Ladder algorithm for Kaggle-style competitions
Game theoretic models
Sanitizing training data
Adversarial training
Backdoor detection algorithms
Gradient masking/obfuscation techniques: to prevent the adversary exploiting the gradient in white-box attacks. This family of defenses is deemed unreliable as these models are still vulnerable to black-box attacks or can be circumvented in other ways.
Ensembles of models have been proposed in the literature but caution should be applied when relying on them: usually ensembling weak classifiers results in a more accurate model but it does not seem to apply in the adversarial context.

See also
Pattern recognition
Fawkes (image cloaking software)
Generative adversarial network

References
External links
MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems
NIST 8269 Draft: A Taxonomy and Terminology of Adversarial Machine Learning
NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security
AlfaSVMLib Archived 2020-09-24 at the Wayback Machine – Adversarial Label Flip Attacks against Support Vector Machines
Laskov, Pavel; Lippmann, Richard (2010). "Machine learning in adversarial environments". Machine Learning. 81 (2): 115–119. doi:10.1007/s10994-010-5207-6. S2CID 12567278.
Dagstuhl Perspectives Workshop on "Machine Learning Methods for Computer Security"
Workshop on Artificial Intelligence and Security, (AISec) Series
Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science. While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.

Areas
Detecting and recognizing emotional information
Detecting emotional information usually begins with passive sensors that capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. For example, a video camera might capture facial expressions, body posture, and gestures, while a microphone might capture speech. Other sensors detect emotional cues by directly measuring physiological data, such as skin temperature and galvanic resistance.
Recognizing emotional information requires the extraction of meaningful patterns from the gathered data. This is done using machine learning techniques that process different modalities, such as speech recognition, natural language processing, or facial expression detection.  The goal of most of these techniques is to produce labels that would match the labels a human perceiver would give in the same situation:  For example, if a person makes a facial expression furrowing their brow, then the computer vision system might be taught to label their face as appearing "confused" or as "concentrating" or "slightly negative" (as opposed to positive, which it might say if they were smiling in a happy-appearing way).  These labels may or may not correspond to what the person is actually feeling.

Emotion in machines
Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. A more practical approach, based on current technological capabilities, is the simulation of emotions in conversational agents in order to enrich and facilitate interactivity between human and machine.
Marvin Minsky, one of the pioneering computer scientists in artificial intelligence, relates emotions to the broader issues of machine intelligence stating in The Emotion Machine that emotion is "not especially different from the processes that we call 'thinking.'" The innovative approach "digital humans" or virtual humans includes an attempt to give these programs, which simulate humans, the emotional dimension as well, including reactions in accordance with the reaction that a real person would react in a certain emotionally stimulating situation as well as facial expressions and gestures.
Emotion in machines often refers to emotion in computational, often AI-based, systems. As a result, the terms 'emotional AI' and 'emotion AI' are being used.

Technologies
In psychology, cognitive science, and in neuroscience, there have been two main approaches for describing how humans perceive and classify emotion: continuous or categorical. The continuous approach tends to use dimensions such as negative vs. positive, calm vs. aroused.
The categorical approach tends to use discrete classes such as happy, sad, angry, fearful, surprise, disgust.  Different kinds of machine learning regression and classification models can be used for having machines produce continuous or discrete labels.  Sometimes models are also built that allow combinations across the categories, e.g. a happy-surprised face or a fearful-surprised face.
The following sections consider many of the kinds of input data used for the task of emotion recognition.

Emotional speech
Various changes in the autonomic nervous system can indirectly alter a person's speech, and affective technologies can leverage this information to recognize emotion. For example, speech produced in a state of fear, anger, or joy becomes fast, loud, and precisely enunciated, with a higher and wider range in pitch, whereas emotions such as tiredness, boredom, or sadness tend to generate slow, low-pitched, and slurred speech. Some emotions have been found to be more easily computationally identified, such as anger or approval.
Emotional speech processing technologies recognize the user's emotional state using computational analysis of speech features. Vocal parameters and prosodic features such as pitch variables and speech rate can be analyzed through pattern recognition techniques.
Speech analysis is an effective method of identifying affective state, having an average reported accuracy of 70 to 80% in research from 2003 and 2006. These systems tend to outperform average human accuracy (approximately 60%) but are less accurate than systems which employ other modalities for emotion detection, such as physiological states or facial expressions. However, since many speech characteristics are independent of semantics or culture, this technique is considered to be a promising route for further research.

Algorithms
The process of speech/text affect detection requires the creation of a reliable database, knowledge base, or vector space model, broad enough to fit every need for its application, as well as the selection of a successful classifier which will allow for quick and accurate emotion identification.
As of 2010, the most frequently used classifiers were linear discriminant classifiers (LDC), k-nearest neighbor (k-NN), Gaussian mixture model (GMM), support vector machines (SVM), artificial neural networks (ANN), decision tree algorithms and hidden Markov models (HMMs). Various studies showed that choosing the appropriate classifier can significantly enhance the overall performance of the system. The list below gives a brief description of each algorithm:

LDC – Classification happens based on the value obtained from the linear combination of the feature values, which are usually provided in the form of vector features.
k-NN – Classification happens by locating the object in the feature space, and comparing it with the k nearest neighbors (training examples). The majority vote decides on the classification.
GMM – is a probabilistic model used for representing the existence of subpopulations within the overall population. Each sub-population is described using the mixture distribution, which allows for classification of observations into the sub-populations.
SVM – is a type of (usually binary) linear classifier which decides in which of the two (or more) possible classes, each input may fall into.
ANN – is a mathematical model, inspired by biological neural networks, that can better grasp possible non-linearities of the feature space.
Decision tree algorithms – work based on following a decision tree in which leaves represent the classification outcome, and branches represent the conjunction of subsequent features that lead to the classification.
HMMs – a statistical Markov model in which the states and state transitions are not directly available to observation. Instead, the series of outputs dependent on the states are visible. In the case of affect recognition, the outputs represent the sequence of speech feature vectors, which allow the deduction of states' sequences through which the model progressed. The states can consist of various intermediate steps in the expression of an emotion, and each of them has a probability distribution over the possible output vectors. The states' sequences allow us to predict the affective state which we are trying to classify, and this is one of the most commonly used techniques within the area of speech affect detection.
It is proved that having enough acoustic evidence available the emotional state of a person can be classified by a set of majority voting classifiers. The proposed set of classifiers is based on three main classifiers: kNN, C4.5 and SVM-RBF Kernel. This set achieves better performance than each basic classifier taken separately. It is compared with two other sets of classifiers: one-against-all (OAA) multiclass SVM with Hybrid kernels and the set of classifiers which consists of the following two basic classifiers: C5.0 and Neural Network. The proposed variant achieves better performance than the other two sets of classifiers.

Databases
The vast majority of present systems are data-dependent. This creates one of the biggest challenges in detecting emotions based on speech, as it implicates choosing an appropriate database used to train the classifier. Most of the currently possessed data was obtained from actors and is thus a representation of archetypal emotions. Those so-called acted databases are usually based on the Basic Emotions theory (by Paul Ekman), which assumes the existence of six basic emotions (anger, fear, disgust, surprise, joy, sadness), the others simply being a mix of the former ones. Nevertheless, these still offer high audio quality and balanced classes (although often too few), which contribute to high success rates in recognizing emotions.
However, for real life application, naturalistic data is preferred. A naturalistic database can be produced by observation and analysis of subjects in their natural context. Ultimately, such database should allow the system to recognize emotions based on their context as well as work out the goals and outcomes of the interaction. The nature of this type of data allows for authentic real life implementation, due to the fact it describes states naturally occurring during the human–computer interaction (HCI).
Despite the numerous advantages which naturalistic data has over acted data, it is difficult to obtain and usually has low emotional intensity. Moreover, data obtained in a natural context has lower signal quality, due to surroundings noise and distance of the subjects from the microphone. The first attempt to produce such database was the FAU Aibo Emotion Corpus for CEICES (Combining Efforts for Improving Automatic Classification of Emotional User States), which was developed based on a realistic context of children (age 10–13) playing with Sony's Aibo robot pet. Likewise, producing one standard database for all emotional research would provide a method of evaluating and comparing different affect recognition systems.

Speech descriptors
The complexity of the affect recognition process increases with the number of classes (affects) and speech descriptors used within the classifier. It is, therefore, crucial to select only the most relevant features in order to assure the ability of the model to successfully identify emotions, as well as increasing the performance, which is particularly significant to real-time detection. The range of possible choices is vast, with some studies mentioning the use of over 200 distinct features. It is crucial to identify those that are redundant and undesirable in order to optimize the system and increase the success rate of correct emotion detection. The most common speech characteristics are categorized into the following groups.

Frequency characteristics
Accent shape – affected by the rate of change of the fundamental frequency.
Average pitch – description of how high/low the speaker speaks relative to the normal speech.
Contour slope – describes the tendency of the frequency change over time, it can be rising, falling or level.
Final lowering – the amount by which the frequency falls at the end of an utterance.
Pitch range – measures the spread between the maximum and minimum frequency of an utterance.
Time-related features:
Speech rate – describes the rate of words or syllables uttered over a unit of time
Stress frequency – measures the rate of occurrences of pitch accented utterances
Voice quality parameters and energy descriptors:
Breathiness – measures the aspiration noise in speech
Brilliance – describes the dominance of high or low frequencies In the speech
Loudness – measures the amplitude of the speech waveform, translates to the energy of an utterance
Pause Discontinuity – describes the transitions between sound and silence
Pitch Discontinuity – describes the transitions of the fundamental frequency.

Facial affect detection
The detection and processing of facial expression are achieved through various methods such as optical flow, hidden Markov models, neural network processing or active appearance models. More than one modalities can be combined or fused (multimodal recognition, e.g. facial expressions and speech prosody, facial expressions and hand gestures, or facial expressions with speech and text for multimodal data and metadata analysis) to provide a more robust estimation of the subject's emotional state.

Facial expression databases
Creation of an emotion database is a difficult and time-consuming task. However, database creation is an essential step in the creation of a system that will recognize human emotions. Most of the publicly available emotion databases include posed facial expressions only. In posed expression databases, the participants are asked to display different basic emotional expressions, while in spontaneous expression database, the expressions are natural. Spontaneous emotion elicitation requires significant effort in the selection of proper stimuli which can lead to a rich display of intended emotions. Secondly, the process involves tagging of emotions by trained individuals manually which makes the databases highly reliable. Since perception of expressions and their intensity is subjective in nature, the annotation by experts is essential for the purpose of validation.
Researchers work with three types of databases, such as a database of peak expression images only, a database of image sequences portraying an emotion from neutral to its peak, and video clips with emotional annotations. Many facial expression databases have been created and made public for expression recognition purpose. Two of the widely used databases are CK+ and JAFFE.

Emotion classification
By doing cross-cultural research in Papua, New Guinea, on the Fore Tribesmen, at the end of the 1960s, Paul Ekman proposed the idea that facial expressions of emotion are not culturally determined, but universal. Thus, he suggested that they are biological in origin and can, therefore, be safely and correctly categorized.
He therefore officially put forth six basic emotions, in 1972:

Anger
Disgust
Fear
Happiness
Sadness
Surprise
However, in the 1990s Ekman expanded his list of basic emotions, including a range of positive and negative emotions not all of which are encoded in facial muscles. The newly included emotions are:

Amusement
Contempt
Contentment
Embarrassment
Excitement
Guilt
 Pride in achievement
Relief
Satisfaction
Sensory pleasure
Shame

Facial Action Coding System
A system has been conceived by psychologists in order to formally categorize the physical expression of emotions on faces. The central concept of the Facial Action Coding System, or FACS, as created by Paul Ekman and Wallace V. Friesen in 1978 based on earlier work by Carl-Herman Hjortsjö are action units (AU).
They are, basically, a contraction or a relaxation of one or more muscles. Psychologists have proposed the following classification of six basic emotions, according to their action units ("+" here mean "and"):

Challenges in facial detection
As with every computational practice, in affect detection by facial processing, some obstacles need to be surpassed, in order to fully unlock the hidden potential of the overall algorithm or method employed. In the early days of almost every kind of AI-based detection (speech recognition, face recognition, affect recognition), the accuracy of modeling and tracking has been an issue. As hardware evolves, as more data are collected and as new discoveries are made and new practices introduced, this lack of accuracy fades, leaving behind noise issues. However, methods for noise removal exist including neighborhood averaging, linear Gaussian smoothing, median filtering, or newer methods such as the Bacterial Foraging Optimization Algorithm.
Other challenges include

The fact that posed expressions, as used by most subjects of the various studies, are not natural, and therefore algorithms trained on these may not apply to natural expressions.
The lack of rotational movement freedom. Affect detection works very well with frontal use, but upon rotating the head more than 20 degrees, "there've been problems".
Facial expressions do not always correspond to an underlying emotion that matches them (e.g. they can be posed or faked, or a person can feel emotions but maintain a "poker face").
FACS did not include dynamics, while dynamics can help disambiguate (e.g. smiles of genuine happiness tend to have different dynamics than "try to look happy" smiles.)
The FACS combinations do not correspond in a 1:1 way with the emotions that the psychologists originally proposed  (note that this lack of a 1:1 mapping also occurs in speech recognition with homophones and homonyms and many other sources of ambiguity, and may be mitigated by bringing in other channels of information).
Accuracy of recognition is improved by adding context; however, adding context and other modalities increases computational cost and complexity

Body gesture
Gestures could be efficiently used as a means of detecting a particular emotional state of the user, especially when used in conjunction with speech and face recognition. Depending on the specific action, gestures could be simple reflexive responses, like lifting your shoulders when you don't know the answer to a question, or they could be complex and meaningful as when communicating with sign language. Without making use of any object or surrounding environment, we can wave our hands, clap or beckon. On the other hand, when using objects, we can point at them, move, touch or handle these. A computer should be able to recognize these, analyze the context and respond in a meaningful way, in order to be efficiently used for Human–Computer Interaction.
There are many proposed methods to detect the body gesture. Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles. On the other hand, appearance-based systems use images or videos to for direct interpretation. Hand gestures have been a common focus of body gesture detection methods.

Physiological monitoring
This could be used to detect a user's affective state by monitoring and analyzing their physiological signs. These signs range from changes in heart rate and skin conductance to minute contractions of the facial muscles and changes in facial blood flow. This area is gaining momentum and we are now seeing real products that implement the techniques. The four main physiological signs that are usually analyzed are blood volume pulse, galvanic skin response, facial electromyography, and facial color patterns.

Blood volume pulse
Overview
A subject's blood volume pulse (BVP) can be measured by a process called photoplethysmography, which produces a graph indicating blood flow through the extremities. The peaks of the waves indicate a cardiac cycle where the heart has pumped blood to the extremities. If the subject experiences fear or is startled, their heart usually 'jumps' and beats quickly for some time, causing the amplitude of the cardiac cycle to increase. This can clearly be seen on a photoplethysmograph when the distance between the trough and the peak of the wave has decreased. As the subject calms down, and as the body's inner core expands, allowing more blood to flow back to the extremities, the cycle will return to normal.

Methodology
Infra-red light is shone on the skin by special sensor hardware, and the amount of light reflected is measured. The amount of reflected and transmitted light correlates to the BVP as light is absorbed by hemoglobin which is found richly in the bloodstream.

Disadvantages
It can be cumbersome to ensure that the sensor shining an infra-red light and monitoring the reflected light is always pointing at the same extremity, especially seeing as subjects often stretch and readjust their position while using a computer.
There are other factors that can affect one's blood volume pulse. As it is a measure of blood flow through the extremities, if the subject feels hot, or particularly cold, then their body may allow more, or less, blood to flow to the extremities, all of this regardless of the subject's emotional state.

Facial electromyography
Facial electromyography is a technique used to measure the electrical activity of the facial muscles by amplifying the tiny electrical impulses that are generated by muscle fibers when they contract.
The face expresses a great deal of emotion, however, there are two main facial muscle groups that are usually studied to detect emotion:
The corrugator supercilii muscle, also known as the 'frowning' muscle, draws the brow down into a frown, and therefore is the best test for negative, unpleasant emotional response.↵The zygomaticus major muscle is responsible for pulling the corners of the mouth back when you smile, and therefore is the muscle used to test for a positive emotional response.

Galvanic skin response
Galvanic skin response (GSR) is an outdated term for a more general phenomenon known as [Electrodermal Activity] or EDA.  EDA is a general phenomena whereby the skin's electrical properties change.  The skin is innervated by the [sympathetic nervous system], so measuring its resistance or conductance provides a way to quantify small changes in the sympathetic branch of the autonomic nervous system.  As the sweat glands are activated, even before the skin feels sweaty, the level of the EDA can be captured (usually using conductance) and used to discern small changes in autonomic arousal.  The more aroused a subject is, the greater the skin conductance tends to be.
Skin conductance is often measured using two small silver-silver chloride electrodes placed somewhere on the skin and applying a small voltage between them. To maximize comfort and reduce irritation the electrodes can be placed on the wrist, legs, or feet, which leaves the hands fully free for daily activity.

Facial color
Overview
The surface of the human face is innervated with a large network of blood vessels. Blood flow variations in these vessels yield visible color changes on the face. Whether or not facial emotions activate facial muscles, variations in blood flow, blood pressure, glucose levels, and other changes occur. Also, the facial color signal is independent from that provided by facial muscle movements.

Methodology
Approaches are based on facial color changes. Delaunay triangulation is used to create the triangular local areas. Some of these triangles which define the interior of the mouth and eyes (sclera and iris) are removed. Use the left triangular areas’ pixels to create feature vectors. It shows that converting the pixel color of the standard RGB color space to a color space such as oRGB color space or LMS channels perform better when dealing with faces. So, map the above vector onto the better color space and decompose into red-green and yellow-blue channels. Then use deep learning methods to find equivalent emotions.

Visual aesthetics
Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities is a highly subjective task. Computer scientists at Penn State treat the challenge of automatically inferring the aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated on-line photo sharing website as a data source. They extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images.

Potential applications
Education
Affection influences learners' learning state. Using affective computing technology, computers can judge the learners' affection and learning state by recognizing their facial expressions. In education, the teacher can use the analysis result to understand the student's learning and accepting ability, and then formulate reasonable teaching plans. At the same time, they can pay attention to students' inner feelings, which is helpful to students' psychological health. Especially in distance education, due to the separation of time and space, there is no emotional incentive between teachers and students for two-way communication. Without the atmosphere brought by traditional classroom learning, students are easily bored, and affect the learning effect. Applying affective computing in distance education system can effectively improve this situation.

Transportation
The applications of sensory computing may contribute to improving road safety. For example, a car can monitor the emotion of all occupants and engage in additional safety measures, such as alerting other vehicles if it detects the driver to be angry. In addition, affective computing systems for monitoring the driver's stress may allow various interventions such as driver assistance systems adjusted according to the stress level and minimal and direct interventions to change the emotional state of the driver.

Healthcare
Social robots, as well as a growing number of robots used in health care benefit from emotional awareness because they can better judge users' and patient's emotional states and alter their actions/programming appropriately. This is especially important in those countries with growing aging populations and/or a lack of younger workers to address their needs.
Affective computing is also being applied to the development of communicative technologies for use by people with autism. The affective component of a text is also increasingly gaining attention, particularly its role in the so-called emotional or emotive Internet.

Video games
Affective video games can access their players' emotional states through biofeedback devices. A particularly simple form of biofeedback is available through gamepads that measure the pressure with which a button is pressed: this has been shown to correlate strongly with the players' level of arousal; at the other end of the scale are brain–computer interfaces. Affective games have been used in medical research to support the emotional development of autistic children.

Psychomotor training
Training methods of psychomotor operations such as steering and maneuvering are used in various fields such as aviation, transportation and medicine. Integrating affective computing capabilities in this type of training systems, in accordance with the adaptive automation approach, has been found to be effective in improving the quality of training and shortening the required training duration.

Other applications
Affective computing has potential applications in human–computer interaction, such as affective mirrors allowing the user to see how he or she performs; emotion monitoring agents sending a warning before one sends an angry email; or even music players selecting tracks based on mood.
One idea put forth by the Romanian researcher Dr. Nicu Sebe in an interview is the analysis of a person's face while they are using a certain product (he mentioned ice cream as an example). Companies would then be able to use such analysis to infer whether their product will or will not be well received by the respective market.
One could also use affective state recognition in order to judge the impact of a TV advertisement through a real-time video recording of that person and through the subsequent study of his or her facial expression. Averaging the results obtained on a large group of subjects, one can tell whether that commercial (or movie) has the desired effect and what the elements which interest the watcher most are.

Cognitivist vs. interactional approaches
Within the field of human–computer interaction, Rosalind Picard's cognitivist or "information model" concept of emotion has been criticized by and contrasted with the "post-cognitivist" or "interactional" pragmatist approach taken by Kirsten Boehner and others which views emotion as inherently social.
Picard's focus is human–computer interaction, and her goal for affective computing is to "give computers the ability to recognize, express, and in some cases, 'have' emotions". In contrast, the interactional approach seeks to help "people to understand and experience their own emotions" and to improve computer-mediated interpersonal communication.  It does not necessarily seek to map emotion into an objective mathematical model for machine interpretation, but rather let humans make sense of each other's emotional expressions in open-ended ways that might be ambiguous, subjective, and sensitive to context.: 284 
Picard's critics describe her concept of emotion as "objective, internal, private, and mechanistic". They say it reduces emotion to a discrete psychological signal occurring inside the body that can be measured and which is an input to cognition, undercutting the complexity of emotional experience.: 280 : 278 
The interactional approach asserts that though emotion has biophysical aspects, it is "culturally grounded, dynamically experienced, and to some degree constructed in action and interaction".: 276  Put another way, it considers "emotion as a social and cultural product experienced through our interactions".

See also
References
Citations
Works cited
Hudlicka, Eva (2003). "To feel or not to feel: The role of affect in human–computer interaction". International Journal of Human–Computer Studies. 59 (1–2): 1–32. CiteSeerX 10.1.1.180.6429. doi:10.1016/s1071-5819(03)00047-8.
Scherer, Klaus R; Bänziger, Tanja; Roesch, Etienne B (2010). A Blueprint for Affective Computing: A Sourcebook and Manual. Oxford: Oxford University Press. ISBN 978-0-19-956670-9.
Agriculture encompasses crop and livestock production, aquaculture, and forestry for food and non-food products. Agriculture was a key factor in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. While humans started gathering grains at least 105,000 years ago, nascent farmers only began planting them around 11,500 years ago. Sheep, goats, pigs, and cattle were domesticated around 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. In the 20th century, industrial agriculture based on large-scale monocultures came to dominate agricultural output.
As of 2021, small farms produce about one-third of the world's food, but large farms are prevalent. The largest 1% of farms in the world are greater than 50 hectares (120 acres) and operate more than 70% of the world's farmland. Nearly 40% of agricultural land is found on farms larger than 1,000 hectares (2,500 acres). However, five of every six farms in the world consist of fewer than 2 hectares (4.9 acres), and take up only around 12% of all agricultural land. Farms and farming greatly influence rural economics and greatly shape rural society, effecting both the direct agricultural workforce and broader businesses that support the farms and farming populations.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber). Food classes include cereals (grains), vegetables, fruits, cooking oils, meat, milk, eggs, and fungi. Global agricultural production amounts to approximately 11 billion tonnes of food, 32 million tonnes of natural fibers and 4 billion m3 of wood. However, around 14% of the world's food is lost from production before reaching the retail level.
Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased crop yields, but also contributed to ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to climate change, depletion of aquifers, deforestation, antibiotic resistance, and other agricultural pollution. Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation, and climate change, all of which can cause decreases in crop yield. Genetically modified organisms are widely used, although some countries ban them.

Etymology and scope
The word agriculture is a late Middle English adaptation of Latin agricultūra, from ager 'field' and cultūra 'cultivation' or 'growing'. While agriculture usually refers to human activities, certain species of ant, termite and beetle have been cultivating crops for up to 60 million years. Agriculture is defined with varying scopes, in its broadest sense using natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services". Thus defined, it includes arable farming, horticulture, animal husbandry and forestry, but horticulture and forestry are in practice often excluded.
It may also be broadly decomposed into plant agriculture, which concerns the cultivation of useful plants, and animal agriculture, the production of agricultural animals.

History
Origins
The development of agriculture enabled the human population to grow many times larger than could be sustained by hunting and gathering. Agriculture began independently in different parts of the globe, and included a diverse range of taxa, in at least 11 separate centers of origin. Wild grains were collected and eaten from at least 105,000 years ago. In the Paleolithic Levant, 23,000 years ago, cereals cultivation of emmer, barley, and oats has been observed near the sea of Galilee. Rice was domesticated in China between 11,500 and 6,200 BC with the earliest known cultivation from 5,700 BC, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago. Pig production emerged in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago. In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia. In Mesoamerica, wild teosinte was bred into maize (corn) from 10,000 to 6,000 years ago. The horse was domesticated in the Eurasian Steppes around 3500 BC.
Scholars have offered multiple hypotheses to explain the historical origins of agriculture. Studies of the transition from hunter-gatherer to agricultural societies indicate an initial period of intensification and increasing sedentism; examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China. Then, wild stands that had previously been harvested started to be planted, and gradually came to be domesticated.

Civilizations
In Eurasia, the Sumerians started to live in villages from about 8,000 BC, relying on the Tigris and Euphrates rivers and a canal system for irrigation. Ploughs appear in pictographs around 3,000 BC; seed-ploughs around 2,300 BC. Farmers grew wheat, barley, vegetables such as lentils and onions, and fruits including dates, grapes, and figs. Ancient Egyptian agriculture relied on the Nile River and its seasonal flooding. Farming started in the predynastic period at the end of the Paleolithic, after 10,000 BC. Staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus. In India, wheat, barley and jujube were domesticated by 9,000 BC, soon followed by sheep and goats. Cattle, sheep and goats were domesticated in Mehrgarh culture by 8,000–6,000 BC. Cotton was cultivated by the 5th–4th millennium BC. Archeological evidence indicates an animal-drawn plough from 2,500 BC in the Indus Valley civilization.

In China, from the 5th century BC, there was a nationwide granary system and widespread silk farming. Water-powered grain mills were in use by the 1st century BC, followed by irrigation. By the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards. These spread westwards across Eurasia. Asian rice was domesticated 8,200–13,500 years ago – depending on the molecular clock estimate that is used– on the Pearl River in southern China with a single genetic origin from the wild rice Oryza rufipogon. In Greece and Rome, the major cereals were wheat, emmer, and barley, alongside vegetables including peas, beans, and olives. Sheep and goats were kept mainly for dairy products.
In the Americas, crops domesticated in Mesoamerica (apart from teosinte) include squash, beans, and cacao. Cocoa was domesticated by the Mayo Chinchipe of the upper Amazon around 3,000 BC.
The turkey was probably domesticated in Mexico or the American Southwest. The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands. The Mayas used extensive canal and raised field systems to farm swampland from 400 BC. In South America agriculture may have begun about 9000 BC with the domestication of squash (Cucurbita) and other plants. Coca was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple. Cotton was domesticated in Peru by 3,600 BC. Animals including llamas, alpacas, and guinea pigs were domesticated there. In North America, the indigenous people of the East domesticated crops such as sunflower, tobacco, squash and Chenopodium. Wild foods including wild rice and maple sugar were harvested. The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America. The indigenous people of the Southwest and the Pacific Northwest practiced forest gardening and fire-stick farming. The natives controlled fire on a regional scale to create a low-intensity fire ecology that sustained a low-density agriculture in loose rotation; a sort of "wild" permaculture. A system of companion planting called the Three Sisters was developed in North America. The three crops were winter squash, maize, and climbing beans.
Indigenous Australians, long supposed to have been nomadic hunter-gatherers, practiced systematic burning, possibly to enhance natural productivity in fire-stick farming. Scholars have pointed out that hunter-gatherers need a productive environment to support gathering without cultivation. Because the forests of New Guinea have few food plants, early humans may have used "selective burning" to increase the productivity of the wild karuka fruit trees to support the hunter-gatherer way of life.
The Gunditjmara and other groups developed eel farming and fish trapping systems from some 5,000 years ago. There is evidence of 'intensification' across the whole continent over that period. In two regions of Australia, the central west coast and eastern central, early farmers cultivated yams, native millet, and bush onions, possibly in permanent settlements.

Revolution
In the Middle Ages, compared to the Roman period, agriculture in Western Europe became more focused on self-sufficiency. The agricultural population under feudalism was typically organized into manors consisting of several hundred or more acres of land presided over by a lord of the manor with a Roman Catholic church and priest.
Thanks to the exchange with the Al-Andalus where the Arab Agricultural Revolution was underway, European agriculture transformed, with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees (such as the orange).
After 1492, the Columbian exchange brought New World crops such as maize, potatoes, tomatoes, sweet potatoes, and manioc to Europe, and Old World crops such as wheat, barley, rice, and turnips, and livestock (including horses, cattle, sheep and goats) to the Americas.
Irrigation, crop rotation, and fertilizers advanced from the 17th century with the British Agricultural Revolution, allowing global population to rise significantly. Since 1900, agriculture in developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as mechanization replaces human labor, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields and sustaining a further increase in global population.
Modern agriculture has raised or encountered ecological, political, and economic issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement. Unsustainable farming practices in North America led to the Dust Bowl of the 1930s.

Types
Pastoralism involves managing domesticated animals. In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water. This type of farming is practiced in arid and semi-arid regions of Sahara, Central Asia and some parts of India.

In shifting cultivation, a small area of forest is cleared by cutting and burning the trees. The cleared land is used for growing crops for a few years until the soil becomes too infertile, and the area is abandoned. Another patch of land is selected and the process is repeated. This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly. This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.
Subsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere. It is intensively practiced in Monsoon Asia and South-East Asia. An estimated 2.5 billion subsistence farmers worked in 2018, cultivating about 60% of the earth's arable land.
Intensive farming is cultivation to maximize productivity, with a low fallow ratio and a high use of inputs (water, fertilizer, pesticide and automation). It is practiced mainly in developed countries.

Contemporary agriculture
Status
From the twentieth century onwards, intensive agriculture increased crop productivity. It substituted synthetic fertilizers and pesticides for labour, but caused increased water pollution, and often involved farm subsidies. Soil degradation and diseases such as stem rust are major concerns globally; approximately 40% of the world's agricultural land is seriously degraded. In recent years there has been a backlash against the environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management, selective breeding, and controlled-environment agriculture. There are concerns about the lower yield associated with organic farming and its impact on global food security. Recent mainstream technological developments include genetically modified food.

By 2015, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States. Economists measure the total factor productivity of agriculture, according to which agriculture in the United States is roughly 1.7 times more productive than it was in 1948.
Agriculture employed 873 million people in 2021, or 27% of the global workforce, compared with 1 027 million (or 40%) in 2000. The share of agriculture in global GDP was stable at around 4% since 2000 - 2023.
Despite increases in agricultural production and productivity, between 702 and 828 million people were affected by hunger in 2021. Food insecurity and malnutrition can be the result of conflict, climate extremes and variability and economic swings. It can also be caused by a country's structural characteristics such as income status and natural resource endowments as well as its political economy.
Pesticide use in agriculture went up 62% between 2000 and 2021, with the Americas accounting for half the use in 2021.
The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security, given the favorable experience of Vietnam.

Workforce
Agriculture provides about one-quarter of all global employment, more than half in sub-Saharan Africa and almost 60 percent in low-income countries. As countries develop, other jobs have historically pulled workers away from agriculture, and labor-saving innovations increase agricultural productivity by reducing labor requirements per unit of output. Over time, a combination of labor supply and labor demand trends have driven down the share of population employed in agriculture.
During the 16th century in Europe, between 55 and 75% of the population was engaged in agriculture; by the 19th century, this had dropped to between 35 and 65%. In the same countries today, the figure is less than 10%.
At the start of the 21st century, some one billion people, or over 1/3 of the available work force, were employed in agriculture. This constitutes approximately 70% of the global employment of children, and in many countries constitutes the largest percentage of women of any industry. The service sector overtook the agricultural sector as the largest global employer in 2007.
In many developed countries, immigrants help fill labor shortages in high-value agriculture activities that are difficult to mechanize. Foreign farm workers from mostly Eastern Europe, North Africa and South Asia constituted around one-third of the salaried agricultural workforce in Spain, Italy, Greece and Portugal in 2013. In the United States of America, more than half of all hired farmworkers (roughly 450,000 workers) were immigrants in 2019, although the number of new immigrants arriving in the country to work in agriculture has fallen by 75 percent in recent years and rising wages indicate this has led to a major labor shortage on U.S. farms.

Women in agriculture
Around the world, women make up a large share of the population employed in agriculture. This share is growing in all developing regions except East and Southeast Asia where women already make up about 50 percent of the agricultural workforce. Women make up 47 percent of the agricultural workforce in sub-Saharan Africa, a rate that has not changed significantly in the past few decades. However, the Food and Agriculture Organization of the United Nations (FAO) posits that the roles and responsibilities of women in agriculture may be changing – for example, from subsistence farming to wage employment, and from contributing household members to primary producers in the context of male-out-migration.
In general, women account for a greater share of agricultural employment at lower levels of economic development, as inadequate education, limited access to basic infrastructure and markets, high unpaid work burden and poor rural employment opportunities outside agriculture severely limit women's opportunities for off-farm work.
Women who work in agricultural production tend to do so under highly unfavorable conditions. They tend to be concentrated in the poorest countries, where alternative livelihoods are not available, and they maintain the intensity of their work in conditions of climate-induced weather shocks and in situations of conflict. Women are less likely to participate as entrepreneurs and independent farmers and are engaged in the production of less lucrative crops.
The gender gap in land productivity between female- and male managed farms of the same size is 24 percent. On average, women earn 18.4 percent less than men in wage employment in agriculture; this means that women receive 82 cents for every dollar earned by men. Progress has been slow in closing gaps in women's access to irrigation and in ownership of livestock, too.
Women in agriculture still have significantly less access than men to inputs, including improved seeds, fertilizers and mechanized equipment. On a positive note, the gender gap in access to mobile internet in low- and middle-income countries fell from 25 percent to 16 percent between 2017 and 2021, and the gender gap in access to bank accounts narrowed from 9 to 6 percentage points. Women are as likely as men to adopt new technologies when the necessary enabling factors are put in place and they have equal access to complementary resources.

Safety
Agriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Ages 0–6 may be an especially vulnerable population in agriculture; common causes of fatal injuries among young farm workers include drowning, machinery and motor accidents, including with all-terrain vehicles.
The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors". It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.
In the United States, agriculture has been identified by the National Institute for Occupational Safety and Health as a priority industry sector in the National Occupational Research Agenda to identify and provide intervention strategies for occupational health and safety issues.
In the European Union, the European Agency for Safety and Health at Work has issued guidelines on implementing health and safety directives in agriculture, livestock farming, horticulture, and forestry. The Agricultural Safety and Health Council of America (ASHCA) also holds a yearly summit to discuss safety.

Production
Overall production varies by country as listed.

Crop cultivation systems
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.
Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.

Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.
In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.
Important categories of food crops include cereals, legumes, forage, fruits and vegetables. Natural fibers include cotton, wool, hemp, silk and flax. Specific crops are cultivated in distinct growing regions throughout the world. Production is listed in millions of metric tons, based on FAO estimates.

Livestock production systems
Animal husbandry is the breeding and raising of animals for meat, milk, eggs, or wool, and for work and transport. Working animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, have for centuries been used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers.
Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. As of 2010, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.
During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.

Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.
Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organization for Economic Co-operation and Development member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure use becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.

Production practices
Tillage is the practice of breaking up the soil with tools such as the plow or harrow to prepare for planting, for nutrient incorporation, or for pest control. Tillage varies in intensity from conventional to no-till. It can improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.
Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.
Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of use of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.

Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture for the following year. Recent technological innovations in precision agriculture allow for water status monitoring and automate water usage, leading to more efficient management. Agriculture represents 70% of freshwater use worldwide. However, water withdrawal ratios for agriculture vary significantly by income level. In least developed countries and landlocked developing countries, water withdrawal ratios for agriculture are as high as 90 percent of total water withdrawals and about 60 percent in Small Island Developing States.
According to 2014 report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other. Using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Payment for ecosystem services is a method of providing additional incentives to encourage farmers to conserve some aspects of the environment. Measures might include paying for reforestation upstream of a city, to improve the supply of fresh water.

Agricultural automation
Different definitions exist for agricultural automation and for the variety of tools and technologies that are used to automate production. One view is that agricultural automation refers to autonomous navigation by robots without human intervention. Alternatively it is defined as the accomplishment of production tasks through mobile, autonomous, decision-making, mechatronic devices. However, FAO finds that these definitions do not capture all the aspects and forms of automation, such as robotic milking machines that are static, most motorized machinery that automates the performing of agricultural operations, and digital tools (e.g., sensors) that automate only diagnosis. FAO defines agricultural automation as the use of machinery and equipment in agricultural operations to improve their diagnosis, decision-making or performing, reducing the drudgery of agricultural work or improving the timeliness, and potentially the precision, of agricultural operations.
The technological evolution in agriculture has involved a progressive move from manual tools to animal traction, to motorized mechanization, to digital equipment and finally, to robotics with artificial intelligence (AI). Motorized mechanization using engine power automates the performance of agricultural operations such as ploughing and milking. With digital automation technologies, it also becomes possible to automate diagnosis and decision-making of agricultural operations. For example, autonomous crop robots can harvest and seed crops, while drones can gather information to help automate input application. Precision agriculture often employs such automation technologies. Motorized machines are increasingly complemented, or even superseded, by new digital equipment that automates diagnosis and decision-making. A conventional tractor, for example, can be converted into an automated vehicle allowing it to sow a field autonomously.
Motorized mechanization has increased significantly across the world in recent years, although reliable global data with broad country coverage exist only for tractors and only up to 2009. Sub-Saharan Africa is the only region where the adoption of motorized mechanization has stalled over the past decades.
Automation technologies are increasingly used for managing livestock, though evidence on adoption is lacking. Global automatic milking system sales have increased over recent years, but adoption is likely mostly in Northern Europe, and likely almost absent in low- and middle-income countries. Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce.
Measuring the overall employment impacts of agricultural automation is difficult because it requires large amounts of data tracking all the transformations and the associated reallocation of workers both upstream and downstream. While automation technologies reduce labor needs for the newly automated tasks, they also generate new labor demand for other tasks, such as equipment maintenance and operation. Agricultural automation can also stimulate employment by allowing producers to expand production and by creating other agrifood systems jobs. This is especially true when it happens in context of rising scarcity of rural labor, as is the case in high-income countries and many middle-income countries. On the other hand, if forcedly promoted, for example through government subsidies in contexts of abundant rural labor, it can lead to labor displacement and falling or stagnant wages, particularly affecting poor and low-skilled workers.

Effects of climate change on yields
Climate change and agriculture are interrelated on a global scale. Climate change affects agriculture through changes in average temperatures, rainfall, and weather extremes (like storms and heat waves); changes in pests and diseases; changes in atmospheric carbon dioxide and ground-level ozone concentrations; changes in the nutritional quality of some foods; and changes in sea level. Global warming is already affecting agriculture, with effects unevenly distributed across the world.
In a 2022 report, the Intergovernmental Panel on Climate Change describes how human-induced warming has slowed growth of agricultural productivity over the past 50 years in mid and low latitudes. Methane emissions have negatively impacted crop yields by increasing temperatures and surface ozone concentrations. Warming is also negatively affecting crop and grassland quality and harvest stability. Ocean warming has decreased sustainable yields of some wild fish populations while ocean acidification and warming have already affected farmed aquatic species. Climate change will probably increase the risk of food insecurity for some vulnerable groups, such as the poor.

Crop alteration and biotechnology
Plant breeding
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.
Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.

The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties". For example, average yields of corn (maize) in the US have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, and growth control to avoid lodging).

Investments into innovation for agriculture are long term. This is because it takes time for research to become commercialized and for technology to be adapted to meet multiple regions’ needs, as well as meet national guidelines before being adopted and planted in a farmer’s fields. For instance, it took at least 60 years from the introduction of hybrid corn technology before its adoption became widespread.
Agricultural innovation developed for the specific agroecological conditions of one region is not easily transferred and used in another region with different agroecological conditions. Instead, the innovation would have to be adapted to the specific conditions of that other region and respect its biodiversity and environmental requirements and guidelines. Some such adaptations can be seen through the steadily increasing number of plant varieties protected under the plant variety protection instrument administered by the International Union for the Protection of New Varieties of Plants (UPOV).

Genetic engineering
Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to use in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops. The Biosafety Protocol, an international treaty, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.
Herbicide-resistant seeds have a gene implanted into their genome that allows the plants to tolerate exposure to herbicides, including glyphosate. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium Bacillus thuringiensis (Bt), which produces a toxin specific to insects. These crops resist damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.

Environmental impact
Effects and costs
Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation and climate change, which cause decreases in crop yield. Agriculture is one of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. Agriculture is the main source of toxins released into the environment, including insecticides, especially those used on cotton. The 2011 UNEP Green Economy report stated that agricultural operations produced some 13 per cent of anthropogenic global greenhouse gas emissions. This includes gases from the use of inorganic fertilizers, agro-chemical pesticides, and herbicides, as well as fossil fuel-energy inputs.
Agriculture imposes multiple external costs upon society through effects such as pesticide damage to nature (especially herbicides and insecticides), nutrient runoff, excessive water usage, and loss of natural environment. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the US concluded that cropland imposes approximately $5 to $16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society.
Agriculture seeks to increase yield and to reduce costs, often employing measures that cut biodiversity to very low levels. Yield increases with inputs such as fertilizers and removal of pathogens, predators, and competitors (such as weeds). Costs decrease with increasing scale of farm units, such as making fields larger; this means removing hedges, ditches and other areas of habitat. Pesticides kill insects, plants and fungi. Effective yields fall with on-farm losses, which may be caused by poor production practices during harvesting, handling, and storage.
The environmental effects of climate change show that research on pests and diseases that do not generally afflict areas is essential. In 2021, farmers discovered stem rust on wheat in the Champagne area of France, a disease that had previously only occurred in Morocco for 20 to 30 years. Because of climate change, insects that used to die off over the winter are now alive and multiplying.

Livestock issues
A senior UN official, Henning Steinfeld, said that "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feed crops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. A well documented phenomenon is woody plant encroachment, caused by overgrazing in rangelands. Furthermore, the United Nations Environment Programme (UNEP) states that "methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns."

Land and water issues
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is the driving force causing biodiversity loss. Estimates of the amount of land transformed by humans vary from 39 to 50%. It is estimated that 24% of land globally experiences land degradation, a long-term decline in ecosystem function and productivity, with cropland being disproportionately affected. Land management is the driving factor behind degradation; 1.5 billion people rely upon the degrading land. Degradation can be through deforestation, desertification, soil erosion, mineral depletion, acidification, or salinization. In 2021, the global agricultural land area was 4.79 billion hectares (ha), down 2 percent, or 0.09 billion ha compared with 2000. Between 2000 and 2021, roughly two-thirds of agricultural land were used for permanent meadows and pastures (3.21 billion ha in 2021), which declined by 5 percent (0.17 billion ha). One-third of the total agricultural land was cropland (1.58 billion ha in 2021), which increased by 6 percent (0.09 billion ha).
Eutrophication, excessive nutrient enrichment in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems and pollution of groundwater, with harmful effects on human populations. Fertilizers also reduce terrestrial biodiversity by increasing competition for light, favoring those species that are able to benefit from the added nutrients.
Agriculture simultaneously is facing growing freshwater demand and precipitation anomalies (droughts, floods, and extreme rainfall and weather events) on rainfed areas fields and grazing lands. Agriculture accounts for 70 percent of withdrawals of freshwater resources, and an estimated 41 percent of current global irrigation water use occurs at the expense of environmental flow requirements. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. While industrial withdrawals have declined in the past few decades and municipal withdrawals have increased only marginally since 2010, agricultural withdrawals have continued to grow at an ever faster pace. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.

Pesticides
Pesticide use has increased since 1950 to 2.5 million short tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that three million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the "pesticide treadmill" in which pest resistance warrants the development of a new pesticide.
An alternative argument is that the way to "save the environment" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides can replace good agronomic practices such as crop rotation. The Push–pull agricultural pest management technique involves intercropping, using plant aromas to repel pests from crops (push) and to lure them to a place from which they can then be removed (pull).

Contribution to climate change
Agriculture contributes towards climate change through greenhouse gas emissions and by the conversion of non-agricultural land such as forests into agricultural land. The agriculture, forestry and land use sector contribute between 13% and 21% of global greenhouse gas emissions. Emissions of nitrous oxide, methane make up over half of total greenhouse gas emission from agriculture. Animal husbandry is a major source of greenhouse gas emissions.
Approximately 57% of global GHG emissions from the production of food are from the production of animal-based food while plant-based foods contribute 29% and the remaining 14% is for other utilizations. Farmland management and land-use change represented major shares of total emissions (38% and 29%, respectively), whereas rice and beef were the largest contributing plant- and animal-based commodities (12% and 25%, respectively). South and Southeast Asia and South America were the largest emitters of production-based GHGs.

Sustainability
Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. There is not enough water to continue farming using current practices; therefore how water, land, and ecosystem resources are used to boost crop yields must be reconsidered. A solution would be to give value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for more productive farmland, or the preservation of a wetland system that limits fishing rights.
Technological advancements help provide farmers with tools and resources to make farming more sustainable. Technology permits innovations like conservation tillage, a farming process which helps prevent land loss to erosion, reduces water pollution, and enhances carbon sequestration.
Agricultural automation can help address some of the challenges associated with climate change and thus facilitate adaptation efforts. For example, the application of digital automation technologies (e.g. in precision agriculture) can improve resource-use efficiency in conditions which are increasingly constrained for agricultural producers. Moreover, when applied to sensing and early warning, they can help address the uncertainty and unpredictability of weather conditions associated with accelerating climate change.
Other potential sustainable practices include conservation agriculture, agroforestry, improved grazing, avoided grassland conversion, and biochar. Current mono-crop farming practices in the United States preclude widespread adoption of sustainable practices, such as 2–3 crop rotations that incorporate grass or hay with annual crops, unless negative emission goals such as soil carbon sequestration become policy.
The food demand of Earth's projected population, with current climate change predictions, could be satisfied by improvement of agricultural methods, expansion of agricultural areas, and a sustainability-oriented consumer mindset.

Energy dependence
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960s and the 1980s, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Heavy reliance on petrochemicals has raised concerns that oil shortages could increase costs and reduce agricultural output.
Industrialized agriculture depends on fossil fuels in two fundamental ways: direct consumption on the farm and manufacture of inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery.
Indirect consumption includes the manufacture of fertilizers, pesticides, and farm machinery. In particular, the production of nitrogen fertilizer can account for over half of agricultural energy usage. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has since gradually declined. Food systems encompass not just agriculture but off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.

Plastic pollution
Plastic products are used extensively in agriculture, including to increase crop yields and improve the efficiency of water and agrichemical use. "Agriplastic" products include films to cover greenhouses and tunnels, mulch to cover soil (e.g. to suppress weeds, conserve water, increase soil temperature and aid fertilizer application), shade cloth, pesticide containers, seedling trays, protective mesh and irrigation tubing. The polymers most commonly used in these products are low- density polyethylene (LPDE), linear low-density polyethylene (LLDPE), polypropylene (PP) and polyvinyl chloride (PVC).
The total amount of plastics used in agriculture is difficult to quantify. A 2012 study reported that almost 6.5 million tonnes per year were consumed globally while a later study estimated that global demand in 2015 was between 7.3 million and 9 million tonnes. Widespread use of plastic mulch and lack of systematic collection and management have led to the generation of large amounts of mulch residue. Weathering and degradation eventually cause the mulch to fragment. These fragments and larger pieces of plastic accumulate in soil. Mulch residue has been measured at levels of 50 to 260 kg per hectare in topsoil in areas where mulch use dates back more than 10 years, which confirms that mulching is a major source of both microplastic and macroplastic soil contamination.
Agricultural plastics, especially plastic films, are not easy to recycle because of high contamination levels (up to 40–50% by weight contamination by pesticides, fertilizers, soil and debris, moist vegetation, silage juice water, and UV stabilizers) and collection difficulties . Therefore, they are often buried or abandoned in fields and watercourses or burned. These disposal practices lead to soil degradation and can result in contamination of soils and leakage of microplastics into the marine environment as a result of precipitation run-off and tidal washing. In addition, additives in residual plastic film (such as UV and thermal stabilizers) may have deleterious effects on crop growth, soil structure, nutrient transport and salt levels. There is a risk that plastic mulch will deteriorate soil quality, deplete soil organic matter stocks, increase soil water repellence and emit greenhouse gases. Microplastics released through fragmentation of agricultural plastics can absorb and concentrate contaminants capable of being passed up the trophic chain.

Disciplines
Agricultural economics
Agricultural economics is economics as it relates to the "production, distribution and consumption of [agricultural] goods and services". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
National government policies, such as taxation, subsidies, tariffs and others, can significantly change the economic marketplace for agricultural products. Since at least the 1960s, a combination of trade restrictions, exchange rate policies and subsidies have affected farmers in both the developing and the developed world. In the 1980s, non-subsidized farmers in developing countries experienced adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements limited agricultural tariffs, subsidies and other trade restrictions.
However, as of 2009, there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the most trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the most taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have decreases more among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities such as corn, soybeans, and cattle are generally graded to indicate quality, affecting the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.

Agricultural science
Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences used in the practice and understanding of agriculture. It covers topics such as agronomy, plant breeding and genetics, plant pathology, crop modelling, soil science, entomology, production techniques and improvement, study of pests and their management, and study of adverse environmental effects such as soil degradation, waste management, and bioremediation.
The scientific study of agriculture began in the 18th century, when Johann Friedrich Mayer conducted experiments on the use of gypsum (hydrated calcium sulphate) as a fertilizer. Research became more systematic when in 1843, John Lawes and Henry Gilbert began a set of long-term agronomy field experiments at Rothamsted Research Station in England; some of them, such as the Park Grass Experiment, are still running. In America, the Hatch Act of 1887 provided funding for what it was the first to call "agricultural science", driven by farmers' interest in fertilizers. In agricultural entomology, the USDA began to research biological control in 1881; it instituted its first large program in 1905, searching Europe and Japan for natural enemies of the spongy moth and brown-tail moth, establishing parasitoids (such as solitary wasps) and predators of both pests in the US.

Policy
Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.
A 2021 report finds that globally, support to agricultural producers accounts for almost US$540 billion a year. This amounts to 15 percent of total agricultural production value, and is heavily biased towards measures that are leading to inefficiency, as well as are unequally distributed and harmful for the environment and human health.  
There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.

See also
References
Cited sources
Acquaah, George (2002). Principles of Crop Production: Theory, Techniques, and Technology. Prentice Hall. ISBN 978-0-13-022133-9.
Chrispeels, Maarten J.; Sadava, David E. (1994). Plants, Genes, and Agriculture. Boston, Massachusetts: Jones and Bartlett. ISBN 978-0-86720-871-9.
Needham, Joseph (1986). Science and Civilization in China. Taipei: Caves Books.
 This article incorporates text from a free content work.  Licensed under CC BY-SA 3.0 IGO (license statement/permission). Text taken from Drowning in Plastics – Marine Litter and Plastic Waste Vital Graphics​,   United Nations Environment Programme.  
 This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief: The State of Food and Agriculture 2019. Moving forward on food loss and waste reduction​,  FAO, FAO.  
 This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief to The State of Food Security and Nutrition in the World 2022. Repurposing food and agricultural policies to make healthy diets more affordable​,   FAO.  
 This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief: The State of Food and Agriculture 2018. Migration, agriculture and rural development​,  FAO, FAO.  
 This article incorporates text from a free content work.  (license statement/permission). Text taken from In Brief to The State of Food and Agriculture 2022. Leveraging automation in agriculture for transforming agrifood systems​,  FAO, FAO.  
 This article incorporates text from a free content work.  (license statement/permission). Text taken from Enabling inclusive agricultural automation​,  FAO, FAO.  
 This article incorporates text from a free content work.  Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from The status of women in agrifood systems – Overview​,  FAO, FAO.  
 This article incorporates text from a free content work.  Licensed under CC BY-SA IGO 3.0 (license statement/permission). Text taken from World Food and Agriculture – Statistical Yearbook 2023​,  FAO, FAO.  
 This article incorporates text from a free content work.  Licensed under CC BY 4.0 (license statement/permission). Text taken from World Intellectual Property Report 2024 - The importance of local capabilities in AgTech specialization​,  WIPO, WIPO.

External links

Food and Agriculture Organization
United States Department of Agriculture
Agriculture material from the World Bank Group
Agriculture collected news and commentary at The New York Times
Agriculture collected news and commentary at The Guardian
Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as "The Founding Father" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001 to 2014.

Education
Mackworth was educated at the University of Toronto (B.A.Sc.), Harvard University (A.M.) and University of Sussex (D.Phil.).

Research
He works on constraint-based artificial intelligence with applications in vision, robotics, situated agents, assistive technology and sustainability. He is known as a pioneer in the areas of constraint satisfaction, robot soccer, hybrid systems and constraint-based agents. He has authored over 100 papers and co-authored two books: Computational Intelligence: A Logical Approach (1998) and Artificial Intelligence: Foundations of Computational Agents (2010).

RoboCup
Mackworth proposed and built the world's first soccer-playing robots, which led to the development of robot soccer as the premier global platform for multi-agent robotic research through the International RoboCup Foundation, where he has been honoured as "The Founding Father". Robot soccer as a challenge problem has great scientific significance. It has now become a standard test environment for cross-testing research ideas: a forum for evolving theories of multi-agent systems. Through regular international RoboCup tournaments many research teams of students and professors compete and cooperate in the development, testing and evolution of new theories and new algorithms.

Career
He served as the founding director of the UBC Laboratory for Computational Intelligence. He was president and trustee of International Joint Conferences on AI (IJCAI) Inc.; he is on the IJCAI executive committee. He has served on many editorial boards and program committees.  He was VP and president of the Canadian Society for Computational Studies of Intelligence (CSCSI). He served as president of the Association for the Advancement of Artificial Intelligence (AAAI).

Awards
Mackworth has received the ITAC/NSERC Award for Academic Excellence, the Killam Research Prize, the CSCSI Distinguished Service Award, the AAAI Distinguished Service Award, the Association for Constraint Programming Award for Research Excellence and the Lifetime Achievement Award of the Canadian AI Association (CAIAC). He is a Fellow of AAAI, the Canadian Institute for Advanced Research and the Royal Society of Canada.

References
External links
Alan Mackworth web page
Alan Mathison Turing  (; 23 June 1912 – 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science.
Born in London, Turing was raised in southern England. He graduated from King's College, Cambridge, and in 1938, earned a doctorate degree from Princeton University. During World War II, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. He led Hut 8, the section responsible for German naval cryptanalysis. Turing devised techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. He played a crucial role in cracking intercepted messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.
After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. Turing wrote on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, he was never fully recognised during his lifetime because much of his work was covered by the Official Secrets Act.
In 1952, Turing was prosecuted for homosexual acts. He accepted hormone treatment, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, aged 41, from cyanide poisoning. An inquest determined his death as suicide, but the evidence is also consistent with accidental poisoning. 
Following a campaign in 2009, British prime minister Gordon Brown made an official public apology for "the appalling way [Turing] was treated". Queen Elizabeth II granted a pardon in 2013. The term "Alan Turing law" is used informally to refer to a 2017 law in the UK that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.
Turing left an extensive legacy in mathematics and computing which today is recognised more widely, with statues and many things named after him, including an annual award for computing innovation. His portrait appears on the Bank of England £50 note, first released on 23 June 2021 to coincide with his birthday. The audience vote in a, 2019 BBC series, named Turing the greatest person of the 20th century.

Early life and education
Family
Turing was born in Maida Vale, London, while his father, Julius Mathison Turing, was on leave from his position with the Indian Civil Service (ICS) of the British Raj government at Chatrapur, then in the Madras Presidency and presently in Odisha state, in India. Turing's father was the son of a clergyman, the Rev. John Robert Turing, from a Scottish family of merchants that had been based in the Netherlands and included a baronet. Turing's mother, Julius's wife, was Ethel Sara Turing (née Stoney), daughter of Edward Waller Stoney, chief engineer of the Madras Railways. The Stoneys were a Protestant Anglo-Irish gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare. Julius and Ethel married on 1 October 1907 at the Church of Ireland St. Bartholomew's Church on Clyde Road in Ballsbridge, Dublin.
Julius's work with the ICS brought the family to British India, where his grandfather had been a general in the Bengal Army. However, both Julius and Ethel wanted their children to be brought up in Britain, so they moved to Maida Vale, London, where Alan Turing was born on 23 June 1912, as recorded by a blue plaque on the outside of the house of his birth, later the Colonnade Hotel. Turing had an elder brother, John Ferrier Turing, father of Sir John Dermot Turing, 12th Baronet of the Turing baronets.
Turing's father's civil service commission was still active during Turing's childhood years, and his parents travelled between Hastings in the United Kingdom and India, leaving their two sons to stay with a retired Army couple. At Hastings, Turing stayed at Baston Lodge, Upper Maze Hill, St Leonards-on-Sea, now marked with a blue plaque. The plaque was unveiled on 23 June 2012, the centenary of Turing's birth.
Very early in life, Turing's parents purchased a house in Guildford in 1927, and Turing lived there during school holidays. The location is also marked with a blue plaque.

School
Turing's parents enrolled him at St Michael's, a primary school at 20 Charles Road, St Leonards-on-Sea, from the age of six to nine. The headmistress recognised his talent, noting that she "...had clever boys and hardworking boys, but Alan is a genius".
Between January 1922 and 1926, Turing was educated at Hazelhurst Preparatory School, an independent school in the village of Frant in Sussex (now East Sussex). In 1926, at the age of 13, he went on to Sherborne School, an independent boarding school in the market town of Sherborne in Dorset, where he boarded at Westcott House. The first day of term coincided with the 1926 General Strike, in Britain, but Turing was so determined to attend that he rode his bicycle unaccompanied 60 miles (97 km) from Southampton to Sherborne, stopping overnight at an inn.
Turing's natural inclination towards mathematics and science did not earn him respect from some of the teachers at Sherborne, whose definition of education placed more emphasis on the classics. His headmaster wrote to his parents: "I hope he will not fall between two stools. If he is to stay at public school, he must aim at becoming educated. If he is to be solely a Scientific Specialist, he is wasting his time at a public school". Despite this, Turing continued to show remarkable ability in the studies he loved, solving advanced problems in 1927 without having studied even elementary calculus. In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in which this was never made explicit.

Christopher Morcom
At Sherborne, Turing formed a significant friendship with fellow pupil Christopher Collan Morcom (13 July 1911 – 13 February 1930), who has been described as Turing's first love. Their relationship provided inspiration in Turing's future endeavours, but it was cut short by Morcom's death, in February 1930, from complications of bovine tuberculosis, contracted after drinking infected cow's milk some years previously.
The event caused Turing great sorrow. He coped with his grief by working that much harder on the topics of science and mathematics that he had shared with Morcom. In a letter to Morcom's mother, Frances Isobel Morcom (née Swan), Turing wrote:

I am sure I could not have found anywhere another companion so brilliant and yet so charming and unconceited. I regarded my interest in my work, and in such things as astronomy (to which he introduced me) as something to be shared with him and I think he felt a little the same about me ... I know I must put as much energy if not as much interest into my work as if he were alive, because that is what he would like me to do.
Turing's relationship with Morcom's mother continued long after Morcom's death, with her sending gifts to Turing, and him sending letters, typically on Morcom's birthday. A day before the third anniversary of Morcom's death (13 February 1933), he wrote to Mrs. Morcom:

I expect you will be thinking of Chris when this reaches you. I shall too, and this letter is just to tell you that I shall be thinking of Chris and of you tomorrow. I am sure that he is as happy now as he was when he was here. Your affectionate Alan.
Some have speculated that Morcom's death was the cause of Turing's atheism and materialism. Apparently, at this point in his life he still believed in such concepts as a spirit, independent of the body and surviving death. In a later letter, also written to Morcom's mother, Turing wrote:

Personally, I believe that spirit is really eternally connected with matter but certainly not by the same kind of body ... as regards the actual connection between spirit and body I consider that the body can hold on to a 'spirit', whilst the body is alive and awake the two are firmly connected. When the body is asleep I cannot guess what happens but when the body dies, the 'mechanism' of the body, holding the spirit is gone and the spirit finds a new body sooner or later, perhaps immediately.

University and work on computability
After graduating from Sherborne, Turing applied for several Cambridge colleges scholarships, including Trinity and King's, eventually earning an £80 per annum scholarship (equivalent to about £4,300 as of 2023) to study at the latter. There, Turing studied the undergraduate course in Schedule B (that is, a three-year Parts I and II, of the Mathematical Tripos, with extra courses at the end of the third year, as Part III only emerged as a separate degree in 1934) from February 1931 to November 1934 at King's College, Cambridge, where he was awarded first-class honours in mathematics. His dissertation, On the Gaussian error function, written during his senior year and delivered in November 1934 (with a deadline date of 6 December) proved a version of the central limit theorem. It was finally accepted on 16 March 1935. By spring of that same year, Turing started his master's course (Part III)—which he completed in 1937—and, at the same time, he published his first paper, a one-page article called Equivalence of left and right almost periodicity (sent on 23 April), featured in the tenth volume of the Journal of the London Mathematical Society. Later that year, Turing was elected a Fellow of King's College on the strength of his dissertation where he served as a lecturer. However, and, unknown to Turing, this version of the theorem he proved in his paper, had already been proven, in 1922, by Jarl Waldemar Lindeberg. Despite this, the committee found Turing's methods original and so regarded the work worthy of consideration for the fellowship. Abram Besicovitch's report for the committee went so far as to say that if Turing's work had been published before Lindeberg's, it would have been "an important event in the mathematical literature of that year".
Between the springs of 1935 and 1936, at the same time as Church, Turing worked on the decidability of problems, starting from Gödel's incompleteness theorems. In mid-April 1936, Turing sent Max Newman the first draft typescript of his investigations. That same month, Alonzo Church published his An Unsolvable Problem of Elementary Number Theory, with similar conclusions to Turing's then-yet unpublished work. Finally, on 28 May of that year, he finished and delivered his 36-page paper for publication called "On Computable Numbers, with an Application to the Entscheidungsproblem". It was published in the Proceedings of the London Mathematical Society journal in two parts, the first on 30 November and the second on 23 December. In this paper, Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. The Entscheidungsproblem (decision problem) was originally posed by German mathematician David Hilbert in 1928. Turing proved that his "universal computing machine" would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the decision problem by first showing that the halting problem for Turing machines is undecidable: it is not possible to decide algorithmically whether a Turing machine will ever halt. This paper has been called "easily the most influential math paper in history".

Although Turing's proof was published shortly after Alonzo Church's equivalent proof using his lambda calculus, Turing's approach is considerably more accessible and intuitive than Church's. It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the Church–Turing thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper. To this day, Turing machines are a central object of study in theory of computation.
From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University, in the second year as a Jane Eliza Procter Visiting Fellow. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier. In June 1938, he obtained his PhD from the Department of Mathematics at Princeton; his dissertation, Systems of Logic Based on Ordinals, introduced the concept of ordinal logic and the notion of relative computing, in which Turing machines are augmented with so-called oracles, allowing the study of problems that cannot be solved by Turing machines. John von Neumann wanted to hire him as his postdoctoral assistant, but he went back to the United Kingdom.

Career and research
When Turing returned to Cambridge, he attended lectures given in 1939 by Ludwig Wittgenstein about the foundations of mathematics. The lectures have been reconstructed verbatim, including interjections from Turing and other students, from students' notes. Turing and Wittgenstein argued and disagreed, with Turing defending formalism and Wittgenstein propounding his view that mathematics does not discover any absolute truths, but rather invents them.

Cryptanalysis
During the Second World War, Turing was a leading participant in the breaking of German ciphers at Bletchley Park. The historian and wartime codebreaker Asa Briggs has said, "You needed exceptional talent, you needed genius at Bletchley and Turing's was that genius."
From September 1938, Turing worked part-time with the Government Code and Cypher School (GC&CS), the British codebreaking organisation. He concentrated on cryptanalysis of the Enigma cipher machine used by Nazi Germany, together with Dilly Knox, a senior GC&CS codebreaker. Soon after the July 1939 meeting near Warsaw at which the Polish Cipher Bureau gave the British and French details of the wiring of Enigma machine's rotors and their method of decrypting Enigma machine's messages, Turing and Knox developed a broader solution. The Polish method relied on an insecure indicator procedure that the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe (an improvement on the Polish Bomba).

On 4 September 1939, the day after the UK declared war on Germany, Turing reported to Bletchley Park, the wartime station of GC&CS. Like all others who came to Bletchley, he was required to sign the Official Secrets Act, in which he agreed not to disclose anything about his work at Bletchley, with severe legal penalties for violating the Act.
Specifying the bombe was the first of five major cryptanalytical advances that Turing made during the war. The others were: deducing the indicator procedure used by the German navy; developing a statistical procedure dubbed Banburismus for making much more efficient use of the bombes; developing a procedure dubbed Turingery for working out the cam settings of the wheels of the Lorenz SZ 40/42 (Tunny) cipher machine and, towards the end of the war, the development of a portable secure voice scrambler at Hanslope Park that was codenamed Delilah.
By using statistical techniques to optimise the trial of different possibilities in the code breaking process, Turing made an innovative contribution to the subject. He wrote two papers discussing mathematical approaches, titled The Applications of Probability to Cryptography and Paper on Statistics of Repetitions, which were of such value to GC&CS and its successor GCHQ that they were not released to the UK National Archives until April 2012, shortly before the centenary of his birth. A GCHQ mathematician, "who identified himself only as Richard," said at the time that the fact that the contents had been restricted under the Official Secrets Act for some 70 years demonstrated their importance, and their relevance to post-war cryptanalysis:

[He] said the fact that the contents had been restricted "shows what a tremendous importance it has in the foundations of our subject". ... The papers detailed using "mathematical analysis to try and determine which are the more likely settings so that they can be tried as quickly as possible". ... Richard said that GCHQ had now "squeezed the juice" out of the two papers and was "happy for them to be released into the public domain".
Turing had a reputation for eccentricity at Bletchley Park. He was known to his colleagues as "Prof" and his treatise on Enigma was known as the "Prof's Book". According to historian Ronald Lewin, Jack Good, a cryptanalyst who worked with Turing, said of his colleague:

In the first week of June each year he would get a bad attack of hay fever, and he would cycle to the office wearing a service gas mask to keep the pollen off. His bicycle had a fault: the chain would come off at regular intervals. Instead of having it mended he would count the number of times the pedals went round and would get off the bicycle in time to adjust the chain by hand. Another of his eccentricities is that he chained his mug to the radiator pipes to prevent it being stolen.
Peter Hilton recounted his experience working with Turing in Hut 8 in his "Reminiscences of Bletchley Park" from A Century of Mathematics in America:

 It is a rare experience to meet an authentic genius. Those of us privileged to inhabit the world of scholarship are familiar with the intellectual stimulation furnished by talented colleagues. We can admire the ideas they share with us and are usually able to understand their source; we may even often believe that we ourselves could have created such concepts and originated such thoughts. However, the experience of sharing the intellectual life of a genius is entirely different; one realizes that one is in the presence of an intelligence, a sensibility of such profundity and originality that one is filled with wonder and excitement.
Alan Turing was such a genius, and those, like myself, who had the astonishing and unexpected opportunity, created by the strange exigencies of the Second World War, to be able to count Turing as colleague and friend will never forget that experience, nor can we ever lose its immense benefit to us.
Hilton echoed similar thoughts in the Nova PBS documentary Decoding Nazi Secrets.
While working at Bletchley, Turing, who was a talented long-distance runner, occasionally ran the 40 miles (64 km) to London when he was needed for meetings, and he was capable of world-class marathon standards. Turing tried out for the 1948 British Olympic team, but he was hampered by an injury. His tryout time for the marathon was only 11 minutes slower than British silver medallist Thomas Richards' Olympic race time of 2 hours 35 minutes. He was Walton Athletic Club's best runner, a fact discovered when he passed the group while running alone. When asked why he ran so hard in training he replied:

I have such a stressful job that the only way I can get it out of my mind is by running hard; it's the only way I can get some release.
Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war. However, official war historian Harry Hinsley estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.
At the end of the war, a memo was sent to all those who had worked at Bletchley Park, reminding them that the code of silence dictated by the Official Secrets Act did not end with the war but would continue indefinitely. Thus, even though Turing was appointed an Officer of the Order of the British Empire (OBE) in 1946 by King George VI for his wartime services, his work remained secret for many years.

Bombe
Within weeks of arriving at Bletchley Park, Turing had specified an electromechanical machine called the bombe, which could break Enigma more effectively than the Polish bomba kryptologiczna, from which its name was derived. The bombe, with an enhancement suggested by mathematician Gordon Welchman, became one of the primary tools, and the major automated one, used to attack Enigma-enciphered messages.

The bombe searched for possible correct settings used for an Enigma message (i.e., rotor order, rotor settings and plugboard settings) using a suitable crib: a fragment of probable plaintext. For each possible setting of the rotors (which had on the order of 1019 states, or 1022 states for the four-rotor U-boat variant), the bombe performed a chain of logical deductions based on the crib, implemented electromechanically.
The bombe detected when a contradiction had occurred and ruled out that setting, moving on to the next. Most of the possible settings would cause contradictions and be discarded, leaving only a few to be investigated in detail. A contradiction would occur when an enciphered letter would be turned back into the same plaintext letter, which was impossible with the Enigma. The first bombe was installed on 18 March 1940.

Action This Day
By late 1941, Turing and his fellow cryptanalysts Gordon Welchman, Hugh Alexander and Stuart Milner-Barry were frustrated. Building on the work of the Poles, they had set up a good working system for decrypting Enigma signals, but their limited staff and bombes meant they could not translate all the signals. In the summer, they had considerable success, and shipping losses had fallen to under 100,000 tons a month; however, they badly needed more resources to keep abreast of German adjustments. They had tried to get more people and fund more bombes through the proper channels, but had failed.
On 28 October they wrote directly to Winston Churchill explaining their difficulties, with Turing as the first named. They emphasised how small their need was compared with the vast expenditure of men and money by the forces and compared with the level of assistance they could offer to the forces. As Andrew Hodges, biographer of Turing, later wrote, "This letter had an electric effect." Churchill wrote a memo to General Ismay, which read: "ACTION THIS DAY. Make sure they have all they want on extreme priority and report to me that this has been done." On 18 November, the chief of the secret service reported that every possible measure was being taken. The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, "All that we did notice was that almost from that day the rough ways began miraculously to be made smooth." More than two hundred bombes were in operation by the end of the war.

Hut 8 and the naval Enigma
Turing decided to tackle the particularly difficult problem of cracking the German naval use of Enigma "because no one else was doing anything about it and I could have it to myself". In December 1939, Turing solved the essential part of the naval indicator system, which was more complex than the indicator systems used by the other services.
That same night, he also conceived of the idea of Banburismus, a sequential statistical technique (what Abraham Wald later called sequential analysis) to assist in breaking the naval Enigma, "though I was not sure that it would work in practice, and was not, in fact, sure until some days had actually broken". For this, he invented a measure of weight of evidence that he called the ban. Banburismus could rule out certain sequences of the Enigma rotors, substantially reducing the time needed to test settings on the bombes. Later this sequential process of accumulating sufficient weight of evidence using decibans (one tenth of a ban) was used in cryptanalysis of the Lorenz cipher.
Turing travelled to the United States in November 1942 and worked with US Navy cryptanalysts on the naval Enigma and bombe construction in Washington. He also visited their Computing Machine Laboratory in Dayton, Ohio.
Turing's reaction to the American bombe design was far from enthusiastic:

The American Bombe programme was to produce 336 Bombes, one for each wheel order. I used to smile inwardly at the conception of Bombe hut routine implied by this programme, but thought that no particular purpose would be served by pointing out that we would not really use them in that way.
Their test (of commutators) can hardly be considered conclusive as they were not testing for the bounce with electronic stop finding devices. Nobody seems to be told about rods or offiziers or banburismus unless they are really going to do something about it.
During this trip, he also assisted at Bell Labs with the development of secure speech devices. He returned to Bletchley Park in March 1943. During his absence, Hugh Alexander had officially assumed the position of head of Hut 8, although Alexander had been de facto head for some time (Turing having little interest in the day-to-day running of the section). Turing became a general consultant for cryptanalysis at Bletchley Park.
Alexander wrote of Turing's contribution:

There should be no question in anyone's mind that Turing's work was the biggest factor in Hut 8's success. In the early days, he was the only cryptographer who thought the problem worth tackling and not only was he primarily responsible for the main theoretical work within the Hut, but he also shared with Welchman and Keen the chief credit for the invention of the bombe. It is always difficult to say that anyone is 'absolutely indispensable', but if anyone was indispensable to Hut 8, it was Turing. The pioneer's work always tends to be forgotten when experience and routine later make everything seem easy and many of us in Hut 8 felt that the magnitude of Turing's contribution was never fully realised by the outside world.

Turingery
In July 1942, Turing devised a technique termed Turingery (or jokingly Turingismus) for use against the Lorenz cipher messages produced by the Germans' new Geheimschreiber (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed Tunny at Bletchley Park. Turingery was a method of wheel-breaking, i.e., a procedure for working out the cam settings of Tunny's wheels. He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages. Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher, but he was not directly involved in the Colossus development.

Delilah
Following his work at Bell Labs in the US, Turing pursued the idea of electronic enciphering of speech in the telephone system. In the latter part of the war, he moved to work for the Secret Service's Radio Security Service (later HMGCC) at Hanslope Park. At the park, he further developed his knowledge of electronics with the assistance of REME officer Donald Bayley. Together they undertook the design and construction of a portable secure voice communications machine codenamed Delilah. The machine was intended for different applications, but it lacked the capability for use with long-distance radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a recording of a Winston Churchill speech, Delilah was not adopted for use. Turing also consulted with Bell Labs on the development of SIGSALY, a secure voice system that was used in the later years of the war.

Early computers and the Turing test
Between 1945 and 1947, Turing lived in Hampton, London, while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer. Von Neumann's incomplete First Draft of a Report on the EDVAC had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it "contains a number of ideas which are Dr. Turing's own".
Although ACE was a feasible design, the effect of the Official Secrets Act surrounding the wartime work at Bletchley Park made it impossible for Turing to explain the basis of his analysis of how a computer installation involving human operators would work. This led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on Intelligent Machinery that was not published in his lifetime. While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.
According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Düsseldorf, there was a meeting between Turing and Konrad Zuse. It took place in Göttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing (for more details see Herbert Bruderer, Konrad Zuse und die Schweiz).
In 1948, Turing was appointed reader in the Mathematics Department at the Victoria University of Manchester. A year later, he became deputy director of the Computing Machine Laboratory, where he worked on software for one of the earliest stored-program computers—the Manchester Mark 1. Turing wrote the first version of the Programmer's Manual for this machine, and was recruited by Ferranti as a consultant in the development of their commercialised machine, the Ferranti Mark 1. He continued to be paid consultancy fees by Ferranti until his death. During this time, he continued to do more abstract work in mathematics, and in "Computing Machinery and Intelligence" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called "intelligent". The idea was that a computer could be said to "think" if a human interrogator could not tell it apart, through conversation, from a human being. In the paper, Turing suggested that rather than building a program to simulate the adult mind, it would be better to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.
In 1948, Turing, working with his former undergraduate colleague, D.G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the program was completed and dubbed the Turochamp. In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the program. Instead, Turing "ran" the program by flipping through the pages of the algorithm and carrying out its instructions on a chessboard, taking about half an hour per move. The game was recorded. According to Garry Kasparov, Turing's program "played a recognizable game of chess". The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife, Isabel.
His Turing test was a significant, characteristically provocative, and lasting contribution to the debate regarding artificial intelligence, which continues after more than half a century.

Pattern formation and mathematical biology
When Turing was 39 years old in 1951, he turned to mathematical biology, finally publishing his masterpiece "The Chemical Basis of Morphogenesis" in January 1952. He was interested in morphogenesis, the development of patterns and shapes in biological organisms. He suggested that a system of chemicals reacting with each other and diffusing across space, termed a reaction–diffusion system, could account for "the main phenomena of morphogenesis". He used systems of partial differential equations to model catalytic chemical reactions. For example, if a catalyst A is required for a certain chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear differential equations. Turing discovered that patterns could be created if the chemical reaction not only produced catalyst A, but also produced an inhibitor B that slowed down the production of A. If A and B then diffused through the container at different rates, then you could have some regions where A dominated and some where B did. To calculate the extent of this, Turing would have needed a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and produced, for example, a uniform mixture that oddly enough had regularly spaced fixed red spots. The Russian biochemist Boris Belousov had performed experiments with similar results, but could not get his papers published because of the contemporary prejudice that any such thing violated the second law of thermodynamics. Belousov was not aware of Turing's paper in the Philosophical Transactions of the Royal Society.
Although published before the structure and role of DNA was understood, Turing's work on morphogenesis remains relevant today and is considered a seminal piece of work in mathematical biology. One of the early applications of Turing's paper was the work by James Murray explaining spots and stripes on the fur of cats, large and small. Further research in the area suggests that Turing's work can partially explain the growth of "feathers, hair follicles, the branching pattern of lungs, and even the left-right asymmetry that puts the heart on the left side of the chest". In 2012, Sheth, et al. found that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the wavelength of a Turing-type mechanism. Later papers were not available until Collected Works of A. M. Turing was published in 1992.
A study conducted in 2023 confirmed Turing's mathematical model hypothesis. Presented by the American Physical Society, the experiment involved growing chia seeds in even layers within trays, later adjusting the available moisture. Researchers experimentally tweaked the factors which appear in the Turing equations, and, as a result, patterns resembling those seen in natural environments emerged. This is believed to be the first time that experiments with living vegetation have verified Turing's mathematical insight.

Personal life
Treasure
In the 1940s, Turing became worried about losing his savings in the event of a German invasion. In order to protect it, he bought two silver bars weighing 3,200 oz (90 kg) and worth £250 (in 2022, £8,000 adjusted for inflation, £48,000 at spot price) and buried them in a wood near Bletchley Park. Upon returning to dig them up, Turing found that he was unable to break his own code describing where exactly he had hidden them. This, along with the fact that the area had been renovated, meant that he never regained the silver.

Engagement
In 1941, Turing proposed marriage to Hut 8 colleague Joan Clarke, a fellow mathematician and cryptanalyst, but their engagement was short-lived. After admitting his homosexuality to his fiancée, who was reportedly "unfazed" by the revelation, Turing decided that he could not go through with the marriage.

Homosexuality and indecency conviction
In January 1952, Turing was 39 when he met Arnold Murray, a 19-year-old unemployed man. The two agreed to meet again and in 1952 began an intimate relationship. Just before Christmas, Turing was walking along Manchester's Oxford Road when he met Murray just outside the Regal Cinema and invited him to lunch. On 23 January, Turing's house was burgled. Murray told Turing that he and the burglar were acquainted, and Turing reported the crime to the police. During the investigation, he acknowledged a sexual relationship with Murray. Homosexual acts were criminal offences in the United Kingdom at that time, and both men were charged with "gross indecency" under Section 11 of the Criminal Law Amendment Act 1885. Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor "reserved his defence", i.e., did not argue or provide evidence against the allegations. The proceedings were held at the Sessions House in Knutsford.
Turing was later convinced by the advice of his brother and his own solicitor, and he entered a plea of guilty. The case, Regina v. Turing and Murray, was brought to trial on 31 March 1952. Turing was convicted and given a choice between imprisonment and probation. His probation would be conditional on his agreement to undergo hormonal physical changes designed to reduce libido, known as "chemical castration". He accepted the option of injections of what was then called stilboestrol (now known as diethylstilbestrol or DES), a synthetic oestrogen; this feminization of his body was continued for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form. In a letter, Turing wrote that "no doubt I shall emerge from it all a different man, but quite who I've not found out". Murray was given a conditional discharge.
Turing's conviction led to the removal of his security clearance and barred him from continuing with his cryptographic consultancy for the Government Communications Headquarters (GCHQ), the British signals intelligence agency that had evolved from GC&CS in 1946, though he kept his academic job. His trial took place only months after the defection to the Soviet Union of Guy Burgess and Donald Maclean in summer 1951 after which the Foreign Office started to consider anyone known to be homosexual as a potential security risk.
Turing was denied entry into the United States after his conviction in 1952, but was free to visit other European countries. In the summer of 1952 he visited Norway which was more tolerant of homosexuals. Among the various men he met there was one named Kjell Carlson. Kjell intended to visit Turing in the UK but the authorities intercepted Kjell's postcard detailing his travel arrangements and were able to intercept and deport him before the two could meet. It was also during this time that Turing started consulting a psychiatrist, Dr Franz Greenbaum, with whom he got on well and who subsequently became a family friend.

Death
On 8 June 1954, at his house at 43 Adlington Road, Wilmslow, Turing's housekeeper found him dead. A post mortem was held that evening, which determined that he had died the previous day at age 41 with cyanide poisoning cited as the cause of death. When his body was discovered, an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide, it was speculated that this was the means by which Turing had consumed a fatal dose.
Turing's brother, John, identified the body the following day and took the advice given by Dr. Greenbaum to accept the verdict of the inquest, as there was little prospect of establishing that the death was accidental. The inquest was held the following day, which determined the cause of death to be suicide. Turing's remains were cremated at Woking Crematorium just two days later on 12 June 1954 with just three people attending and his ashes were scattered in the gardens of the crematorium, just as his father's had been. Turing's mother was on holiday in Italy at the time of his death and returned home after the inquest. She never accepted the verdict of suicide.
Philosopher Jack Copeland has questioned various aspects of the coroner's historical verdict. He suggested an alternative explanation for the cause of Turing's death: the accidental inhalation of cyanide fumes from an apparatus used to electroplate gold onto spoons. The potassium cyanide was used to dissolve the gold. Turing had such an apparatus set up in his tiny spare room. Copeland noted that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before going to bed, and it was not unusual for the apple to be discarded half-eaten. Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) "with good humour" and had shown no sign of despondency before his death. He even set down a list of tasks that he intended to complete upon returning to his office after the holiday weekend. Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals. 
Turing's biographer Andrew Hodges theorised that Turing deliberately left the nature of his death ambiguous in order to shield his mother from the knowledge that he had killed himself. Doubts on the suicide thesis have been also cast by John W. Dawson Jr. who, in his review of Hodges' book, recalls "Turing's vulnerable position in the Cold War political climate" and points out that "Turing was found dead by a maid, who discovered him 'lying neatly in his bed'—hardly what one would expect of "a man fighting for life against the suffocation induced by cyanide poisoning." Turing had given no hint of suicidal inclinations to his friends and had made no effort to put his affairs in order.
Hodges and a later biographer, David Leavitt, have both speculated that Turing was re-enacting a scene from the Walt Disney film Snow White and the Seven Dwarfs (1937), his favourite fairy tale. Both men noted that (in Leavitt's words) he took "an especially keen pleasure in the scene where the Wicked Queen immerses her apple in the poisonous brew".

It has also been suggested that Turing's belief in fortune-telling may have caused his depressed mood. As a youth, Turing had been told by a fortune-teller that he would be a genius. In mid-May 1954, shortly before his death, Turing again decided to consult a fortune-teller during a day-trip to St Annes-on-Sea with the Greenbaum family. According to the Greenbaums' daughter, Barbara:

But it was a lovely sunny day and Alan was in a cheerful mood and off we went ... Then he thought it would be a good idea to go to the Pleasure Beach at Blackpool. We found a fortune-teller's tent and Alan said he'd like to go in[,] so we waited around for him to come back ... And this sunny, cheerful visage had shrunk into a pale, shaking, horror-stricken face. Something had happened. We don't know what the fortune-teller said but he obviously was deeply unhappy. I think that was probably the last time we saw him before we heard of his suicide.

Government apology and pardon
In August 2009, British programmer John Graham-Cumming started a petition urging the British government to apologise for Turing's prosecution as a homosexual. The petition received more than 30,000 signatures. The prime minister, Gordon Brown, acknowledged the petition, releasing a statement on 10 September 2009 apologising and describing the treatment of Turing as "appalling":

Thousands of people have come together to demand justice for Alan Turing and recognition of the appalling way he was treated. While Turing was dealt with under the law of the time and we can't put the clock back, his treatment was of course utterly unfair and I am pleased to have the chance to say how deeply sorry I and we all are for what happened to him ... So on behalf of the British government, and all those who live freely thanks to Alan's work I am very proud to say: we're sorry, you deserved so much better.
In December 2011, William Jones and his member of Parliament, John Leech, created an e-petition requesting that the British government pardon Turing for his conviction of "gross indecency":

We ask the HM Government to grant a pardon to Alan Turing for the conviction of "gross indecency". In 1952, he was convicted of "gross indecency" with another man and was forced to undergo so-called "organo-therapy"—chemical castration. Two years later, he killed himself with cyanide, aged just 41. Alan Turing was driven to a terrible despair and early death by the nation he'd done so much to save. This remains a shame on the British government and British history. A pardon can go some way to healing this damage. It may act as an apology to many of the other gay men, not as well-known as Alan Turing, who were subjected to these laws.
The petition gathered over 37,000 signatures, and was submitted to Parliament by the Manchester MP John Leech but the request was discouraged by Justice Minister Lord McNally, who said:

A posthumous pardon was not considered appropriate as Alan Turing was properly convicted of what at the time was a criminal offence. He would have known that his offence was against the law and that he would be prosecuted. It is tragic that Alan Turing was convicted of an offence that now seems both cruel and absurd—particularly poignant given his outstanding contribution to the war effort. However, the law at the time required a prosecution and, as such, long-standing policy has been to accept that such convictions took place and, rather than trying to alter the historical context and to put right what cannot be put right, ensure instead that we never again return to those times.
John Leech, the MP for Manchester Withington (2005–15), submitted several bills to Parliament and led a high-profile campaign to secure the pardon. Leech made the case in the House of Commons that Turing's contribution to the war made him a national hero and that it was "ultimately just embarrassing" that the conviction still stood. Leech continued to take the bill through Parliament and campaigned for several years, gaining the public support of numerous leading scientists, including Stephen Hawking. At the British premiere of a film based on Turing's life, The Imitation Game, the producers thanked Leech for bringing the topic to public attention and securing Turing's pardon. Leech is now regularly described as the "architect" of Turing's pardon and subsequently the Alan Turing Law which went on to secure pardons for 75,000 other men and women convicted of similar crimes.
On 26 July 2012, a bill was introduced in the House of Lords to grant a statutory pardon to Turing for offences under section 11 of the Criminal Law Amendment Act 1885, of which he was convicted on 31 March 1952. Late in the year in a letter to The Daily Telegraph, the physicist Stephen Hawking and 10 other signatories including the Astronomer Royal Lord Rees, President of the Royal Society Sir Paul Nurse, Lady Trumpington (who worked for Turing during the war) and Lord Sharkey (the bill's sponsor) called on Prime Minister David Cameron to act on the pardon request. The government indicated it would support the bill, and it passed its third reading in the House of Lords in October.
At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014, but before the bill could be debated in the House of Commons, the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for "gross indecency", with immediate effect. Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be "remembered and recognised for his fantastic contribution to the war effort" and not for his later criminal conviction. The Queen officially pronounced Turing pardoned in August 2014. The Queen's action is only the fourth royal pardon granted since the conclusion of the Second World War. Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.
In September 2016, the government announced its intention to expand this retroactive exoneration to other men convicted of similar historical indecency offences, in what was described as an "Alan Turing law". The Alan Turing law is now an informal term for the law in the United Kingdom, contained in the Policing and Crime Act 2017, which serves as an amnesty law to retroactively pardon men who were cautioned or convicted under historical legislation that outlawed homosexual acts. The law applies in England and Wales.
On 19 July 2023, following an apology to LGBT veterans from the UK Government, Defence Secretary Ben Wallace suggested Turing should be honoured with a permanent statue on the fourth plinth of Trafalgar Square, describing Turing as "probably the greatest war hero, in my book, of the Second World War, [whose] achievements shortened the war, saved thousands of lives, helped defeat the Nazis. And his story is a sad story of a society and how it treated him."

Publications
Turing, A. M. (1938). "On Computable Numbers, with an Application to the Entscheidungsproblem: A correction". Proceedings of the London Mathematical Society. 2. 43 (1) (published 1937): 544–46. doi:10.1112/plms/s2-43.6.544.
Turing, Alan (1950). "Computing Machinery and Intelligence" (PDF). Mind. 49 (236): 433–460. doi:10.1093/mind/LIX.236.433. Archived (PDF) from the original on 9 October 2022.

See also
Legacy of Alan Turing
List of things named after Alan Turing

References
Notes
Citations
Works cited
Further reading
Articles
Books
External links

Oral history interview with Nicholas C. Metropolis, Charles Babbage Institute, University of Minnesota. Metropolis was the first director of computing services at Los Alamos National Laboratory; topics include the relationship between Turing and John von Neumann
How Alan Turing Cracked The Enigma Code Imperial War Museums
Alan Turing Year Archived 17 February 2019 at the Wayback Machine
CiE 2012: Turing Centenary Conference
Science in the MakingArchived 4 April 2023 at the Wayback Machine Alan Turing's papers in the Royal Society's archives
Alan Turing site maintained by Andrew Hodges including a short biography
AlanTuring.net – Turing Archive for the History of Computing by Jack Copeland
The Turing Digital Archive – contains scans of some unpublished documents and material from the King's College, Cambridge archive
Alan Turing Papers – University of Manchester Library, Manchester
Jones, G. James (11 December 2001). "Alan Turing – Towards a Digital Mind: Part 1". System Toolbox. The Binary Freedom Project. Archived from the original on 3 August 2007.
Sherborne School Archives – holds papers relating to Turing's time at Sherborne School
Alan Turing plaques recorded on openplaques.org
Alan Turing archive on New Scientist
AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.
The three formed team SuperVision and submitted AlexNet in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up.  The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.

Historic context
AlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU. A deep CNN of Dan Cireșan et al. (2011) at IDSIA was 60 times faster than a CPU version. Between May 15, 2011, and September 10, 2012, their CNN won four image competitions and achieved SOTA for multiple image databases.
According to the AlexNet paper, Cireșan's earlier net is "somewhat similar." Both were originally written with CUDA to run with GPU support. In fact, both are actually just variants of the CNN designs introduced by Yann LeCun et al. (1989) who applied the backpropagation algorithm to a variant of Kunihiko Fukushima's original CNN architecture called "neocognitron." The architecture was later modified by J. Weng's method called max-pooling.
The training set had 1.2 million images. It was trained for 90 epochs, which took five to six days on two NVIDIA GTX 580 3GB GPUs.
In 2015, AlexNet was outperformed by a Microsoft Research Asia project with over 100 layers, which won the ImageNet 2015 contest.

Network design
AlexNet contains eight layers: the first five are convolutional layers, some of them followed by max-pooling layers, and the last three are fully connected layers. The network, except the last layer, is split into two copies, each run on one GPU. The entire structure can be written as: 
  
    
      
        (
        
          
            C
            N
            N
          
        
        →
        
          
            R
            N
          
        
        →
        
          
            M
            P
          
        
        
          )
          
            2
          
        
        →
        (
        
          
            
              C
              N
              N
            
          
          
            3
          
        
        →
        
          
            M
            P
          
        
        )
        →
        (
        
          
            F
            C
          
        
        →
        
          
            D
            O
          
        
        
          )
          
            2
          
        
        →
        
          
            L
            i
            n
            e
            a
            r
          
        
        →
        
          
            s
            o
            f
            t
            m
            a
            x
          
        
      
    
    {\displaystyle ({\mathit {CNN}}\to {\mathit {RN}}\to {\mathit {MP}})^{2}\to ({\mathit {CNN}}^{3}\to {\mathit {MP}})\to ({\mathit {FC}}\to {\mathit {DO}})^{2}\to {\mathit {Linear}}\to {\mathit {softmax}}}
  
 where

CNN = convolutional layer (with ReLU activation)
RN = local response normalization
MP = maxpooling
FC = fully connected layer (with ReLU activation)
Linear = fully connected layer (without activation)
DO = dropout
It used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid.

Influence
AlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of mid 2024, the AlexNet paper has been cited over 157,000 times according to Google Scholar.


== References ==
Alex Graves is a computer scientist and research scientist at DeepMind.

Education
Graves earned his Bachelor of Science degree in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence from the Technical University of Munich supervised by Jürgen Schmidhuber at the Dalle Molle Institute for Artificial Intelligence Research.

Career and research
After his PhD, Graves was postdoc working with Schmidhuber at the Technical University of Munich and Geoffrey Hinton at the University of Toronto.
At the Dalle Molle Institute for Artificial Intelligence Research, Graves trained long short-term memory (LSTM) neural networks by a novel method called connectionist temporal classification (CTC). This method outperformed traditional speech recognition models in certain applications. In 2009, his CTC-trained LSTM was the first recurrent neural network (RNN) to win pattern recognition contests, winning several competitions in connected handwriting recognition.
Google uses CTC-trained LSTM for speech recognition on the smartphone.
Graves is also the creator of neural Turing machines and the closely related differentiable neural computer.
In 2023, he published the paper Bayesian Flow Networks.


== References ==
In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning). 
In contrast, a heuristic is an approach to problem-solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.  For example, although social media recommender systems are commonly called "algorithms", they actually rely on heuristics as there is no truly "correct" recommendation.
As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

Etymology
Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī ("Book of Indian computation") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī ("Addition and subtraction in Indian arithmetic"). In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or "Thus spoke Al-Khwarizmi". Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term. In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, "number"; cf. "arithmetic"), the Latin word was altered to algorithmus.

Definition
One informal definition is "a set of rules that precisely defines a sequence of operations", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure
or cook-book recipe. In general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.

History
Ancient algorithms
Since antiquity, step-by-step procedures for solving mathematical problems have been recorded. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), and Arabic mathematics (around 800 AD).
The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.
Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta. 
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.

Computers
Weight-driven clocks
Bolter credits the invention of the weight-driven clock as "the key invention [of Europe in the Middle Ages]," specifically the verge escapement mechanism that provides the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Lovelace is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.

Electromechanical relay
Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and "telephone switching technologies" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.
Telephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the "burdensome" use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".

Formalization
In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.

Representations
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form, but are also used to define or document algorithms.

Turing machines
There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called "sets of quadruples", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data in order to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.

Flowchart representation
The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.

Algorithmic analysis
It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
⁠, otherwise ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ is required.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
⁠) outperforms a sequential search (cost ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ ) when used for table lookups on sorted lists or arrays.

Formal versus empirical
The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many "one-off" problems but it may be critical for algorithms designed for fast interactive, commercial or long life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.

Execution efficiency
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.

Design
Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.

Structured programming
Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.

Legal status
By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

Classification
By implementation
Recursion
A recursive algorithm is one that invokes itself repeatedly until it meets a termination condition, and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other.Towers of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.
Deterministic or non-deterministic
Deterministic algorithms solve the problem with exact decision at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.
Quantum algorithm
Quantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.

By design paradigm
Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:

Brute-force or exhaustive search
Brute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords.
Divide and conquer
A divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.
Back tracking
In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.

Optimization problems
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.

Examples
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:
High-level description:

If there are no numbers in the set, then there is no highest number.
Assume the first number in the set is the largest number in the set.
For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

See also
Notes
Bibliography
Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363

Further reading
External links

"Algorithm". Encyclopedia of Mathematics. EMS Press. 2001 [1994].
Weisstein, Eric W. "Algorithm". MathWorld.
Dictionary of Algorithms and Data Structures – National Institute of Standards and Technology
Algorithm repositories
The Stony Brook Algorithm Repository – State University of New York at Stony Brook
Collected Algorithms of the ACM – Associations for Computing Machinery
The Stanford GraphBase Archived December 6, 2015, at the Wayback Machine – Stanford University
In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning). 
In contrast, a heuristic is an approach to problem-solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.  For example, although social media recommender systems are commonly called "algorithms", they actually rely on heuristics as there is no truly "correct" recommendation.
As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

Etymology
Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī ("Book of Indian computation") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī ("Addition and subtraction in Indian arithmetic"). In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or "Thus spoke Al-Khwarizmi". Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term. In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, "number"; cf. "arithmetic"), the Latin word was altered to algorithmus.

Definition
One informal definition is "a set of rules that precisely defines a sequence of operations", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure
or cook-book recipe. In general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.

History
Ancient algorithms
Since antiquity, step-by-step procedures for solving mathematical problems have been recorded. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), and Arabic mathematics (around 800 AD).
The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.
Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta. 
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.

Computers
Weight-driven clocks
Bolter credits the invention of the weight-driven clock as "the key invention [of Europe in the Middle Ages]," specifically the verge escapement mechanism that provides the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Lovelace is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.

Electromechanical relay
Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and "telephone switching technologies" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.
Telephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the "burdensome" use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".

Formalization
In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.

Representations
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form, but are also used to define or document algorithms.

Turing machines
There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called "sets of quadruples", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data in order to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.

Flowchart representation
The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.

Algorithmic analysis
It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
⁠, otherwise ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ is required.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
⁠) outperforms a sequential search (cost ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ ) when used for table lookups on sorted lists or arrays.

Formal versus empirical
The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many "one-off" problems but it may be critical for algorithms designed for fast interactive, commercial or long life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.

Execution efficiency
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.

Design
Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.

Structured programming
Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.

Legal status
By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

Classification
By implementation
Recursion
A recursive algorithm is one that invokes itself repeatedly until it meets a termination condition, and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other.Towers of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.
Deterministic or non-deterministic
Deterministic algorithms solve the problem with exact decision at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.
Quantum algorithm
Quantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.

By design paradigm
Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:

Brute-force or exhaustive search
Brute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords.
Divide and conquer
A divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.
Back tracking
In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.

Optimization problems
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.

Examples
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:
High-level description:

If there are no numbers in the set, then there is no highest number.
Assume the first number in the set is the largest number in the set.
For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

See also
Notes
Bibliography
Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363

Further reading
External links

"Algorithm". Encyclopedia of Mathematics. EMS Press. 2001 [1994].
Weisstein, Eric W. "Algorithm". MathWorld.
Dictionary of Algorithms and Data Structures – National Institute of Standards and Technology
Algorithm repositories
The Stony Brook Algorithm Repository – State University of New York at Stony Brook
Collected Algorithms of the ACM – Associations for Computing Machinery
The Stanford GraphBase Archived December 6, 2015, at the Wayback Machine – Stanford University
Algorithmic bias describes systematic and repeatable errors in a computer system that create "unfair" outcomes, such as "privileging" one category over another in ways different from the intended function of the algorithm.
Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect "systematic and unfair" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024).
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single "algorithm" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.

Definitions
Algorithms are difficult to define, but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.: 13  For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence.: 14–15  By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more.
Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.: 2 : 563 : 294  The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased.: 332  This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).

Methods
Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria.: 3  Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.: 4  Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.: 8  Other algorithms may reinforce stereotypes and preferences as they process and display "relevant" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.: 6 
Beyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).: 36  Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. These criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.: 4

History
Early critiques
The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.: 149 
Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs "embody law",: 40  that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including their biases and expectations.: 109  While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects "human decisionmaking processes" as data is being selected.: 70, 105 
Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results.: 65  Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.: 226 
An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with "foreign-sounding names" based on historical trends in admissions. While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale.
In recent years, when more algorithms started to use machine learning methods on real world data, algorithmic bias can be found more often due to the bias existing in the data.

Contemporary critiques and responses
Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.: 115  In theory, these biases may create new patterns of behavior, or "scripts", in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.: 180 
The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,: 15  a process described by author Clay Shirky as "algorithmic authority". Shirky uses the term to describe "the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources", such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as "trending" or "popular" may be created based on significantly wider criteria than just their popularity.: 14 
Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.: 16 : 6  This can have the effect of reducing alternative options, compromises, or flexibility.: 16  Sociologist Scott Lash has critiqued algorithms as a new form of "generative power", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.: 71 
Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,
and Transparency in Machine Learning.: 115  Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.: 117  In recent years, the study of the Fairness, Accountability,
and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT. Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied.

Types
Pre-existing
Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.: 334 : 294  Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.: 17  Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm.: 116 : 8 
An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act.: 341  The program accurately reflected the tenets of the law, which stated that "a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.": 341 : 375  In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.: 342 
Another source of bias, which has been called "label choice bias", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely-used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients Solutions to the "label choice bias" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.

Machine learning bias
Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms. These biases can manifest in various ways and are often a reflection of the data used to train these algorithms. Here are some key aspects:

Language bias
Language bias refers a type of statistical sampling bias tied to the language of a query that leads to "a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository." Luo et al.'s work shows that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like "What is liberalism?", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like "opposes state intervention in personal and economic life" from the dominant Vietnamese perspective and "limitation of government power" from the prevalent Chinese perspective are absent.

Gender bias
Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms; it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.

Stereotyping
Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.
A recent focus in research has been on the complex interplay between the grammatical properties of a language and real-world biases that can become embedded in AI systems, potentially perpetuating harmful stereotypes and assumptions. The study on gender bias in language models trained on Icelandic, a highly grammatically gendered language, revealed that the models exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms, even for female-dominated professions. This suggests the models amplified societal gender biases present in the training data.

Political bias
Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.

Technical
Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.: 332  Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.: 336  Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.: 332 
A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.: 332  The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.: 574 
Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.: 332  Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.: 21–22

Emergent
Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.: 334  Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.: 334, 336  This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.: 179 : 294  Similarly, problems may emerge when training data (the samples "fed" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.
In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).: 338  The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.: 338 
Additional emergent biases include:

Correlations
Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.: 6  In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.

Unanticipated uses
Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.: 334  These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.: 179 
Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law.: 342

Feedback loops
Emergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing. Another well known example of such an algorithm exhibiting such behavior is COMPAS, a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on.
Recommender systems such as those used to recommend online videos or news articles can create feedback loops. When users click on content that is suggested by algorithms, it influences the next set of suggestions. Over time this may lead to users entering a filter bubble and being unaware of important or useful content.

Impact
Commercial influences
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.: 2 : 331 
In a 1998 paper describing Google, the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that "advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers." This bias would be an "invisible" manipulation of the user.: 3

Voting behavior
A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have "no means of competing" if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a "digital gerrymandering" effect in elections, "the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users", if intentionally manipulated.: 335

Gender discrimination
In 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, "Andrea" would bring up a prompt asking if users meant "Andrew", but queries for "Andrew" did not ask if users meant to find "Andrea". The company said this was the result of an analysis of users' interactions with the site.
In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners.: 94  Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.: 98 
Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, "lesbian". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, "Top 25 Sexiest Women Athletes" articles displayed as first-page results in searches for "women athletes".: 31  In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites. Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. In fact, current machine translation systems fail to reproduce the real world distribution of female workers.
In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women. The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word "women's". A similar problem emerged with music streaming services—In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists. Spotify's song recommendations suggested more male artists over women artists.

Racial and ethnic discrimination
Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.: 158  Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data.
In 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.: 154  Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.
Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name. A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function.
In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.
A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on "creditworthiness" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.

Law enforcement and legal proceedings
Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label "high-risk" as white defendants.
One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores.: 4  Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.
One study that set out to examine "Risk, Race, & Recidivism: Predictive Bias and Disparate Impact" alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.
In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored.

Online hate speech
In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing "Muslims" would be blocked, while posts denouncing "Radical Muslims" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the "children" subset of blacks, rather than "all blacks", whereas "all white men" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target "Jew haters" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.
While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in African American English. Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged.

Surveillance
Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.: 572  The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technology's context will always have a disproportionate impact on communities whose actions are over-surveilled. For example, a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.: 190  A 2018 study found that facial recognition software most likely accurately identified light-skinned (typically European) males, with slightly lower accuracy rates for light-skinned females. Dark-skinned males and females were significanfly less likely to be accurately identified by facial recognition software. These disparities are attributed to the under-representation of darker-skinned participants in data sets used to develop this software.

Discrimination against the LGBTQ community
In 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its "adult content" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain.: 5 
In 2019, it was found that on Facebook, searches for "photos of my female friends" yielded suggestions such as "in bikinis" or "at the beach". In contrast, searches for "photos of my male friends" yielded no results.
Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.
There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individual's sexual orientation based on their facial images. The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being "outed" against their will.

Disability discrimination
While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias – like gender, race and socioeconomic status, disability often is left out of the list. The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusion
The shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations.  The definition of disability has been long debated shifting from a medical model to a social model of disability most recently, which establishes that disability is a result of the mismatch between people's interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary, considered in a constant state of flux. Disabilities are incredibly diverse, fall within a large spectrum, and can be unique to each individual. People's identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support.  The high level of variability across people's experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences are excluded from statistics and datasets, hence underrepresented and nonexistent in training data. Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias. For example, if people with speech impairments are not included in training voice control features and smart AI assistants –they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor.
Given the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges. As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures.

Google Search
While users generate results that are "completed" automatically, Google has failed to remove sexist and racist autocompletion text. For example, Algorithms of Oppression: How Search Engines Reinforce Racism Safiya Noble notes an example of the search for "black girls", which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful.

Obstacles to research
Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.: 5

Defining fairness
Literature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization. For example, defining fairness as an "equality of outcomes" may simply refer to a system producing the same result for all people, while fairness defined as "equality of treatment" might explicitly consider differences between individuals.: 2  As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems.: 2  In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with "fairness" defined for specific applications and contexts.

Complexity
Algorithmic processes are complex, often exceeding the understanding of the people who use them.: 2 : 7  Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.: 183  Social scientist Bruno Latour has identified this process as blackboxing, a process in which "scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become." Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.: 92 
An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.: 118  Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.: 22 
Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.: 367 : 7  One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.: 5 
Companies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.: 5

Lack of transparency
Commercial algorithms are proprietary, and may be treated as trade secrets.: 2 : 7 : 183  Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings.: 366  This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.: 20  Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.: 369  Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.

Lack of data about sensitive categories
A significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.
Some practitioners have tried to estimate and impute these missing sensitive categorizations in order to allow bias mitigation, for example building systems to infer ethnicity from names, however this can introduce other forms of bias if not undertaken with care. Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.
Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult. Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.

Solutions
A study of 84 policy guidelines on ethical AI found that fairness and "mitigation of unwanted bias" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts.

Technical
There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion). Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model. Using machine learning to detect bias is called, "conducting an AI audit", where the "auditor" is an algorithm that goes through the AI model and the training data to identify biases.
Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information
from its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended
by a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this
problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected
and sensitive information about the subject, as first demonstrated in  where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic.
Currently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.

Transparency and monitoring
Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the "right to understanding" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for "Explainable AI" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.
An initial approach towards transparency included the open-sourcing of algorithms. Software code can be looked into and improvements can be proposed through source-code-hosting facilities. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.

Right to remedy
From a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias. This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes. Others propose the need for clear liability insurance mechanisms.

Diversity and inclusion
Amid concerns that the design of AI systems is primarily the domain of white, male engineers, a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems. For example, just 12% of machine learning engineers are women, with black AI leaders pointing to a "diversity crisis" in the field. Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research. Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms.: 4  Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the "whiteness" of the culture of AI.

Interdisciplinarity and Collaboration
Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies, a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact. This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford University's Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition.
Collaboration with outside experts and various stakeholders facilitates ethical, inclusive, and accountable development of intelligent systems. It incorporates ethical considerations, understands the social and cultural context, promotes human-centered design, leverages technical expertise, and addresses policy and legal considerations. Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair, transparent, and accountable.

Regulation
Europe
The General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses "Automated individual decision-making, including profiling" in Article 22. These rules prohibit "solely" automated decisions which have a "significant" or "legal" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.

The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.

United States
The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to "design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases". Intended only as guidance, the report did not create any legal precedent.: 26 
In 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required "the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems." The task force is required to present findings and recommendations for further regulatory action in 2019.

India
On July 31, 2018, a draft of the Personal Data Bill was presented. The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for "harm resulting from any processing or any kind of processing undertaken by the fiduciary". It defines "any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal" or "any discriminatory treatment" as a source of harm that could arise from improper use of data. It also makes special provisions for people of "Intersex status".

See also
Algorithmic wage discrimination
Ethics of artificial intelligence
Fairness (machine learning)
Hallucination (artificial intelligence)
Misaligned goals in artificial intelligence
Predictive policing
SenseTime

References
Further reading
Baer, Tobias (2019). Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists. New York: Apress. ISBN 9781484248843.
Noble, Safiya Umoja (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. New York: New York University Press. ISBN 9781479837243.
In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.
For maximum efficiency it is desirable to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.
For example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort organizes the list in time proportional to the number of elements squared (
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\textstyle O(n^{2})}
  
, see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (
  
    
      
        O
        (
        1
        )
      
    
    {\textstyle O(1)}
  
). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (
  
    
      
        O
        (
        n
        log
        ⁡
        n
        )
      
    
    {\textstyle O(n\log n)}
  
), but has a space requirement linear in the length of the list (
  
    
      
        O
        (
        n
        )
      
    
    {\textstyle O(n)}
  
). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.

Background
The importance of efficiency with respect to time was emphasized by Ada Lovelace in 1843 as applied to Charles Babbage's mechanical analytical engine:

"In almost every computation a great variety of arrangements for the succession of the processes is possible, and various considerations must influence the selections amongst them for the purposes of a calculating engine. One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation"
Early electronic computers had both limited speed and limited random access memory. Therefore, a space–time trade-off occurred. A task could use a fast algorithm using a lot of memory, or it could use a slow algorithm using little memory. The engineering trade-off was therefore to use the fastest algorithm that could fit in the available memory.
Modern computers are significantly faster than early computers and have a much larger amount of memory available (gigabytes instead of kilobytes). Nevertheless, Donald Knuth emphasized that efficiency is still an important consideration:

  "In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering"

Overview
An algorithm is considered efficient if its resource consumption, also known as computational cost, is at or below some acceptable level. Roughly speaking, 'acceptable' means:  it will run in a reasonable amount of time or space on an available computer, typically as a function of the size of the input. Since the 1950s computers have seen dramatic increases in both the available computational power and in the available amount of memory, so current acceptable levels would have been unacceptable even 10 years ago. In fact, thanks to the approximate doubling of computer power every 2 years, tasks that are acceptably efficient on modern smartphones and embedded systems may have been unacceptably inefficient for industrial servers 10 years ago.
Computer manufacturers frequently bring out new models, often with higher performance. Software costs can be quite high, so in some cases the simplest and cheapest way of getting higher performance might be to just buy a faster computer, provided it is compatible with an existing computer.
There are many ways in which the resources used by an algorithm can be measured: the two most common measures are speed and memory usage; other measures could include transmission speed, temporary disk usage, long-term disk usage, power consumption, total cost of ownership, response time to external stimuli, etc. Many of these measures depend on the size of the input to the algorithm, i.e. the amount of data to be processed. They might also depend on the way in which the data is arranged; for example, some sorting algorithms perform poorly on data which is already sorted, or which is sorted in reverse order.
In practice, there are other factors which can affect the efficiency of an algorithm, such as requirements for accuracy and/or reliability. As detailed below, the way in which an algorithm is implemented can also have a significant effect on actual efficiency, though many aspects of this relate to optimization issues.

Theoretical analysis
In the theoretical analysis of algorithms, the normal practice is to estimate their complexity in the asymptotic sense. The most commonly used notation to describe resource consumption or "complexity" is Donald Knuth's Big O notation, representing the complexity of an algorithm as a function of the size of the input 
  
    
      
        n
      
    
    {\textstyle n}
  
. Big O notation is an asymptotic measure of function complexity, where 
  
    
      
        f
        (
        n
        )
        =
        O
        
          
            (
          
        
        g
        (
        n
        )
        
          
            )
          
        
      
    
    {\textstyle f(n)=O{\bigl (}g(n){\bigr )}}
  
 roughly means the time requirement for an algorithm is proportional to 
  
    
      
        g
        (
        n
        )
      
    
    {\displaystyle g(n)}
  
, omitting lower-order terms that contribute less than 
  
    
      
        g
        (
        n
        )
      
    
    {\displaystyle g(n)}
  
 to the growth of the function as 
  
    
      
        n
      
    
    {\textstyle n}
  
 grows arbitrarily large. This estimate may be misleading when 
  
    
      
        n
      
    
    {\textstyle n}
  
 is small, but is generally sufficiently accurate when 
  
    
      
        n
      
    
    {\textstyle n}
  
 is large as the notation is asymptotic. For example, bubble sort may be faster than merge sort when only a few items are to be sorted; however either implementation is likely to meet performance requirements for a small list. Typically, programmers are interested in algorithms that scale efficiently to large input sizes, and merge sort is preferred over bubble sort for lists of length encountered in most data-intensive programs.
Some examples of Big O notation applied to algorithms' asymptotic time complexity include:

Measuring performance
For new versions of software or to provide comparisons with competitive systems, benchmarks are sometimes used, which assist with gauging an algorithms relative performance. If a new sort algorithm is produced, for example, it can be compared with its predecessors to ensure that at least it is efficient as before with known data, taking into consideration any functional improvements. Benchmarks can be used by customers when comparing various products from alternative suppliers to estimate which product will best suit their specific requirements in terms of functionality and performance. For example, in the mainframe world certain proprietary sort products from independent software companies such as Syncsort compete with products from the major suppliers such as IBM for speed.
Some benchmarks provide opportunities for producing an analysis comparing the relative speed of various compiled and interpreted languages for example
and The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages.
Even creating "do it yourself" benchmarks can demonstrate the relative performance of different programming languages, using a variety of user specified criteria. This is quite simple, as a "Nine language performance roundup" by Christopher W. Cowell-Shah demonstrates by example.

Implementation concerns
Implementation issues can also have an effect on efficiency, such as the choice of programming language, or the way in which the algorithm is actually coded, or the choice of a compiler for a particular language, or the compilation options used, or even the operating system being used. In many cases a language implemented by an interpreter may be much slower than a language implemented by a compiler. See the articles on just-in-time compilation and interpreted languages.
There are other factors which may affect time or space issues, but which may be outside of a programmer's control; these include data alignment, data granularity, cache locality, cache coherency, garbage collection, instruction-level parallelism, multi-threading (at either a hardware or software level), simultaneous multitasking, and subroutine calls.
Some processors have capabilities for vector processing, which allow a single instruction to operate on multiple operands; it may or may not be easy for a programmer or compiler to use these capabilities. Algorithms designed for sequential processing may need to be completely redesigned to make use of parallel processing, or they could be easily reconfigured. As parallel and distributed computing grow in importance in the late 2010s, more investments are being made into efficient high-level APIs for parallel and distributed computing systems such as CUDA, TensorFlow, Hadoop, OpenMP and MPI.
Another problem which can arise in programming is that processors compatible with the same instruction set (such as x86-64 or ARM) may implement an instruction in different ways, so that instructions which are relatively fast on some models may be relatively slow on other models. This often presents challenges to optimizing compilers, which must have extensive knowledge of the specific CPU and other hardware available on the compilation target to best optimize a program for performance. In the extreme case, a compiler may be forced to emulate instructions not supported on a compilation target platform, forcing it to generate code or link an external library call to produce a result that is otherwise incomputable on that platform, even if it is natively supported and more efficient in hardware on other platforms. This is often the case in embedded systems with respect to floating-point arithmetic, where small and low-power microcontrollers often lack hardware support for floating-point arithmetic and thus require computationally expensive software routines to produce floating point calculations.

Measures of resource usage
Measures are normally expressed as a function of the size of the input 
  
    
      
        
          
            n
          
        
      
    
    {\displaystyle \scriptstyle {n}}
  
.
The two most common measures are:

Time: how long does the algorithm take to complete?
Space: how much working memory (typically RAM) is needed by the algorithm? This has two aspects: the amount of memory needed by the code (auxiliary space usage), and the amount of memory needed for the data on which the code operates (intrinsic space usage).
For computers whose power is supplied by a battery (e.g. laptops and smartphones), or for very long/large calculations (e.g. supercomputers), other measures of interest are:

Direct power consumption: power needed directly to operate the computer.
Indirect power consumption: power needed for cooling, lighting, etc.
As of 2018, power consumption is growing as an important metric for computational tasks of all types and at all scales ranging from embedded Internet of things devices to system-on-chip devices to server farms. This trend is often referred to as green computing.
Less common measures of computational efficiency may also be relevant in some cases:

Transmission size: bandwidth could be a limiting factor. Data compression can be used to reduce the amount of data to be transmitted. Displaying a picture or image (e.g. Google logo) can result in transmitting tens of thousands of bytes (48K in this case) compared with transmitting six bytes for the text "Google". This is important for I/O bound computing tasks.
External space: space needed on a disk or other external memory device; this could be for temporary storage while the algorithm is being carried out, or it could be long-term storage needed to be carried forward for future reference.
Response time (latency): this is particularly relevant in a real-time application when the computer system must respond quickly to some external event.
Total cost of ownership: particularly if a computer is dedicated to one particular algorithm.

Time
Theory
Analysis of algorithms, typically using concepts like time complexity, can be used to get an estimate of the running time as a function of the size of the input data. The result is normally expressed using Big O notation. This is useful for comparing algorithms, especially when a large amount of data is to be processed. More detailed estimates are needed to compare algorithm performance when the amount of data is small, although this is likely to be of less importance. Parallel algorithms may be more difficult to analyze.

Practice
A benchmark can be used to assess the performance of an algorithm in practice. Many programming languages have an available function which provides CPU time usage. For long-running algorithms the elapsed time could also be of interest. Results should generally be averaged over several tests.
Run-based profiling can be very sensitive to hardware configuration and the possibility of other programs or tasks running at the same time in a multi-processing and multi-programming environment.
This sort of test also depends heavily on the selection of a particular programming language, compiler, and compiler options, so algorithms being compared must all be implemented under the same conditions.

Space
This section is concerned with use of memory resources (registers, cache, RAM, virtual memory, secondary memory) while the algorithm is being executed. As for time analysis above, analyze the algorithm, typically using space complexity analysis to get an estimate of the run-time memory needed as a function as the size of the input data. The result is normally expressed using Big O notation.
There are up to four aspects of memory usage to consider:

The amount of memory needed to hold the code for the algorithm.
The amount of memory needed for the input data.
The amount of memory needed for any output data.
Some algorithms, such as sorting, often rearrange the input data and do not need any additional space for output data. This property is referred to as "in-place" operation.
The amount of memory needed as working space during the calculation.
This includes local variables and any stack space needed by routines called during a calculation; this stack space can be significant for algorithms which use recursive techniques.
Early electronic computers, and early home computers, had relatively small amounts of working memory. For example, the 1949 Electronic Delay Storage Automatic Calculator (EDSAC) had a maximum working memory of 1024 17-bit words, while the 1980 Sinclair ZX80 came initially with 1024 8-bit bytes of working memory. In the late 2010s, it is typical for personal computers to have between 4 and 32 GB of RAM, an increase of over 300 million times as much memory.

Caching and memory hierarchy
Modern computers can have relatively large amounts of memory (possibly gigabytes), so having to squeeze an algorithm into a confined amount of memory is not the kind of problem it used to be. However, the different types of memory and their relative access speeds can be significant:

Processor registers, are the fastest memory with the least amount of space. Most direct computation on modern computers occurs with source and destination operands in registers before being updated to the cache, main memory and virtual memory if needed. On a processor core, there are typically on the order of hundreds of bytes or fewer of register availability, although a register file may contain more physical registers than architectural registers defined in the instruction set architecture.
Cache memory is the second fastest, and second smallest, available in the memory hierarchy. Caches are present in processors such as CPUs or GPUs, where they are typically implemented in static RAM, though they can also be found in peripherals such as disk drives. Processor caches often have their own multi-level hierarchy; lower levels are larger, slower and typically shared between processor cores in multi-core processors. In order to process operands in cache memory, a processing unit must fetch the data from the cache, perform the operation in registers and write the data back to the cache. This operates at speeds comparable (about 2-10 times slower) with the CPU or GPU's arithmetic logic unit or floating-point unit if in the L1 cache. It is about 10 times slower if there is an L1 cache miss and it must be retrieved from and written to the L2 cache, and a further 10 times slower if there is an L2 cache miss and it must be retrieved from an L3 cache, if present.
Main physical memory is most often implemented in dynamic RAM (DRAM). The main memory is much larger (typically gigabytes compared to ≈8 megabytes) than an L3 CPU cache, with read and write latencies typically 10-100 times slower. As of 2018, RAM is increasingly implemented on-chip of processors, as CPU or GPU memory.
Paged memory, often used for virtual memory management, is memory stored in secondary storage such as a hard disk, and is an extension to the memory hierarchy which allows use of a potentially larger storage space, at the cost of much higher latency, typically around 1000 times slower than a cache miss for a value in RAM. While originally motivated to create the impression of higher amounts of memory being available than were truly available, virtual memory is more important in contemporary usage for its time-space tradeoff and enabling the usage of virtual machines. Cache misses from main memory are called page faults, and incur huge performance penalties on programs.
An algorithm whose memory needs will fit in cache memory will be much faster than an algorithm which fits in main memory, which in turn will be very much faster than an algorithm which has to resort to paging. Because of this, cache replacement policies are extremely important to high-performance computing, as are cache-aware programming and data alignment. To further complicate the issue, some systems have up to three levels of cache memory, with varying effective speeds. Different systems will have different amounts of these various types of memory, so the effect of algorithm memory needs can vary greatly from one system to another.
In the early days of electronic computing, if an algorithm and its data would not fit in main memory then the algorithm could not be used. Nowadays the use of virtual memory appears to provide much more memory, but at the cost of performance. Much higher speed can be obtained if an algorithm and its data fit in cache memory; in this case minimizing space will also help minimize time. This is called the principle of locality, and can be subdivided into locality of reference, spatial locality, and temporal locality. An algorithm which will not fit completely in cache memory but which exhibits locality of reference may perform reasonably well.

See also
Analysis of algorithms—how to determine the resources needed by an algorithm
Benchmark—a method for measuring comparative execution times in defined cases
Best, worst and average case—considerations for estimating execution times in three scenarios
Compiler optimization—compiler-derived optimization
Computational complexity theory
Computer performance—computer hardware metrics
Empirical algorithmics—the practice of using empirical methods to study the behavior of algorithms
Optimization (computer science)
Performance analysis—methods of measuring actual performance of an algorithm at run-time


== References ==
Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.
The phrases "algorithmic transparency" and "algorithmic accountability" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, "algorithmic transparency" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  "Algorithmic accountability" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.
Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a "right to explanation" of decisions made by algorithms, though it is unclear what this means. Furthermore, the European Union founded The European Center for Algoritmic Transparency (ECAT).

See also
Black box
Explainable AI
Regulation of algorithms
Reverse engineering
Right to explanation
Algorithmic accountability


== References ==
AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure. The program is designed as a deep learning system.
AlphaFold software has had three major versions. A team of researchers that used AlphaFold 1 (2018) placed first in the overall rankings of the 13th Critical Assessment of Structure Prediction (CASP) in December 2018. The program was particularly successful at predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. A team that used AlphaFold 2 (2020) repeated the placement in the CASP14 competition in November 2020. The team achieved a level of accuracy much higher than any other group. It scored above 90 for around two-thirds of the proteins in CASP's global distance test (GDT), a test that measures the degree to which a computational program predicted structure is similar to the lab experiment determined structure, with 100 being a complete match, within the distance cutoff used for calculating GDT.
AlphaFold 2's results at CASP14 were described as "astounding" and "transformational". Some researchers noted that the accuracy is not high enough for a third of its predictions, and that it does not reveal the mechanism or rules of protein folding for the protein folding problem to be considered solved. Nevertheless, there has been widespread respect for the technical achievement. On 15 July 2021 the AlphaFold 2 paper was published in Nature as an advance access publication alongside open source software and a searchable database of species proteomes.
AlphaFold 3 was announced on 8 May 2024. It can predict the structure of complexes created by proteins with DNA, RNA, various ligands, and ions.

Background
Proteins consist of chains of amino acids which spontaneously fold to form the three dimensional (3-D) structures of the proteins. The 3-D structure is crucial to understanding the biological function of the protein.
Protein structures can be determined experimentally through techniques such as X-ray crystallography, cryo-electron microscopy and nuclear magnetic resonance, which are all expensive and time-consuming. Such efforts, using the experimental methods, have identified the structures of about 170,000 proteins over the last 60 years, while there are over 200 million known proteins across all life forms.
Over the years, researchers have applied numerous computational methods to predict the 3D structures of proteins from their amino acid sequences, but the accuracy of such methods has not been close to experimental techniques. CASP, which was launched in 1994 to challenge the scientific community to produce their best protein structure predictions, found that GDT scores of only about 40 out of 100 can be achieved for the most difficult proteins by 2016. AlphaFold started competing in the 2018 CASP using an artificial intelligence (AI) deep learning technique.

Algorithm
DeepMind is known to have trained the program on over 170,000 proteins from a public repository of protein sequences and structures. The program uses a form of attention network, a deep learning technique that focuses on having the AI identify parts of a larger problem, then piece it together to obtain the overall solution. The overall training was conducted on processing power between 100 and 200 GPUs.

AlphaFold 1 (2018)
AlphaFold 1 (2018) was built on work developed by various teams in the 2010s, work that looked at the large databanks of related DNA sequences now available from many different organisms (most without known 3D structures), to try to find changes at different residues that appeared to be correlated, even though the residues were not consecutive in the main chain.  Such correlations suggest that the residues may be close to each other physically, even though not close in the sequence, allowing a contact map to be estimated.  Building on recent work prior to 2018, AlphaFold 1 extended this to estimate a probability distribution for just how close the residues might be likely to be—turning the contact map into a likely distance map.  It also used more advanced learning methods than previously to develop the inference.

AlphaFold 2 (2020)
The 2020 version of the program (AlphaFold 2, 2020) is significantly different from the original version that won CASP 13 in 2018, according to the team at DeepMind.
The software design used in AlphaFold 1 contained a number of modules, each trained separately, that were used to produce the guide potential that was then combined with the physics-based energy potential.  AlphaFold 2 replaced this with a system of sub-networks coupled together into a single differentiable end-to-end model, based entirely on pattern recognition, which was trained in an integrated way as a single integrated structure. Local physics, in the form of energy refinement based on the AMBER model, is applied only as a final refinement step once the neural network prediction has converged, and only slightly adjusts the predicted structure.
A key part of the 2020 system are two modules, believed to be based on a transformer design, which are used to progressively refine a vector of information for each relationship (or "edge" in graph-theory terminology) between an amino acid residue of the protein and another amino acid residue (these relationships are represented by the array shown in green); and between each amino acid position and each different sequences in the input sequence alignment (these relationships are represented by the array shown in red).  Internally these refinement transformations contain layers that have the effect of bringing relevant data together and filtering out irrelevant data (the "attention mechanism") for these relationships, in a context-dependent way, learnt from training data.  These transformations are iterated, the updated information output by one step becoming the input of the next, with the sharpened residue/residue information feeding into the update of the residue/sequence information, and then the improved residue/sequence information feeding into the update of the residue/residue information.  As the iteration progresses, according to one report, the "attention algorithm ... mimics the way a person might assemble a jigsaw puzzle: first connecting pieces in small clumps—in this case clusters of amino acids—and then searching for ways to join the clumps in a larger whole."
The output of these iterations then informs the final structure prediction module, which also uses transformers, and is itself then iterated.  In an example presented by DeepMind, the structure prediction module achieved a correct topology for the target protein on its first iteration, scored as having a GDT_TS of 78, but with a large number (90%) of stereochemical violations – i.e. unphysical bond angles or lengths.  With subsequent iterations the number of stereochemical violations fell.  By the third iteration the GDT_TS of the prediction was approaching 90, and by the eighth iteration the number of stereochemical violations was approaching zero.
The training data was originally restricted to single peptide chains. However, the October 2021 update, named AlphaFold-Multimer, included protein complexes in its training data. DeepMind stated this update succeeded about 70% of the time at accurately predicting protein-protein interactions.

AlphaFold 3 (2024)
Announced on 8 May 2024, AlphaFold 3 was co-developed by Google DeepMind and Isomorphic Labs, both subsidiaries of Alphabet. AlphaFold 3 is not limited to single-chain proteins, as it can also predict the structures of protein complexes with DNA, RNA, post-translational modifications and selected ligands and ions.
AlphaFold 3 introduces the "Pairformer", a deep learning architecture inspired from the transformer, considered similar but simpler than the Evoformer introduced with AlphaFold 2. The raw predictions from the Pairformer module are passed to a diffusion model, which starts with a cloud of atoms and uses these predictions to iteratively progress towards a 3D depiction of the molecular structure.
The AlphaFold server was created to provide free access to AlphaFold 3 for non-commercial research.

Competitions
CASP13
In December 2018, DeepMind's AlphaFold placed first in the overall rankings of the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP).
The program was particularly successfully predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. AlphaFold gave the best prediction for 25 out of 43 protein targets in this class, achieving a median score of 58.9 on the CASP's global distance test (GDT) score, ahead of 52.5 and 52.4 by the two next best-placed teams, who were also using deep learning to estimate contact distances. Overall, across all targets, the program achieved a GDT score of 68.5.
In January 2020, implementations and illustrative code of AlphaFold 1 was released open-source on GitHub. but, as stated in the "Read Me" file on that website: "This code can't be used to predict structure of an arbitrary protein sequence. It can be used to predict structure only on the CASP13 dataset (links below). The feature generation code is tightly coupled to our internal infrastructure as well as external tools, hence we are unable to open-source it." Therefore, in essence, the code deposited is not suitable for general use but only for the CASP13 proteins. The company has not announced plans to make their code publicly available as of 5 March 2021.

CASP14
In November 2020, DeepMind's new version, AlphaFold 2, won CASP14. Overall, AlphaFold 2 made the best prediction for 88 out of the 97 targets.
On the competition's preferred global distance test (GDT) measure of accuracy, the program achieved a median score of 92.4 (out of 100), meaning that more than half of its predictions were scored at better than 92.4% for having their atoms in more-or-less the right place, a level of accuracy reported to be comparable to experimental techniques like X-ray crystallography. In 2018 AlphaFold 1 had only reached this level of accuracy in two of all of its predictions. 88% of predictions in the 2020 competition had a GDT_TS score of more than 80. On the group of targets classed as the most difficult, AlphaFold 2 achieved a median score of 87.
Measured by the root-mean-square deviation (RMS-D) of the placement of the alpha-carbon atoms of the protein backbone chain, which tends to be dominated by the performance of the worst-fitted outliers, 88% of AlphaFold 2's predictions had an RMS deviation of less than 4 Å for the set of overlapped C-alpha atoms.  76% of predictions achieved better than 3 Å, and 46% had a C-alpha atom RMS accuracy better than 2 Å, with a median RMS deviation in its predictions of 2.1 Å for a set of overlapped CA atoms.  AlphaFold 2 also achieved an accuracy in modelling surface side chains described as "really really extraordinary".
To additionally verify AlphaFold-2 the conference organisers approached four leading experimental groups for structures they were finding particularly challenging and had been unable to determine. In all four cases the three-dimensional models produced by AlphaFold 2 were sufficiently accurate to determine structures of these proteins by molecular replacement.  These included target T1100 (Af1503), a small membrane protein studied by experimentalists for ten years.
Of the three structures that AlphaFold 2 had the least success in predicting, two had been obtained by protein NMR methods, which define protein structure directly in aqueous solution, whereas AlphaFold was mostly trained on protein structures in crystals.  The third exists in nature as a multidomain complex consisting of 52 identical copies of the same domain, a situation AlphaFold was not programmed to consider.  For all targets with a single domain, excluding only one very large protein and the two structures determined by NMR, AlphaFold 2 achieved a GDT_TS score of over 80.

CASP15
In 2022 DeepMind did not enter CASP15, but most of the entrants used AlphaFold or tools incorporating AlphaFold.

Reception
AlphaFold 2 scoring more than 90 in CASP's global distance test (GDT) is considered a significant achievement in computational biology and great progress towards a decades-old grand challenge of biology. Nobel Prize winner and structural biologist Venki Ramakrishnan called the result "a stunning advance on the protein folding problem", adding that "It has occurred decades before many people in the field would have predicted. It will be exciting to see the many ways in which it will fundamentally change biological research."
Propelled by press releases from CASP and DeepMind, AlphaFold 2's success received wide media attention.  As well as news pieces in the specialist science press, such as Nature, Science, MIT Technology Review, and New Scientist, the story was widely covered by major national newspapers,. A frequent theme was that ability to predict protein structures accurately based on the constituent amino acid sequence is expected to have a wide variety of benefits in the life sciences space including accelerating advanced drug discovery and enabling better understanding of diseases. Some have noted that even a perfect answer to the protein prediction problem would still leave questions about the protein folding problem—understanding in detail how the folding process actually occurs in nature (and how sometimes they can also misfold).
In 2023, Demis Hassabis and John Jumper won the Breakthrough Prize in Life Sciences as well as the Albert Lasker Award for Basic Medical Research for their management of the AlphaFold project.

Source code
The open access to source code of several AlphaFold versions (excluding AlphaFold 3) has been provided by DeepMind after requests from the scientific community. Full source code of AlphaFold-3 is expected to be provided to open access by the end of 2024.

Database of protein models generated by AlphaFold
The AlphaFold Protein Structure Database was launched on July 22, 2021, as a joint effort between AlphaFold and EMBL-EBI. At launch the database contains AlphaFold-predicted models of protein structures of nearly the full UniProt proteome of humans and 20 model organisms, amounting to over 365,000 proteins. The database does not include proteins with fewer than 16 or more than 2700 amino acid residues, but for humans they are available in the whole batch file. AlphaFold planned to add more sequences to the collection, the initial goal (as of beginning of 2022) being to cover most of the UniRef90 set of more than 100 million proteins. As of May 15, 2022, 992,316 predictions were available.
In July 2021, UniProt-KB and InterPro has been updated to show AlphaFold predictions when available.
On July 28, 2022, the team uploaded to the database the structures of around 200 million proteins from 1 million species, covering nearly every known protein on the planet.

Limitations
AlphaFold has various limitations:

AlphaFold DB provides monomeric models of proteins, rather than their biologically relevant complexes.
Many protein regions are predicted with low confidence score, including the intrinsically disordered protein regions.
Aphafold-2 was validated for predicting structural effects of mutations with a limited success.
The model relies to some degree upon co-evolutionary information across similar proteins, and thus may not perform well on synthetic proteins or proteins with very low homology to anything in the database.
The ability of the model to produce multiple native conformations of proteins is limited.
AlphaFold 3 version can predict structures of protein complexes with a very limited set of selected cofactors and co- and post-translational modifications. Between 50% and 70% of the structures of the human proteome are incomplete without covalently-attached glycans. AlphaFill, a derived database, adds cofactors to AlphaFold models where appropriate.
In the algorithm, the residues are moved freely, without any restraints. Therefore, during modeling the integrity of the chain is not maintained. As a result, AlphaFold may produce topologically wrong results, like structures with an arbitrary number of knots.

Applications
AlphaFold has been used to predict structures of proteins of SARS-CoV-2, the causative agent of COVID-19. The structures of these proteins were pending experimental detection in early 2020. Results were examined by the scientists at the Francis Crick Institute in the United Kingdom before release into the larger research community. The team also confirmed accurate prediction against the experimentally determined SARS-CoV-2 spike protein that was shared in the Protein Data Bank, an international open-access database, before releasing the computationally determined structures of the under-studied protein molecules. The team acknowledged that although these protein structures might not be the subject of ongoing therapeutical research efforts, they will add to the community's understanding of the SARS-CoV-2 virus. Specifically, AlphaFold 2's prediction of the structure of the ORF3a protein was very similar to the structure determined by researchers at University of California, Berkeley using cryo-electron microscopy. This specific protein is believed to assist the virus in breaking out of the host cell once it replicates. This protein is also believed to play a role in triggering the inflammatory response to the infection.

Published works
Andrew W. Senior et al. (December 2019), "Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of Protein Structure Prediction (CASP13)", Proteins: Structure, Function, Bioinformatics 87(12) 1141–1148 doi:10.1002/prot.25834
Andrew W. Senior et al. (15 January 2020), "Improved protein structure prediction using potentials from deep learning", Nature 577 706–710 doi:10.1038/s41586-019-1923-7
John Jumper et al. (December 2020), "High Accuracy Protein Structure Prediction Using Deep Learning", in Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), pp. 22–24
John Jumper et al. (December 2020), "AlphaFold 2". Presentation given at CASP 14.

See also
References
Further reading
Carlos Outeiral, CASP14: what Google DeepMind's AlphaFold 2 really achieved, and what it means for protein folding, biology and bioinformatics, Oxford Protein Informatics Group. (3 December)
Mohammed AlQuraishi, AlphaFold2 @ CASP14: "It feels like one's child has left home." (blog), 8 December 2020
Mohammed AlQuraishi, The AlphaFold2 Method Paper: A Fount of Good Ideas (blog), 25 July 2021

External links
AlphaFold-3 web server
AlphaFold v2.1 code and links to model on GitHub
Open access to protein structure predictions for the human proteome and 20 other key organisms at European Bioinformatics Institute (AlphaFold Protein Structure Database)
CASP 14 website
AlphaFold: The making of a scientific breakthrough, DeepMind, via YouTube.
ColabFold (Mirdita, Milot; Schütze, Konstantin; Moriwaki, Yoshitaka; Heo, Lim; Ovchinnikov, Sergey; Steinegger, Martin (2022-05-30). "ColabFold: Making protein folding accessible to all". Nature Methods. 19 (6): 679–682. doi:10.1038/s41592-022-01488-1. PMC 9184281. PMID 35637307.), version for homooligomeric prediction and complexes
AlphaGo is a computer program that plays the board game Go. It was developed by the London-based DeepMind Technologies, an acquired subsidiary of Google. Subsequent versions of AlphaGo became increasingly powerful, including a version that competed under the name Master. After retiring from competitive play, AlphaGo Master was succeeded by an even more powerful version known as AlphaGo Zero, which was completely self-taught without learning from human games. AlphaGo Zero was then generalized into a program known as AlphaZero, which played additional games, including chess and shogi.  AlphaZero has in turn been succeeded by a program known as MuZero which learns without being taught the rules.
AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play. A neural network is trained to identify the best moves and the winning percentages of these moves. This neural network improves the strength of the tree search, resulting in stronger move selection in the next iteration.
In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19×19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. The win by AlphaGo was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.
At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.
After the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. The self-taught AlphaGo Zero achieved a 100–0 victory against the early competitive version of AlphaGo, and its successor AlphaZero was perceived as the world's top player in Go by the end of the 2010s.

History
Go is considered much more difficult for computers to win than other games such as chess, because its strategic and aesthetic nature makes it hard to directly construct an evaluation function, and its much larger branching factor makes it prohibitively difficult to use traditional AI methods such as alpha–beta pruning, tree traversal and heuristic search.
Almost two decades after IBM's computer Deep Blue beat world chess champion Garry Kasparov in the 1997 match, the strongest Go programs using artificial intelligence techniques only reached about amateur 5-dan level, and still could not beat a professional Go player without a handicap. In 2012, the software program Zen, running on a four PC cluster, beat Masaki Takemiya (9p) twice at five- and four-stone handicaps. In 2013, Crazy Stone beat Yoshio Ishida (9p) at a four-stone handicap.
According to DeepMind's David Silver, the AlphaGo research project was formed around 2014 to test how well a neural network using deep learning can compete at Go. AlphaGo represents a significant improvement over previous Go programs. In 500 games against other available Go programs, including Crazy Stone and Zen, AlphaGo running on a single computer won all but one. In a similar matchup, AlphaGo running on multiple computers won all 500 games played against other Go programs, and 77% of games played against AlphaGo running on a single computer. The distributed version in October 2015 was using 1,202 CPUs and 176 GPUs.

Match against Fan Hui
In October 2015, the distributed version of AlphaGo defeated the European Go champion Fan Hui, a 2-dan (out of 9 dan possible) professional, five to zero. This was the first time a computer Go program had beaten a professional human player on a full-sized board without handicap. The announcement of the news was delayed until 27 January 2016 to coincide with the publication of a paper in the journal Nature describing the algorithms used.

Match against Lee Sedol
AlphaGo played South Korean professional Go player Lee Sedol, ranked 9-dan, one of the best players at Go, with five games taking place at the Four Seasons Hotel in Seoul, South Korea on 9, 10, 12, 13, and 15 March 2016, which were video-streamed live.  Out of five games, AlphaGo won four games and Lee won the fourth game which made him recorded as the only human player who beat AlphaGo in all of its 74 official games. AlphaGo ran on Google's cloud computing with its servers located in the United States. The match used Chinese rules with a 7.5-point komi, and each side had two hours of thinking time plus three 60-second byoyomi periods. The version of AlphaGo playing against Lee used a similar amount of computing power as was used in the Fan Hui match. The Economist reported that it used 1,920 CPUs and 280 GPUs. At the time of play, Lee Sedol had the second-highest number of Go international championship victories in the world after South Korean player Lee Changho who kept the world championship title for 16 years. Since there is no single official method of ranking in international Go, the rankings may vary among the sources. While he was ranked top sometimes, some sources ranked Lee Sedol as the fourth-best player in the world at the time. AlphaGo was not specifically trained to face Lee nor was designed to compete with any specific human players.
The first three games were won by AlphaGo following resignations by Lee. However, Lee beat AlphaGo in the fourth game, winning by resignation at move 180. AlphaGo then continued to achieve a fourth win, winning the fifth game by resignation.
The prize was US$1 million. Since AlphaGo won four out of five and thus the series, the prize will be donated to charities, including UNICEF. Lee Sedol received $150,000 for participating in all five games and an additional $20,000 for his win in Game 4.
In June 2016, at a presentation held at a university in the Netherlands, Aja Huang, one of the Deep Mind team, revealed that they had patched the logical weakness that occurred during the 4th game of the match between AlphaGo and Lee, and that after move 78 (which was dubbed the "divine move" by many professionals), it would play as intended and maintain Black's advantage. Before move 78, AlphaGo was leading throughout the game, but Lee's move caused the program's computing powers to be diverted and confused. Huang explained that AlphaGo's policy network of finding the most accurate move order and continuation did not precisely guide AlphaGo to make the correct continuation after move 78, since its value network did not determine Lee's 78th move as being the most likely, and therefore when the move was made AlphaGo could not make the right adjustment to the logical continuation.

Sixty online games
On 29 December 2016, a new account on the Tygem server named "Magister" (shown as 'Magist' at the server's Chinese version) from South Korea began to play games with professional players. It changed its account name to "Master" on 30 December, then moved to the FoxGo server on 1 January 2017. On 4 January, DeepMind confirmed that the "Magister" and the "Master" were both played by an updated version of AlphaGo, called AlphaGo Master. As of 5 January 2017, AlphaGo Master's online record was 60 wins and 0 losses, including three victories over Go's top-ranked player, Ke Jie, who had been quietly briefed in advance that Master was a version of AlphaGo. After losing to Master, Gu Li offered a bounty of 100,000 yuan (US$14,400) to the first human player who could defeat Master. Master played at the pace of 10 games per day. Many quickly suspected it to be an AI player due to little or no resting between games. Its adversaries included many world champions such as Ke Jie, Park Jeong-hwan, Yuta Iyama, Tuo Jiaxi, Mi Yuting, Shi Yue, Chen Yaoye, Li Qincheng, Gu Li, Chang Hao, Tang Weixing, Fan Tingyu, Zhou Ruiyang, Jiang Weijie, Chou Chun-hsun, Kim Ji-seok, Kang Dong-yun, Park Yeong-hun, and Won Seong-jin; national champions or world championship runners-up such as Lian Xiao, Tan Xiao, Meng Tailing, Dang Yifei, Huang Yunsong, Yang Dingxin, Gu Zihao, Shin Jinseo, Cho Han-seung, and An Sungjoon. All 60 games except one were fast-paced games with three 20 or 30 seconds byo-yomi. Master offered to extend the byo-yomi to one minute when playing with Nie Weiping in consideration of his age. After winning its 59th game Master revealed itself in the chatroom to be controlled by Dr. Aja Huang of the DeepMind team, then changed its nationality to the United Kingdom. After these games were completed, the co-founder of DeepMind, Demis Hassabis, said in a tweet, "we're looking forward to playing some official, full-length games later [2017] in collaboration with Go organizations and experts".
Go experts were impressed by the program's performance and its nonhuman play style; Ke Jie stated that "After humanity spent thousands of years improving our tactics, computers tell us that humans are completely wrong... I would go as far as to say not a single human has touched the edge of the truth of Go."

Future of Go Summit
In the Future of Go Summit held in Wuzhen in May 2017, AlphaGo Master played three games with Ke Jie, the world No.1 ranked player, as well as two games with several top Chinese professionals, one pair Go game and one against a collaborating team of five human players.
Google DeepMind offered 1.5 million dollar winner prizes for the three-game match between Ke Jie and Master while the losing side took 300,000 dollars. Master won all three games against Ke Jie, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.
After winning its three-game match against Ke Jie, the top-rated world Go player, AlphaGo retired. DeepMind also disbanded the team that worked on the game to focus on AI research in other areas. After the Summit, Deepmind published 50 full length AlphaGo vs AlphaGo matches, as a gift to the Go community.

AlphaGo Zero and AlphaZero
AlphaGo's team published an article in the journal Nature on 19 October 2017, introducing AlphaGo Zero, a version without human data and stronger than any previous human-champion-defeating version. By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.
In a paper released on arXiv on 5 December 2017, DeepMind claimed that it generalized AlphaGo Zero's approach into a single AlphaZero algorithm, which achieved within 24 hours a superhuman level of play in the games of chess, shogi, and Go by defeating world-champion programs, Stockfish, Elmo, and 3-day version of AlphaGo Zero in each case.

Teaching tool
On 11 December 2017, DeepMind released an AlphaGo teaching tool on its website to analyze winning rates of different Go openings as calculated by AlphaGo Master. The teaching tool collects 6,000 Go openings from 230,000 human games each analyzed with 10,000,000 simulations by AlphaGo Master. Many of the openings include human move suggestions.

Versions
An early version of AlphaGo was tested on hardware with various numbers of CPUs and GPUs, running in asynchronous or distributed mode. Two seconds of thinking time was given to each move. The resulting Elo ratings are listed below. In the matches with more time per move higher ratings are achieved.

In May 2016, Google unveiled its own proprietary hardware "tensor processing units", which it stated had already been deployed in multiple internal projects at Google, including the AlphaGo match against Lee Sedol.
In the Future of Go Summit in May 2017, DeepMind disclosed that the version of AlphaGo used in this Summit was AlphaGo Master, and revealed that it had measured the strength of different versions of the software. AlphaGo Lee, the version used against Lee, could give AlphaGo Fan, the version used in AlphaGo vs. Fan Hui, three stones, and AlphaGo Master was even three stones stronger.

Algorithm
As of 2016, AlphaGo's algorithm uses a combination of machine learning and tree search techniques, combined with extensive training, both from human and computer play. It uses Monte Carlo tree search, guided by a "value network" and a "policy network", both implemented using deep neural network technology. A limited amount of game-specific feature detection pre-processing (for example, to highlight whether a move matches a nakade pattern) is applied to the input before it is sent to the neural networks. The networks are convolutional neural networks with 12 layers, trained by reinforcement learning.
The system's neural networks were initially bootstrapped from human gameplay expertise. AlphaGo was initially trained to mimic human play by attempting to match the moves of expert players from recorded historical games, using a database of around 30 million moves. Once it had reached a certain degree of proficiency, it was trained further by being set to play large numbers of games against other instances of itself, using reinforcement learning to improve its play. To avoid "disrespectfully" wasting its opponent's time, the program is specifically programmed to resign if its assessment of win probability falls beneath a certain threshold; for the match against Lee, the resignation threshold was set to 20%.

Style of play
Toby Manning, the match referee for AlphaGo vs. Fan Hui, has described the program's style as "conservative". AlphaGo's playing style strongly favours greater probability of winning by fewer points over lesser probability of winning by more points. Its strategy of maximising its probability of winning is distinct from what human players tend to do which is to maximise territorial gains, and explains some of its odd-looking moves. It makes a lot of opening moves that have never or seldom been made by humans. It likes to use shoulder hits, especially if the opponent is over concentrated.

Responses to 2016 victory
AI community
AlphaGo's March 2016 victory was a major milestone in artificial intelligence research. Go had previously been regarded as a hard problem in machine learning that was expected to be out of reach for the technology of the time. Most experts thought a Go program as powerful as AlphaGo was at least five years away; some experts thought that it would take at least another decade before computers would beat Go champions. Most observers at the beginning of the 2016 matches expected Lee to beat AlphaGo.
With games such as checkers (that has been "solved" by the Chinook draughts player team), chess, and now Go won by computers, victories at popular board games can no longer serve as major milestones for artificial intelligence in the way that they used to. Deep Blue's Murray Campbell called AlphaGo's victory "the end of an era... board games are more or less done and it's time to move on."
When compared with Deep Blue or Watson, AlphaGo's underlying algorithms are potentially more general-purpose and may be evidence that the scientific community is making progress towards artificial general intelligence. Some commentators believe AlphaGo's victory makes for a good opportunity for society to start preparing for the possible future impact of machines with general purpose intelligence. As noted by entrepreneur Guy Suter, AlphaGo only knows how to play Go and doesn't possess general-purpose intelligence; "[It] couldn't just wake up one morning and decide it wants to learn how to use firearms." AI researcher Stuart Russell said that AI systems such as AlphaGo have progressed quicker and become more powerful than expected, and we must therefore develop methods to ensure they "remain under human control". Some scholars,  such as Stephen Hawking, warned (in May 2015 before the matches) that some future self-improving AI could gain actual general intelligence, leading to an unexpected AI takeover; other scholars disagree: AI expert Jean-Gabriel Ganascia believes that "Things like 'common sense'... may never be reproducible", and says "I don't see why we would speak about fears. On the contrary, this raises hopes in many domains such as health and space exploration." Computer scientist Richard Sutton said "I don't think people should be scared... but I do think people should be paying attention."
In China, AlphaGo was a "Sputnik moment" which helped convince the Chinese government to prioritize and dramatically increase funding for artificial intelligence.
In 2017, the DeepMind AlphaGo team received the inaugural IJCAI Marvin Minsky medal for Outstanding Achievements in AI. "AlphaGo is a wonderful achievement, and a perfect example of what the Minsky Medal was initiated to recognise", said Professor Michael Wooldridge, Chair of the IJCAI Awards Committee.  "What particularly impressed IJCAI was that AlphaGo achieves what it does through a brilliant combination of classic AI techniques as well as the state-of-the-art machine learning techniques that DeepMind is so closely associated with. It's a breathtaking demonstration of contemporary AI, and we are delighted to be able to recognise it with this award."

Go community
Go is a popular game in China, Japan and Korea, and the 2016 matches were watched by perhaps a hundred million people worldwide. Many top Go players characterized AlphaGo's unorthodox plays as seemingly-questionable moves that initially befuddled onlookers, but made sense in hindsight: "All but the very best Go players craft their style by imitating top players. AlphaGo seems to have totally original moves it creates itself." AlphaGo appeared to have unexpectedly become much stronger, even when compared with its October 2015 match where a computer had beaten a Go professional for the first time ever without the advantage of a handicap. The day after Lee's first defeat, Jeong Ahram, the lead Go correspondent for one of South Korea's biggest daily newspapers, said "Last night was very gloomy... Many people drank alcohol." The Korea Baduk Association, the organization that oversees Go professionals in South Korea, awarded AlphaGo an honorary 9-dan title for exhibiting creative skills and pushing forward the game's progress.
China's Ke Jie, an 18-year-old generally recognized as the world's best Go player at the time, initially claimed that he would be able to beat AlphaGo, but declined to play against it for fear that it would "copy my style". As the matches progressed, Ke Jie went back and forth, stating that "it is highly likely that I (could) lose" after analysing the first three matches, but regaining confidence after AlphaGo displayed flaws in the fourth match.
Toby Manning, the referee of AlphaGo's match against Fan Hui, and Hajin Lee, secretary general of the International Go Federation, both reason that in the future, Go players will get help from computers to learn what they have done wrong in games and improve their skills.
After game two, Lee said he felt "speechless": "From the very beginning of the match, I could never manage an upper hand for one single move. It was AlphaGo's total victory." Lee apologized for his losses, stating after game three that "I misjudged the capabilities of AlphaGo and felt powerless." He emphasized that the defeat was "Lee Se-dol's defeat" and "not a defeat of mankind". Lee said his eventual loss to a machine was "inevitable" but stated that "robots will never understand the beauty of the game the same way that we humans do." Lee called his game four victory a "priceless win that I (would) not exchange for anything."

AlphaGo documentary film (2016)
Reception
On Rotten Tomatoes the documentary has an average rating of 100% from 10 reviews. 
Michael Rechtshaffen of the Los Angeles Times gave the documentary a positive review and said: "It helps matters when you have a group of engaging human subjects like soft-spoken Sedol, who's as intensively contemplative as the game itself, contrasted by the spirited, personable Fan Hui, the Paris-based European champ who accepts an offer to serve as an advisor for the DeepMind team after suffering a demoralizing AI trouncing". He also mentioned that with the passion of Hauschka's Volker Bertelmann, the film's producer, this documentary shows many unexpected sequences, including strategic and philosophical components. (Rechtshaffen, 2017
John Defore of The Hollywood Reporter, wrote this documentary is "an involving sports-rivalry doc with an AI twist." "In the end, observers wonder if AlphaGo's odd variety of intuition might not kill Go as an intellectual pursuit but shift its course, forcing the game's scholars to consider it from new angles. So maybe it isn't time to welcome our computer overlords, and won't be for a while - maybe they'll teach us to be better thinkers before turning us into their slaves."
Greg Kohs, the director of the film, said "The complexity of the game of Go, combined with the technical depth of an emerging technology like artificial intelligence seemed like it might create an insurmountable barrier for a film like this. The fact that I was so innocently unaware of Go and AlphaGo actually proved to be beneficial. It allowed me to approach the action and interviews with pure curiosity, the kind that helps make any subject matter emotionally accessible." Kohs also said that "Unlike the film's human characters – who turn their curious quest for knowledge into an epic spectacle with great existential implications, who dare to risk their reputation and pride to contest that curiosity – AI might not yet possess the ability to empathize. But it can teach us profound things about our humanness – the way we play board games, the way we think and feel and grow. It's a deep, vast premise, but my hope is, by sharing it, we can discover something within ourselves we never saw before".

Professional Go player
Hajin Lee, a former professional Go player, described this documentary as being "beautifully filmed". In addition to the story itself, the feelings and atmosphere were also conveyed through different scene arrangements. For example, the close-up shots of Lee Sedol when he realizes that the AlphaGo AI is intelligent, the atmospheric scene of the Korean commentator's distress and affliction following the first defeat, and the tension being held inside the room. The documentary also tells a story by describing the background of AlphaGo technology and the customs of the Korean Go community. She suggests some areas to be covered additionally. For instance, the details of the AI prior to AlphaGo, the confidence and pride of the professional Go players, and the shifting of perspective to the Go AI between and after the match as "If anything could be added, I would include information about the primitive level of top Go A.I.s before AlphaGo, and more about professional Go players' lives and pride, to provide more context for Lee Sedol's pre-match confidence, and Go players' changing perception of AlphaGo as the match advanced".(Lee, 2017).
Fan Hui, a professional Go player, and former player with AlphaGo said that "DeepMind had trained AlphaGo by showing it many strong amateur games of Go to develop its understanding of how a human plays before challenging it to play versions of itself thousands of times, a novel form of reinforcement learning which had given it the ability to rival an expert human. History had been made, and centuries of received learning overturned in the process. The program was free to learn the game for itself.

Technology and AI-related fields
James Vincent, a reporter from The Verge, comments that "It prods and pokes viewers with unsubtle emotional cues, like a reality TV show would. "Now, you should be nervous; now you should feel relieved". The AlphaGo footage slowly captures the moment when Lee Sedol acknowledges the true power of AlphaGo AI. In the first game, he had more experience than his human-programmed AI, so he thought it would be easy to beat the AI. However, the early game dynamics were not what he expected. After losing the first match, he became more nervous and lost confidence. Afterward, he reacted to attacks by saying that he just wanted to win the match, unintentionally displaying his anger, and acting in an unusual way. Also, he spends 12 minutes on one move, while AlphaGo only takes a minute and a half to respond. AlphaGo weighs each alternative equally and consistently. No reaction to Lee's fight. Instead, the game continues as if he was not there.
James also said that "suffice to say that humanity does land at least one blow on the machines, through Lee's so-called "divine move". "More likely, the forces of automation we'll face will be impersonal and incomprehensible. They'll come in the form of star ratings we can't object to, and algorithms we can't fully understand. Dealing with the problems of AI will take a perspective that looks beyond individual battles. AlphaGo is worth seeing because it raises these questions" (Vincent, 2017)
Murray Shanahan, a professor of cognitive robotics at Imperial College London, critics that "Go is an extraordinary game but it represents what we can do with AI in all kinds of other spheres," says Murray Shanahan, professor of cognitive robotics at Imperial College London and senior research scientist at DeepMind, says. "In just the same way there are all kinds of realms of possibility within Go that have not been discovered, we could never have imagined the potential for discovering drugs and other materials."

Spiritual and cultural expert
Director Greg Kohs brings out all of the drama and rush of these games to show the selection process for the master of Go. On this small stage, the human Go champion and the AI challenger engage in an intense duel. In this documentary, Kohs looks into how the human mind functions under pressure, the significance of errors, the actions of the computer, and its inventiveness. (Frederic and Mary Ann Brussat, n.d.)

Similar systems
Facebook has also been working on its own Go-playing system darkforest, also based on combining machine learning and Monte Carlo tree search. Although a strong player against other computer Go programs, as of early 2016, it had not yet defeated a professional human player. Darkforest has lost to CrazyStone and Zen and is estimated to be of similar strength to CrazyStone and Zen.
DeepZenGo, a system developed with support from video-sharing website Dwango and the University of Tokyo, lost 2–1 in November 2016 to Go master Cho Chikun, who holds the record for the largest number of Go title wins in Japan.
A 2018 paper in Nature cited AlphaGo's approach as the basis for a new means of computing potential pharmaceutical drug molecules. Systems consisting of Monte Carlo tree search guided by neural networks have since been explored for a wide array of applications.

Example game
AlphaGo Master (white) v. Tang Weixing (31 December 2016), AlphaGo won by resignation. White 36 was widely praised.

Impacts on Go
The documentary film AlphaGo raised hopes that Lee Sedol and Fan Hui would have benefitted from their experience of playing AlphaGo, but as of May 2018, their ratings were little changed; Lee Sedol was ranked 11th in the world, and Fan Hui 545th. On 19 November 2019, Lee announced his retirement from professional play, arguing that he could never be the top overall player of Go due to the increasing dominance of AI. Lee referred to them as being "an entity that cannot be defeated".

See also
References
External links
 Media related to AlphaGo at Wikimedia Commons
 Quotations related to AlphaGo at Wikiquote
Official website
AlphaGo wiki at Sensei's Library, including links to AlphaGo games
AlphaGo page, with archive and games
Estimated 2017 rating of Alpha Go
AlphaGo - The Movie on YouTube
AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero. 
On December 5, 2017, the DeepMind team released a preprint paper introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, Elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs. 
DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018. While the actual AlphaZero program has not been released to the public, the algorithm described in the paper has been implemented in publicly available software. In 2019, DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.

Relation to AlphaGo Zero
AlphaZero (AZ) is a more generalized variant of the AlphaGo Zero (AGZ) algorithm, and is able to play shogi and chess as well as Go. Differences between AZ and AGZ include:

AZ has hard-coded rules for setting search hyperparameters.
The neural network is now updated continually.
AZ doesn't use symmetries, unlike AGZ.
Chess or Shogi can end in a draw unlike Go; therefore, AlphaZero takes into account the possibility of a drawn game.

Stockfish and Elmo
Comparing Monte Carlo tree search searches, AlphaZero searches just 80,000 positions per second in chess and 40,000 in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variation.

Training
AlphaZero was trained solely via self-play, using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks. Training took several days, totaling about 41 TPU-years. In parallel, the in-training AlphaZero was periodically matched against its benchmark (Stockfish, Elmo, or AlphaGo Zero) in brief one-second-per-move games to determine how well the training was progressing. DeepMind judged that AlphaZero's performance exceeded the benchmark after around four hours of training for Stockfish, two hours for Elmo, and eight hours for AlphaGo Zero.

Preliminary results
Outcome
Chess
In AlphaZero's chess match against Stockfish 8 (2016 TCEC world champion), each program was given one minute per move. AlphaZero was flying the English flag, while Stockfish the Norwegian. Stockfish was allocated 64 threads and a hash size of 1 GB, a setting that Stockfish's Tord Romstad later criticized as suboptimal. AlphaZero was trained on chess for a total of nine hours before the match. During the match, AlphaZero ran on a single machine with four application-specific TPUs. In 100 games from the normal starting position, AlphaZero won 25 games as White, won 3 as Black, and drew the remaining 72. In a series of twelve, 100-game matches (of unspecified time or resource constraints) against Stockfish starting from the 12 most popular human openings, AlphaZero won 290, drew 886 and lost 24.

Shogi
AlphaZero was trained on shogi for a total of two hours before the tournament. In 100 shogi games against Elmo (World Computer Shogi Championship 27 summer 2017 tournament version with YaneuraOu 4.73 search), AlphaZero won 90 times, lost 8 times and drew twice. As in the chess games, each program got one minute per move, and Elmo was given 64 threads and a hash size of 1 GB.

Go
After 34 hours of self-learning of Go and against AlphaGo Zero, AlphaZero won 60 games and lost 40.

Analysis
DeepMind stated in its preprint, "The game of chess represented the pinnacle of AI research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic reinforcement learning algorithm –  originally devised for the game of go –  that achieved superior results within a few hours, searching a thousand times fewer positions, given no domain knowledge except the rules." DeepMind's Demis Hassabis, a chess player himself, called AlphaZero's play style "alien": It sometimes wins by offering counterintuitive sacrifices, like offering up a queen and bishop to exploit a positional advantage. "It's like chess from another dimension."
Given the difficulty in chess of forcing a win against a strong opponent, the +28 –0 =72 result is a significant margin of victory. However, some grandmasters, such as Hikaru Nakamura and Komodo developer Larry Kaufman, downplayed AlphaZero's victory, arguing that the match would have been closer if the programs had access to an opening database (since Stockfish was optimized for that scenario). Romstad additionally pointed out that Stockfish is not optimized for rigidly fixed-time moves and the version used was a year old.
Similarly, some shogi observers argued that the Elmo hash size was too low, that the resignation settings and the "EnteringKingRule" settings (cf. shogi § Entering King) may have been inappropriate, and that Elmo is already obsolete compared with newer programs.

Reaction and criticism
Papers headlined that the chess training took only four hours: "It was managed in little more than the time between breakfast and lunch." Wired described AlphaZero as "the first multi-skilled AI board-game champ". AI expert Joanna Bryson noted that Google's "knack for good publicity" was putting it in a strong position against challengers. "It's not only about hiring the best programmers. It's also very political, as it helps make Google as strong as possible when negotiating with governments and regulators looking at the AI sector."
Human chess grandmasters generally expressed excitement about AlphaZero. Danish grandmaster Peter Heine Nielsen likened AlphaZero's play to that of a superior alien species. Norwegian grandmaster Jon Ludvig Hammer characterized AlphaZero's play as "insane attacking chess" with profound positional understanding. Former champion Garry Kasparov said, "It's a remarkable achievement, even if we should have expected it after AlphaGo."
Grandmaster Hikaru Nakamura was less impressed, stating: "I don't necessarily put a lot of credibility in the results simply because my understanding is that AlphaZero is basically using the Google supercomputer and Stockfish doesn't run on that hardware; Stockfish was basically running on what would be my laptop. If you wanna have a match that's comparable you have to have Stockfish running on a supercomputer as well."
Top US correspondence chess player Wolff Morrow was also unimpressed, claiming that AlphaZero would probably not make the semifinals of a fair competition such as TCEC where all engines play on equal hardware. Morrow further stated that although he might not be able to beat AlphaZero if AlphaZero played drawish openings such as the Petroff Defence, AlphaZero would not be able to beat him in a correspondence chess game either.
Motohiro Isozaki, the author of YaneuraOu, noted that although AlphaZero did comprehensively beat Elmo, the rating of AlphaZero in shogi stopped growing at a point which is at most 100–200 higher than Elmo. This gap is not that high, and Elmo and other shogi software should be able to catch up in 1–2 years.

Final results
DeepMind addressed many of the criticisms in their final version of the paper, published in December 2018 in Science. They further clarified that AlphaZero was not running on a supercomputer; it was trained using 5,000 tensor processing units (TPUs), but only ran on four TPUs and a 44-core CPU in its matches.

Chess
In the final results, Stockfish 9 dev ran under the same conditions as in the TCEC superfinal: 44 CPU cores, Syzygy endgame tablebases, and a 32 GB hash size. Instead of a fixed time control of one move per minute, both engines were given 3 hours plus 15 seconds per move to finish the game. In a 1000-game match, AlphaZero won with a score of 155 wins, 6 losses, and 839 draws. DeepMind also played a series of games using the TCEC opening positions; AlphaZero also won convincingly. Stockfish needed 10-to-1 time odds to match AlphaZero.

Shogi
Similar to Stockfish, Elmo ran under the same conditions as in the 2017 CSA championship. The version of Elmo used was WCSC27 in combination with YaneuraOu 2017 Early KPPT 4.79 64AVX2 TOURNAMENT. Elmo operated on the same hardware as Stockfish: 44 CPU cores and a 32 GB hash size. AlphaZero won 98.2% of games when playing sente (i.e. having the first move) and 91.2% overall.

Reactions and criticisms
Human grandmasters were generally impressed with AlphaZero's games against Stockfish. Former world champion Garry Kasparov said it was a pleasure to watch AlphaZero play, especially since its style was open and dynamic like his own.
In the computer chess community, Komodo developer Mark Lefler called it a "pretty amazing achievement", but also pointed out that the data was old, since Stockfish had gained a lot of strength since January 2018 (when Stockfish 8 was released). Fellow developer Larry Kaufman said AlphaZero would probably lose a match against the latest version of Stockfish, Stockfish 10, under Top Chess Engine Championship (TCEC) conditions. Kaufman argued that the only advantage of neural network–based engines was that they used a GPU, so if there was no regard for power consumption (e.g. in an equal-hardware contest where both engines had access to the same CPU and GPU) then anything the GPU achieved was "free". Based on this, he stated that the strongest engine was likely to be a hybrid with neural networks and standard alpha–beta search.
AlphaZero inspired the computer chess community to develop Leela Chess Zero, using the same techniques as AlphaZero. Leela contested several championships against Stockfish, where it showed roughly similar strength to Stockfish, although Stockfish has since pulled away.
In 2019 DeepMind published MuZero, a unified system that played excellent chess, shogi, and go, as well as games in the Atari Learning Environment, without being pre-programmed with their rules.

See also
Notes
References
External links
Chessprogramming wiki on AlphaZero
Chess.com Youtube playlist for AlphaZero vs. Stockfish
Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Clients will often use this in combination with autoscaling (a process that allows a client to use more computing in times of high application usage, and then scale down to reduce costs when there is less traffic). These cloud computing web services provide various services related to networking, compute, storage, middleware, IoT and other processing capacity, as well as software tools via AWS server farms.  This frees clients from managing, scaling, and patching hardware and operating systems. 
One of the foundational services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, with extremely high availability, which can be interacted with over the internet via REST APIs, a CLI or the AWS console.  AWS's virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; hard-disk (HDD)/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).
AWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a "Pay-as-you-go" model), hardware, operating system, software, and networking features chosen by the subscriber requiring various degrees of availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either. Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including seven in North America.
Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm. All services are billed based on usage, but each service measures usage in varying ways. As of 2023 Q1, AWS has 31% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 25%, and 11% respectively, according to Synergy Research Group.

Services
As of 2021, AWS comprises over 200 products and services including computing, storage, networking, database, analytics, application services, deployment, management, machine learning, mobile, developer tools, RobOps and tools for the Internet of Things. The most popular include Amazon Elastic Compute Cloud (EC2), Amazon Simple Storage Service (Amazon S3), Amazon Connect, and AWS Lambda (a serverless function that can perform arbitrary code written in any language that can be configured to be triggered by hundreds of events, including HTTP calls).
Services expose functionality through APIs for clients to use in their applications.  These APIs are accessed over HTTP, using the REST architectural style and SOAP protocol for older APIs and exclusively JSON for newer ones. Clients can interact with these APIs in various ways, including from the AWS console (a website), by using SDKs written in various languages (such as Python, Java, and JavaScript), or by making direct REST calls.

History
Founding (2000–2005)
The genesis of AWS came in the early 2000s. After building Merchant.com, Amazon's e-commerce-as-a-service platform that offers third-party retailers a way to build their own web-stores, Amazon pursued service-oriented architecture as a means to scale its engineering operations, led by then CTO Allan Vermeulen.
Around the same time frame, Amazon was frustrated with the speed of its software engineering, and sought to implement various recommendations put forth by Matt Round, an engineering leader at the time, including maximization of autonomy for engineering teams, adoption of REST, standardization of infrastructure, removal of gate-keeping decision-makers (bureaucracy), and continuous deployment. He also called for increasing the percentage of the time engineers spent building the software rather than doing other tasks. Amazon created "a shared IT platform" so its engineering organizations, which were spending 70% of their time on "undifferentiated heavy-lifting" such as IT and infrastructure problems, could focus on customer-facing innovation instead. Besides, in dealing with unusual peak traffic patterns, especially during the holiday season, by migrating services to commodity Linux hardware and relying on open source software, Amazon's Infrastructure team, led by Tom Killalea, Amazon's first CISO, had already run its data centers and associated services in a "fast, reliable, cheap" way.
In July 2002 Amazon.com Web Services, managed by Colin Bryar, launched its first web services, opening up the Amazon.com platform to all developers. Over one hundred applications were built on top of it by 2004.  This unexpected developer interest took Amazon by surprise and convinced them that developers were "hungry for more".
By the summer of 2003, Andy Jassy had taken over Bryar's portfolio at Rick Dalzell's behest, after Vermeulen, who was Bezos' first pick, declined the offer. Jassy subsequently mapped out the vision for an "Internet OS" made up of foundational infrastructure primitives that alleviated key impediments to shipping software applications faster. By fall 2003, databases, storage, and compute were identified as the first set of infrastructure pieces that Amazon should launch.
Jeff Barr, an early AWS employee, credits Vermeulen, Jassy, Bezos himself, and a few others for coming up with the idea that would evolve into EC2, S3, and RDS; Jassy recalls the idea was the result of brainstorming for about a week with "ten of the best technology minds and ten of the best product management minds" on about ten different internet applications and the most primitive building blocks required to build them. Werner Vogels cites Amazon's desire to make the process of "invent, launch, reinvent, relaunch, start over, rinse, repeat" as fast as it could was leading them to break down organizational structures with "two-pizza teams" and application structures with distributed systems; and that these changes ultimately paved way for the formation of AWS and its mission "to expose all of the atomic-level pieces of the Amazon.com platform". According to Brewster Kahle, co-founder of Alexa Internet, which was acquired by Amazon in 1999, his start-up's compute infrastructure helped Amazon solve its big data problems and later informed the innovations that underpinned AWS.
Jassy assembled a founding team of 57 employees from a mix of engineering and business backgrounds to kick-start these initiatives, with a majority of the hires coming from outside the company; Jeff Lawson, Twilio CEO, Adam Selipsky, Tableau CEO, and Mikhail Seregine, co-founder at Outschool among them.
In late 2003, the concept for compute, which would later launch as EC2, was reformulated when Chris Pinkham and Benjamin Black presented a paper internally describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the company could generate revenue from the new infrastructure investment. Thereafter Pinkham, Willem van Biljon, and lead developer Christopher Brown developed the Amazon EC2 service, with a team in Cape Town, South Africa.
In November 2004, AWS launched its first infrastructure service for public usage: Simple Queue Service (SQS).

S3, EC2, and other first generation services (2006–2010)
On March 14, 2006, AWS launched Amazon S3 cloud storage followed by EC2 in August 2006. Andy Jassy, AWS founder and vice president in 2006, said at the time that Amazon S3 "helps free developers from worrying about where they are going to store data, whether it will be safe and secure, if it will be available when they need it, the costs associated with server maintenance, or whether they have enough storage available. Amazon S3 enables developers to focus on innovating with data, rather than figuring out how to store it." Pi Corporation, a startup Paul Maritz co-founded, was the first beta-user of EC2 outside of Amazon, while Microsoft was among EC2's first enterprise customers. Later that year, SmugMug, one of the early AWS adopters, attributed savings of around US$400,000 in storage costs to S3. According to Vogels, S3 was built with 8 microservices when it launched in 2006, but had over 300 microservices by 2022.
In September 2007, AWS announced its annual Start-up Challenge, a contest with prizes worth $100,000 for entrepreneurs and software developers based in the US using AWS services such as S3 and EC2 to build their businesses. The first edition saw participation from Justin.tv, which Amazon would later acquire in 2014. Ooyala, an online media company, was the eventual winner.
Additional AWS services from this period include SimpleDB, Mechanical Turk, Elastic Block Store, Elastic Beanstalk, Relational Database Service, DynamoDB, CloudWatch, Simple Workflow, CloudFront, and Availability Zones.

Growth (2010–2015)
In November 2010, it was reported that all of Amazon.com's retail sites had migrated to AWS. Prior to 2012, AWS was considered a part of Amazon.com and so its revenue was not delineated in Amazon financial statements. In that year industry watchers for the first time estimated AWS revenue to be over $1.5 billion.
On November 27, 2012, AWS hosted its first major annual conference, re:Invent with a focus on AWS's partners and ecosystem, with over 150 sessions. The three-day event was held in Las Vegas because of its relatively cheaper connectivity with locations across the United States and the rest of the world. Andy Jassy and Werner Vogels presented keynotes, with Jeff Bezos joining Vogels for a fireside chat. AWS opened early registrations at  US$1,099 per head for their customers from over 190 countries. On stage with Andy Jassy at the event which saw around 6000 attendees, Reed Hastings, CEO at Netflix, announced plans to migrate 100% of Netflix's infrastructure to AWS.
To support industry-wide training and skills standardization, AWS began offering a certification program for computer engineers, on April 30, 2013, to highlight expertise in cloud computing. Later that year, in October, AWS launched Activate, a program for start-ups worldwide to leverage AWS credits, third-party integrations, and free access to AWS experts to help build their business.
In 2014, AWS launched its partner network, AWS Partner Network (APN), which is focused on helping AWS-based companies grow and scale the success of their business with close collaboration and best practices.
In January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics company for a reported US$350–370M.
In April 2015, Amazon.com reported AWS was profitable, with sales of $1.57 billion in the first quarter of the year and $265 million of operating income. Founder Jeff Bezos described it as a fast-growing $5 billion business; analysts described it as "surprisingly more profitable than forecast". In October, Amazon.com said in its Q3 earnings report that AWS's operating income was $521 million, with operating margins at 25 percent. AWS's 2015 Q3 revenue was $2.1 billion, a 78% increase from 2014's Q3 revenue of $1.17 billion. 2015 Q4 revenue for the AWS segment increased 69.5% y/y to $2.4 billion with a 28.5% operating margin, giving AWS a $9.6 billion run rate. In 2015, Gartner estimated that AWS customers are deploying 10x more infrastructure on AWS than the combined adoption of the next 14 providers.

Current era (2016–present)
In 2016 Q1, revenue was $2.57 billion with net income of $604 million, a 64% increase over 2015 Q1 that resulted in AWS being more profitable than Amazon's North American retail business for the first time. Jassy was thereafter promoted to CEO of the division. Around the same time, Amazon experienced a 42% rise in stock value as a result of increased earnings, of which AWS contributed 56% to corporate profits.
AWS had $17.46 billion in annual revenue in 2017. By the end of 2020, the number had grown to $46 billion. Reflecting the success of AWS, Jassy's annual compensation in 2017 hit nearly $36 million.
In January 2018, Amazon launched an autoscaling service on AWS.
In November 2018, AWS announced customized ARM cores for use in its servers. Also in November 2018, AWS is developing ground stations to communicate with customers' satellites.
In 2019, AWS reported 37% yearly growth and accounted for 12% of Amazon's revenue (up from 11% in 2018).
In April 2021, AWS reported 32% yearly growth and accounted for 32% of $41.8 billion cloud market in Q1 2021.
In January 2022, AWS joined the MACH Alliance, a non-profit enterprise technology advocacy group.
In June 2022, it was reported that in 2019 Capital One had not secured their AWS resources properly, and was subject to a data breach by a former AWS employee. The employee was convicted of hacking into the company's cloud servers to steal customer data and use computer power to mine cryptocurrency. The ex-employee was able to download the personal information of more than 100 million Capital One customers.
In June 2022, AWS announced they had launched the AWS Snowcone, a small computing device, to the International Space Station on the Axiom Mission 1.
In September 2023, AWS announced it would become AI startup Anthropic's primary cloud provider. Amazon has committed to investing up to $4 billion in Anthropic and will have a minority ownership position in the company. AWS also announced the GA of Amazon Bedrock, a fully managed service that makes foundation models (FMs) from leading AI companies available through a single application programming interface (API)
In April 2024, AWS announced a new service called Deadline Cloud, which lets customers set up, deploy and scale up graphics and visual effects rendering pipelines on AWS cloud infrastructure.

Customer base
Notable customers include NASA, and the Obama presidential campaign of 2012.
In October 2013, AWS was awarded a $600M contract with the CIA.
In 2019, it was reported that more than 80% of Germany's listed DAX companies use AWS.
In August 2019, the U.S. Navy said it moved 72,000 users from six commands to an AWS cloud system as a first step toward pushing all of its data and analytics onto the cloud.
In 2021, DISH Network announced it will develop and launch its 5G network on AWS.
In October 2021, it was reported that spy agencies and government departments in the UK such as GCHQ, MI5, MI6, and the Ministry of Defence, have contracted AWS to host their classified materials.
Multiple financial services firms have shifted to AWS in some form.

Significant service outages
On April 20, 2011, AWS suffered a major outage. Parts of the Elastic Block Store service became "stuck" and could not fulfill read/write requests. It took at least two days for the service to be fully restored.
On June 29, 2012, several websites that rely on Amazon Web Services were taken offline due to a severe storm in Northern Virginia, where AWS's largest data center cluster is located.
On October 22, 2012, a major outage occurred, affecting many sites such as Reddit, Foursquare, Pinterest, and others. The cause was a memory leak bug in an operational data collection agent.
On December 24, 2012, AWS suffered another outage causing websites such as Netflix to be unavailable for customers in the Northeastern United States. AWS cited their Elastic Load Balancing service as the cause.
On February 28, 2017, AWS experienced a massive outage of S3 services in its Northern Virginia region. A majority of websites that relied on AWS S3 either hung or stalled, and Amazon reported within five hours that AWS was fully online again. No data has been reported to have been lost due to the outage. The outage was caused by a human error made while debugging, that resulted in removing more server capacity than intended, which caused a domino effect of outages.
On November 25, 2020, AWS experienced several hours of outage on the Kinesis service in North Virginia (US-East-1) region. Other services relying on Kinesis were also impacted.
On December 7, 2021, an outage mainly affected the Eastern United States, disrupting delivery service and streaming.

Availability and topology
As of March 2024, AWS has distinct operations in 33 geographical "regions": eight in North America, one in South America, eight in Europe, three in the Middle East, one in Africa, and twelve in Asia Pacific.
Most AWS regions are enabled by default for AWS accounts. Regions introduced after 20 March 2019 are considered to be opt-in regions, requiring a user to explicitly enable them in order for the region to be usable in the account. For opt-in regions, Identity and Access Management (IAM) resources such as users and roles are only propagated to the regions that are enabled.
Each region is wholly contained within a single country and all of its data and services stay within the designated region. Each region has multiple "Availability Zones", which consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. Availability Zones do not automatically provide additional scalability or redundancy within a region, since they are intentionally isolated from each other to prevent outages from spreading between zones. Several services can operate across Availability Zones (e.g., S3, DynamoDB) while others can be configured to replicate across zones to spread demand and avoid downtime from failures.
As of December 2014, Amazon Web Services operated an estimated 1.4 million servers across 11 regions and 28 availability zones. The global network of AWS Edge locations consists of over 300 points of presence worldwide, including locations in North America, Europe, Asia, Australia, Africa, and South America.
As of March 2024, AWS has announced the planned launch of six additional regions in Malaysia, Mexico, New Zealand, Thailand, Saudi Arabia, and the European Union. In mid March 2023, Amazon Web Services signed a cooperation agreement with the New Zealand Government to build large data centers in New Zealand.
In 2014, AWS claimed its aim was to achieve 100% renewable energy usage in the future.  In the United States, AWS's partnerships with renewable energy providers include Community Energy of Virginia, to support the US East region; Pattern Development, in January 2015, to construct and operate Amazon Wind Farm Fowler Ridge; Iberdrola Renewables, LLC, in July 2015, to construct and operate Amazon Wind Farm US East; EDP Renewables North America, in November 2015, to construct and operate Amazon Wind Farm US Central; and Tesla Motors, to apply battery storage technology to address power needs in the US West (Northern California) region.

Pop-up lofts
AWS also has "pop-up lofts" in different locations around the world. These market AWS to entrepreneurs and startups in different tech industries in a physical location. Visitors can work or relax inside the loft, or learn more about what they can do with AWS. In June 2014, AWS opened their first temporary pop-up loft in San Francisco. In May 2015 they expanded to New York City, and in September 2015 expanded to Berlin. AWS opened its fourth location, in Tel Aviv from March 1, 2016, to March 22, 2016. A pop-up loft was open in London from September 10 to October 29, 2015. The pop-up lofts in New York and San Francisco are indefinitely closed due to the COVID-19 pandemic while Tokyo has remained open in a limited capacity.

Charitable work
In 2017, AWS launched AWS re/Start in the United Kingdom to help young adults and military veterans retrain in technology-related skills.  In partnership with the Prince's Trust and the Ministry of Defence (MoD), AWS will help to provide re-training opportunities for young people from disadvantaged backgrounds and former military personnel.  AWS is working alongside a number of partner companies including Cloudreach, Sage Group, EDF Energy, and Tesco Bank.
In April 2022, AWS announced the organization has committed more than $30 million over three years to early-stage start-ups led by Black, Latino, LGBTQIA+, and Women founders as part of its AWS impact Accelerator. The Initiative offers qualifying start-ups up to $225,000 in cash, credits, extensive training, mentoring, technical guidance and includes up to $100,000 in AWS service credits.

Reception
Environmental impact
In 2016, Greenpeace assessed major tech companies—including cloud services providers like AWS, Microsoft, Oracle, Google, IBM, Salesforce and Rackspace—based on their level of "clean energy" usage. Greenpeace evaluated companies on their mix of renewable-energy sources; transparency; renewable-energy commitment and policies; energy efficiency and greenhouse-gas mitigation; renewable-energy procurement; and advocacy. The group gave AWS an overall "C" grade. Greenpeace credited AWS for its advances toward greener computing in recent years and its plans to launch multiple wind and solar farms across the United States. The organization stated that Amazon is opaque about its carbon footprint.
In January 2021, AWS joined an industry pledge to achieve climate neutrality of data centers by 2030, the Climate Neutral Data Centre Pact. As of 2023, Amazon as a whole is the largest corporate purchaser of renewable energy in the world, a position it has held since 2020, and has a global portfolio of over 20 GW of renewable energy capacity. In 2022, 90% of all Amazon operations, including data centers, were powered by renewables.

Denaturalization protest
US Department of Homeland Security has employed the software ATLAS, which runs on Amazon Cloud. It scanned more than 16.5 million records of naturalized Americans and flagged approximately 124,000 of them for manual analysis and review by USCIS officers regarding denaturalization. Some of the scanned data came from the Terrorist Screening Database and the National Crime Information Center. The algorithm and the criteria for the algorithm were secret. Amazon faced protests from its own employees and activists for the anti-migrant collaboration with authorities.

Israeli–Palestinian conflict
The contract for Project Nimbus drew rebuke and condemnation from the companies' shareholders as well as their employees, over concerns that the project would lead to abuses of Palestinians' human rights in the context of the ongoing occupation and the Israeli–Palestinian conflict. Specifically, they voice concern over how the technology will enable further surveillance of Palestinians and unlawful data collection on them as well as facilitate the expansion of Israel's illegal settlements on Palestinian land. A government procurement document featuring 'obligatory customers' of Nimbus, including "two of Israel’s leading state-owned weapons manufacturers" Israel Aerospace Industries and Rafael Advanced Defense Systems, was published in 2021 with periodic updates since (up to Oct 2023).

See also
Tim Bray
Cloud-computing comparison
Comparison of file hosting services
James Gosling

Explanatory notes
References
External links

Official website
In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.
The term "analysis of algorithms" was coined by Donald Knuth. Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.
In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the size n of the sorted list being searched, or in O(log n), colloquially "in logarithmic time". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two "reasonable" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.
Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called a model of computation. A model of computation may be defined in terms of an abstract computer, e.g. Turing machine, and/or by postulating that certain operations are executed in unit time.
For example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2(n) + 1 time units are needed to return an answer.

Cost models
Time efficiency estimates depend on what we define to be a step. For the analysis to correspond usefully to the actual run-time, the time required to perform a step must be guaranteed to be bounded above by a constant. One must be careful here; for instance, some analyses count an addition of two numbers as one step. This assumption may not be warranted in certain contexts. For example, if the numbers involved in a computation may be arbitrarily large, the time required by a single addition can no longer be assumed to be constant.
Two cost models are generally used:

the uniform cost model, also called unit-cost model (and similar variations), assigns a constant cost to every machine operation, regardless of the size of the numbers involved
the logarithmic cost model, also called logarithmic-cost measurement (and similar variations), assigns a cost to every machine operation proportional to the number of bits involved
The latter is more cumbersome to use, so it is only employed when necessary, for example in the analysis of arbitrary-precision arithmetic algorithms, like those used in cryptography.
A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice and therefore there are algorithms that are faster than what would naively be thought possible.

Run-time analysis
Run-time analysis is a theoretical classification that estimates and anticipates the increase in running time (or run-time or execution time) of an algorithm as its input size (usually denoted as n) increases.  Run-time efficiency is a topic of great interest in computer science:  A program can take seconds, hours, or even years to finish executing, depending on which algorithm it implements. While software profiling techniques can be used to measure an algorithm's run-time in practice, they cannot provide timing data for all infinitely many possible inputs; the latter can only be achieved by the theoretical methods of run-time analysis.

Shortcomings of empirical metrics
Since algorithms are platform-independent (i.e. a given algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system), there are additional significant drawbacks to using an empirical approach to gauge the comparative performance of a given set of algorithms.
Take as an example a program that looks up a specific entry in a sorted list of size n.  Suppose this program were implemented on Computer A, a state-of-the-art machine, using a linear search algorithm, and on Computer B, a much slower machine, using a binary search algorithm.  Benchmark testing on the two computers running their respective programs might look something like the following:

Based on these metrics, it would be easy to jump to the conclusion that Computer A is running an algorithm that is far superior in efficiency to that of Computer B.  However, if the size of the input-list is increased to a sufficient number, that conclusion is dramatically demonstrated to be in error:

Computer A, running the linear search program, exhibits a linear growth rate.  The program's run-time is directly proportional to its input size.  Doubling the input size doubles the run-time, quadrupling the input size quadruples the run-time, and so forth.  On the other hand, Computer B, running the binary search program, exhibits a logarithmic growth rate.  Quadrupling the input size only increases the run-time by a constant amount (in this example, 50,000 ns).  Even though Computer A is ostensibly a faster machine, Computer B will inevitably surpass Computer A in run-time because it is running an algorithm with a much slower growth rate.

Orders of growth
Informally, an algorithm can be said to exhibit a growth rate on the order of a mathematical function if beyond a certain input size n, the function f(n) times a positive constant provides an upper bound or limit for the run-time of that algorithm.  In other words, for a given input size n greater than some n0 and a constant c, the run-time of that algorithm will never be larger than c × f(n).  This concept is frequently expressed using Big O notation.  For example, since the run-time of insertion sort grows quadratically as its input size increases, insertion sort can be said to be of order O(n2).
Big O notation is a convenient way to express the worst-case scenario for a given algorithm, although it can also be used to express the average-case — for example, the worst-case scenario for quicksort is O(n2), but the average-case run-time is O(n log n).

Empirical orders of growth
Assuming the run-time follows power rule, t ≈ kna, the coefficient a can be found  by taking empirical measurements of run-time {t1, t2} at some problem-size points {n1, n2}, and calculating t2/t1 = (n2/n1)a so that a = log(t2/t1)/log(n2/n1). In other words, this measures the slope of the empirical line on the log–log plot of run-time vs. input size, at some size point. If the order of growth indeed follows the power rule (and so the line on the log–log plot is indeed a straight line), the empirical value of  will  stay constant at different ranges, and if not, it will change (and the line is a curved line)—but still could serve for comparison of any two given algorithms as to their empirical local orders of growth behaviour. Applied to the above table:

It is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule. The empirical values for the second one are diminishing rapidly, suggesting it follows another rule of growth and in any case has much lower local orders of growth (and improving further still), empirically, than the first one.

Evaluating run-time complexity
The run-time complexity for the worst-case scenario of a given algorithm can sometimes be evaluated by examining the structure of the algorithm and making some simplifying assumptions.  Consider the following pseudocode:

1    get a positive integer n from input
2    if n > 10
3        print "This might take a while..."
4    for i = 1 to n
5        for j = 1 to i
6            print i * j
7    print "Done!"

A given computer will take a discrete amount of time to execute each of the instructions involved with carrying out this algorithm.  Say that the actions carried out in step 1 are considered to consume time at most T1, step 2 uses time at most T2, and so forth.
In the algorithm above, steps 1, 2 and 7 will only be run once.  For a worst-case evaluation, it should be assumed that step 3 will be run as well.  Thus the total amount of time to run steps 1-3 and step 7 is:

  
    
      
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        .
        
      
    
    {\displaystyle T_{1}+T_{2}+T_{3}+T_{7}.\,}
  

The loops in steps 4, 5 and 6 are trickier to evaluate.  The outer loop test in step 4 will execute ( n + 1 )
times, which will consume T4( n + 1 ) time.  The inner loop, on the other hand, is governed by the value of j, which iterates from 1 to i.  On the first pass through the outer loop, j iterates from 1 to 1:  The inner loop makes one pass, so running the inner loop body (step 6) consumes T6 time, and the inner loop test (step 5) consumes 2T5 time.  During the next pass through the outer loop, j iterates from 1 to 2:  the inner loop makes two passes, so running the inner loop body (step 6) consumes 2T6 time, and the inner loop test (step 5) consumes 3T5 time.
Altogether, the total time required to run the inner loop body can be expressed as an arithmetic progression:

  
    
      
        
          T
          
            6
          
        
        +
        2
        
          T
          
            6
          
        
        +
        3
        
          T
          
            6
          
        
        +
        ⋯
        +
        (
        n
        −
        1
        )
        
          T
          
            6
          
        
        +
        n
        
          T
          
            6
          
        
      
    
    {\displaystyle T_{6}+2T_{6}+3T_{6}+\cdots +(n-1)T_{6}+nT_{6}}
  

which can be factored as

  
    
      
        
          [
          
            1
            +
            2
            +
            3
            +
            ⋯
            +
            (
            n
            −
            1
            )
            +
            n
          
          ]
        
        
          T
          
            6
          
        
        =
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            n
            )
          
          ]
        
        
          T
          
            6
          
        
      
    
    {\displaystyle \left[1+2+3+\cdots +(n-1)+n\right]T_{6}=\left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}}
  

The total time required to run the inner loop test can be evaluated similarly:

  
    
      
        
          
            
              
              
                2
                
                  T
                  
                    5
                  
                
                +
                3
                
                  T
                  
                    5
                  
                
                +
                4
                
                  T
                  
                    5
                  
                
                +
                ⋯
                +
                (
                n
                −
                1
                )
                
                  T
                  
                    5
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
                 
              
              
                
                  T
                  
                    5
                  
                
                +
                2
                
                  T
                  
                    5
                  
                
                +
                3
                
                  T
                  
                    5
                  
                
                +
                4
                
                  T
                  
                    5
                  
                
                +
                ⋯
                +
                (
                n
                −
                1
                )
                
                  T
                  
                    5
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    5
                  
                
                −
                
                  T
                  
                    5
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&2T_{5}+3T_{5}+4T_{5}+\cdots +(n-1)T_{5}+nT_{5}+(n+1)T_{5}\\=\ &T_{5}+2T_{5}+3T_{5}+4T_{5}+\cdots +(n-1)T_{5}+nT_{5}+(n+1)T_{5}-T_{5}\end{aligned}}}
  

which can be factored as

  
    
      
        
          
            
              
              
                
                  T
                  
                    5
                  
                
                
                  [
                  
                    1
                    +
                    2
                    +
                    3
                    +
                    ⋯
                    +
                    (
                    n
                    −
                    1
                    )
                    +
                    n
                    +
                    (
                    n
                    +
                    1
                    )
                  
                  ]
                
                −
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  [
                  
                    
                      
                        1
                        2
                      
                    
                    (
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    )
                  
                  ]
                
                
                  T
                  
                    5
                  
                
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    5
                  
                
                −
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  [
                  
                    
                      
                        1
                        2
                      
                    
                    (
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    )
                  
                  ]
                
                
                  T
                  
                    5
                  
                
                +
                n
                
                  T
                  
                    5
                  
                
              
            
            
              
                =
              
              
                
                  [
                  
                    
                      
                        1
                        2
                      
                    
                    (
                    
                      n
                      
                        2
                      
                    
                    +
                    3
                    n
                    )
                  
                  ]
                
                
                  T
                  
                    5
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&T_{5}\left[1+2+3+\cdots +(n-1)+n+(n+1)\right]-T_{5}\\=&\left[{\frac {1}{2}}(n^{2}+n)\right]T_{5}+(n+1)T_{5}-T_{5}\\=&\left[{\frac {1}{2}}(n^{2}+n)\right]T_{5}+nT_{5}\\=&\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}\end{aligned}}}
  

Therefore, the total run-time for this algorithm is:

  
    
      
        f
        (
        n
        )
        =
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        +
        (
        n
        +
        1
        )
        
          T
          
            4
          
        
        +
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            n
            )
          
          ]
        
        
          T
          
            6
          
        
        +
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            3
            n
            )
          
          ]
        
        
          T
          
            5
          
        
      
    
    {\displaystyle f(n)=T_{1}+T_{2}+T_{3}+T_{7}+(n+1)T_{4}+\left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}+\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}}
  

which reduces to

  
    
      
        f
        (
        n
        )
        =
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            n
            )
          
          ]
        
        
          T
          
            6
          
        
        +
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            3
            n
            )
          
          ]
        
        
          T
          
            5
          
        
        +
        (
        n
        +
        1
        )
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
      
    
    {\displaystyle f(n)=\left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}+\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}}
  

As a rule-of-thumb, one can assume that the highest-order term in any given function dominates its rate of growth and thus defines its run-time order.  In this example, n2 is the highest-order term, so one can conclude that f(n) = O(n2).  Formally this can be proven as follows:

Prove that 
  
    
      
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            n
            )
          
          ]
        
        
          T
          
            6
          
        
        +
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            3
            n
            )
          
          ]
        
        
          T
          
            5
          
        
        +
        (
        n
        +
        1
        )
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        ≤
        c
        
          n
          
            2
          
        
        ,
         
        n
        ≥
        
          n
          
            0
          
        
      
    
    {\displaystyle \left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}+\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\leq cn^{2},\ n\geq n_{0}}
  

  
    
      
        
          
            
              
              
                
                  [
                  
                    
                      
                        1
                        2
                      
                    
                    (
                    
                      n
                      
                        2
                      
                    
                    +
                    n
                    )
                  
                  ]
                
                
                  T
                  
                    6
                  
                
                +
                
                  [
                  
                    
                      
                        1
                        2
                      
                    
                    (
                    
                      n
                      
                        2
                      
                    
                    +
                    3
                    n
                    )
                  
                  ]
                
                
                  T
                  
                    5
                  
                
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
              
            
            
              
                ≤
              
              
                
                (
                
                  n
                  
                    2
                  
                
                +
                n
                )
                
                  T
                  
                    6
                  
                
                +
                (
                
                  n
                  
                    2
                  
                
                +
                3
                n
                )
                
                  T
                  
                    5
                  
                
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
                 
                (
                
                  for 
                
                n
                ≥
                0
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&\left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}+\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\\\leq &(n^{2}+n)T_{6}+(n^{2}+3n)T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\ ({\text{for }}n\geq 0)\end{aligned}}}
  

Let k be a constant greater than or equal to [T1..T7]

  
    
      
        
          
            
              
              
                
                  T
                  
                    6
                  
                
                (
                
                  n
                  
                    2
                  
                
                +
                n
                )
                +
                
                  T
                  
                    5
                  
                
                (
                
                  n
                  
                    2
                  
                
                +
                3
                n
                )
                +
                (
                n
                +
                1
                )
                
                  T
                  
                    4
                  
                
                +
                
                  T
                  
                    1
                  
                
                +
                
                  T
                  
                    2
                  
                
                +
                
                  T
                  
                    3
                  
                
                +
                
                  T
                  
                    7
                  
                
                ≤
                k
                (
                
                  n
                  
                    2
                  
                
                +
                n
                )
                +
                k
                (
                
                  n
                  
                    2
                  
                
                +
                3
                n
                )
                +
                k
                n
                +
                5
                k
              
            
            
              
                =
              
              
                2
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                n
                +
                5
                k
                ≤
                2
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                
                  n
                  
                    2
                  
                
                +
                5
                k
                
                  n
                  
                    2
                  
                
                 
                (
                
                  for 
                
                n
                ≥
                1
                )
                =
                12
                k
                
                  n
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&T_{6}(n^{2}+n)+T_{5}(n^{2}+3n)+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\leq k(n^{2}+n)+k(n^{2}+3n)+kn+5k\\=&2kn^{2}+5kn+5k\leq 2kn^{2}+5kn^{2}+5kn^{2}\ ({\text{for }}n\geq 1)=12kn^{2}\end{aligned}}}
  

Therefore 
  
    
      
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            n
            )
          
          ]
        
        
          T
          
            6
          
        
        +
        
          [
          
            
              
                1
                2
              
            
            (
            
              n
              
                2
              
            
            +
            3
            n
            )
          
          ]
        
        
          T
          
            5
          
        
        +
        (
        n
        +
        1
        )
        
          T
          
            4
          
        
        +
        
          T
          
            1
          
        
        +
        
          T
          
            2
          
        
        +
        
          T
          
            3
          
        
        +
        
          T
          
            7
          
        
        ≤
        c
        
          n
          
            2
          
        
        ,
        n
        ≥
        
          n
          
            0
          
        
        
           for 
        
        c
        =
        12
        k
        ,
        
          n
          
            0
          
        
        =
        1
      
    
    {\displaystyle \left[{\frac {1}{2}}(n^{2}+n)\right]T_{6}+\left[{\frac {1}{2}}(n^{2}+3n)\right]T_{5}+(n+1)T_{4}+T_{1}+T_{2}+T_{3}+T_{7}\leq cn^{2},n\geq n_{0}{\text{ for }}c=12k,n_{0}=1}
  

A more elegant approach to analyzing this algorithm would be to declare that [T1..T7] are all equal to one unit of time, in a system of units chosen so that one unit is greater than or equal to the actual times for these steps.  This would mean that the algorithm's run-time breaks down as follows:

  
    
      
        4
        +
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        i
        ≤
        4
        +
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        n
        =
        4
        +
        
          n
          
            2
          
        
        ≤
        5
        
          n
          
            2
          
        
         
        (
        
          for 
        
        n
        ≥
        1
        )
        =
        O
        (
        
          n
          
            2
          
        
        )
        .
      
    
    {\displaystyle 4+\sum _{i=1}^{n}i\leq 4+\sum _{i=1}^{n}n=4+n^{2}\leq 5n^{2}\ ({\text{for }}n\geq 1)=O(n^{2}).}

Growth rate analysis of other resources
The methodology of run-time analysis can also be utilized for predicting other growth rates, such as consumption of memory space.  As an example, consider the following pseudocode which manages and reallocates memory usage by a program based on the size of a file which that program manages:

while file is still open:
    let n = size of file
    for every 100,000 kilobytes of increase in file size
        double the amount of memory reserved

In this instance, as the file size n increases, memory will be consumed at an exponential growth rate, which is order O(2n). This is an extremely rapid and most likely unmanageable growth rate for consumption of memory resources.

Relevance
Algorithm analysis is important in practice because the accidental or unintentional use of an inefficient algorithm can significantly impact system performance. In time-sensitive applications, an algorithm taking too long to run can render its results outdated or useless. An inefficient algorithm can also end up requiring an uneconomical amount of computing power or storage in order to run, again rendering it practically useless.

Constant factors
Analysis of algorithms typically focuses on the asymptotic performance, particularly at the elementary level, but in practical applications constant factors are important, and real-world data is in practice always limited in size. The limit is typically the size of addressable memory, so on 32-bit machines 232 = 4 GiB (greater if segmented memory is used) and on 64-bit machines 264 = 16 EiB. Thus given a limited size, an order of growth (time or space) can be replaced by a constant factor, and in this sense all practical algorithms are O(1) for a large enough constant, or for small enough data.
This interpretation is primarily useful for functions that grow extremely slowly: (binary) iterated logarithm (log*) is less than 5 for all practical data (265536 bits); (binary) log-log (log log n) is less than 6 for virtually all practical data (264 bits); and binary log (log n) is less than 64 for virtually all practical data (264 bits). An algorithm with non-constant complexity may nonetheless be more efficient than an algorithm with constant complexity on practical data if the overhead of the constant time algorithm results in a larger constant factor, e.g., one may have 
  
    
      
        K
        >
        k
        log
        ⁡
        log
        ⁡
        n
      
    
    {\displaystyle K>k\log \log n}
  
 so long as 
  
    
      
        K
        
          /
        
        k
        >
        6
      
    
    {\displaystyle K/k>6}
  
 and 
  
    
      
        n
        <
        
          2
          
            
              2
              
                6
              
            
          
        
        =
        
          2
          
            64
          
        
      
    
    {\displaystyle n<2^{2^{6}}=2^{64}}
  
.
For large data linear or quadratic factors cannot be ignored, but for small data an asymptotically inefficient algorithm may be more efficient. This is particularly used in hybrid algorithms, like Timsort, which use an asymptotically efficient algorithm (here merge sort, with time complexity 
  
    
      
        n
        log
        ⁡
        n
      
    
    {\displaystyle n\log n}
  
), but switch to an asymptotically inefficient algorithm (here insertion sort, with time complexity 
  
    
      
        
          n
          
            2
          
        
      
    
    {\displaystyle n^{2}}
  
) for small data, as the simpler algorithm is faster on small data.

See also
Amortized analysis
Analysis of parallel algorithms
Asymptotic computational complexity
Best, worst and average case
Big O notation
Computational complexity theory
Master theorem (analysis of algorithms)
NP-Complete
Numerical analysis
Polynomial time
Program optimization
Profiling (computer programming)
Scalability
Smoothed analysis
Termination analysis — the subproblem of checking whether a program will terminate at all
Time complexity — includes table of orders of growth for common algorithms
Information-based complexity

Notes
References
Sedgewick, Robert; Flajolet, Philippe (2013). An Introduction to the Analysis of Algorithms (2nd ed.). Addison-Wesley. ISBN 978-0-321-90575-8.
Greene, Daniel A.; Knuth, Donald E. (1982). Mathematics for the Analysis of Algorithms (Second ed.). Birkhäuser. ISBN 3-7643-3102-X.
Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. & Stein, Clifford (2001). Introduction to Algorithms. Chapter 1: Foundations (Second ed.). Cambridge, MA: MIT Press and McGraw-Hill. pp. 3–122. ISBN 0-262-03293-7.
Sedgewick, Robert (1998). Algorithms in C, Parts 1-4: Fundamentals, Data Structures, Sorting, Searching (3rd ed.). Reading, MA: Addison-Wesley Professional. ISBN 978-0-201-31452-6.
Knuth, Donald. The Art of Computer Programming. Addison-Wesley.
Goldreich, Oded (2010). Computational Complexity: A Conceptual Perspective. Cambridge University Press. ISBN 978-0-521-88473-0.

External links
 Media related to Analysis of algorithms at Wikimedia Commons
Andrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a British-American computer scientist and technology entrepreneur focusing on machine learning and artificial intelligence (AI). Ng was a cofounder and head of Google Brain and was the former Chief Scientist at Baidu, building the company's Artificial Intelligence Group into a team of several thousand people.
Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL). Ng has also worked in the field of online education, cofounding Coursera and DeepLearning.AI. He has spearheaded many efforts to "democratize deep learning" teaching over 8 million students through his online courses. Ng is renowned globally in computer science, recognized in Time magazine's 100 Most Influential People in 2012 and Fast Company's Most Creative People in 2014. His influence extends to being named in the Time100 AI Most Influential People in 2023.
In 2018, he launched and currently heads the AI Fund, initially a $175-million investment fund for backing artificial intelligence startups. He has founded Landing AI, which provides AI-powered SaaS products.
On April 11, 2024, Amazon announced the appointment of Ng to its board of directors.

Biography
Ng was born in the United Kingdom, in 1976 to Ronald Paul Ng, a hematologist and Tisa Ho, an arts administrator, who were both immigrants from Hong Kong. He has at least one brother. In his youth, Ng lived in Hong Kong and Singapore. Ng attended and graduated from Raffles Institution.
In 1997, he earned his undergraduate degree with a triple major in computer science, statistics, and economics from Carnegie Mellon University in Pittsburgh, Pennsylvania. Between 1996 and 1998 he also conducted research on reinforcement learning, model selection, and feature selection at the AT&T Bell Labs.
In 1998, Ng earned his master's degree in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology (MIT) in Cambridge, Massachusetts. At MIT, he built the first publicly available, automatically indexed web-search engine for research papers on the web. It was a precursor to CiteSeerX/ResearchIndex, but specialized in machine learning.
In 2002, he received his Doctor of Philosophy (Ph.D.) in Computer Science from the University of California, Berkeley, under the supervision of Michael I. Jordan. His thesis is titled "Shaping and policy search in reinforcement learning" and is well-cited to this day.
He started working as an assistant professor at Stanford University in 2002 and as an associate professor in 2009.
He currently lives in Los Altos Hills, California. In 2014, he married Carol E. Reiley. They have two children: a daughter born in 2019 and a son born in 2021. The MIT Technology Review named Ng and Reiley an "AI power couple".

Career
Academia and teaching
Ng is a professor at Stanford University departments of Computer Science and electrical engineering. He served as the director of the Stanford Artificial Intelligence Laboratory (SAIL), where he taught students and undertook research related to data mining, big data, and machine learning. His machine learning course CS229 at Stanford is the most popular course offered on campus with over 1,000 students enrolling some years. As of 2020, three of most popular courses on Coursera are Ng's: Machine Learning (#1), AI for Everyone (#5), Neural Networks and Deep Learning (#6).
In 2008, his group at Stanford was one of the first in the US to start advocating the use of GPUs in deep learning. The rationale was that an efficient computation infrastructure could speed up statistical model training by orders of magnitude, ameliorating some of the scaling issues associated with big data. At the time it was a controversial and risky decision, but since then and following Ng's lead, GPUs have become a cornerstone in the field. Since 2017, Ng has been advocating the shift to high-performance computing (HPC) for scaling up deep learning and accelerating progress in the field.
In 2012, along with Stanford computer scientist Daphne Koller he cofounded and was CEO of Coursera, a website that offers free online courses to everyone. It took off with over 100,000 students registered for Ng's popular CS229A course. Today, several million people have enrolled in Coursera courses, making the site one of the leading massive open online courses (MOOCs) in the world.

Industry
From 2011 to 2012, he worked at Google, where he founded and directed the Google Brain Deep Learning Project with Jeff Dean, Greg Corrado, and Rajat Monga.
In 2014, he joined Baidu as chief scientist, and carried out research related to big data and AI. There he set up several research teams for things like facial recognition and Melody, an AI chatbot for healthcare. He also developed for the company the AI platform called DuerOS and other technologies that positioned Baidu ahead of Google in the discourse and development of AI. In March 2017, he announced his resignation from Baidu.
He soon afterward launched Deeplearning.AI, an online series of deep learning courses (including the AI for Good Specialization). Then Ng launched Landing AI, which provides AI-powered SaaS products.
In January 2018, Ng unveiled the AI Fund, raising $175 million to invest in new startups. In November 2021, Landing AI secured a $57 million round of series A funding led by McRock Capital, to help manufacturers adopt computer vision.

Research
Ng researches primarily in machine learning, deep learning, machine perception, computer vision, and natural language processing; and is one of the world's most famous and influential computer scientists. He's frequently won best paper awards at academic conferences and has had a huge impact on the field of AI, computer vision, and robotics.
During graduate school, together with David M. Blei and Michael I. Jordan, Ng co-authored the influential paper that introduced latent Dirichlet allocation (LDA) for his thesis on reinforcement learning for drones.
His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world. He was the leading scientist and principal investigator on the STAIR (Stanford Artificial Intelligence Robot) project, which resulted in Robot Operating System (ROS), a widely used open source software robotics platform. His vision to build an AI robot and put a robot in every home inspired Scott Hassan to back him and create Willow Garage. He is also one of the founding team members for the Stanford WordNet project, which uses machine learning to expand the Princeton WordNet database created by Christiane Fellbaum.
In 2011, Ng founded the Google Brain project at Google, which developed large-scale artificial neural networks using Google's distributed computing infrastructure. Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, which learned to recognize cats after watching only YouTube videos, and without ever having been told what a "cat" is. The project's technology is also currently used in the Android operating system's speech recognition system.

Online education: massive open online course
In 2011, Stanford launched a total of three massive open online course (MOOCs) on machine learning (CS229a), databases, and AI, taught by Ng, Peter Norvig, Sebastian Thrun, and Jennifer Widom. This has led to the modern MOOC movement. Ng taught machine learning and Widom taught databases. The course on AI taught by Thrun led to the genesis of Udacity. 
The seeds of massive open online courses (MOOCs) go back a few years before the founding of Coursera in 2012. Two themes emphasized in the founding of modern MOOCs were scale and availability.

By 2023, Ng has notably expanded access to AI education, with an estimated 8 million individuals worldwide taking his courses via platforms like DeepLearning.AI and Coursera.

Founding of Coursera
Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 published a number of Stanford courses online for free. Ng taught one of these courses, "Machine Learning", which includes his video lectures, along with the student materials used in the Stanford CS229 class. It offered a similar experience to MIT OpenCourseWare, except it aimed at providing a more "complete course" experience, equipped with lectures, course materials, problems and solutions, etc. The SEE videos were viewed by the millions and inspired Ng to develop and iterate new versions of online tech.
Within Stanford, they include Daphne Koller with her "blended learning experiences" and codesigning a peer-grading system, John Mitchell (Courseware, a Learning Management System), Dan Boneh (using machine learning to sync videos, later teaching cryptography on Coursera), Bernd Girod (ClassX), and others. Outside Stanford, Ng and Thrun credit Sal Khan of Khan Academy as a huge source of inspiration. Ng was also inspired by lynda.com and the design of the forums of Stack Overflow.
Widom, Ng, and others were ardent advocates of Khan-styled tablet recordings, and between 2009 and 2011, several hundred hours of lecture videos recorded by Stanford instructors were recorded and uploaded. Ng tested some of the original designs with a local high school to figure the best practices for recording lessons.
In October 2011, the "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and launched, with over 100,000 students registered for its first edition. The course featured quizzes and graded programming assignments and became one of the first and most successful massive open online courses (MOOCs) created by a Stanford professor.
Two other courses on databases (db-class.org) and AI (ai-class.org) were launched. The ml-class and db-class ran on a platform developed by students, including Frank Chen, Jiquan Ngiam, Chuan-Yu Foo, and Yifan Mai. Word spread through social media and popular press. The three courses were 10 weeks long, and over 40,000 "Statements of Accomplishment" were awarded.

Ng tells the following story on the early days of Coursera:In 2011, I was working with four Stanford students. We were under tremendous pressure to build new features for the 100,000+ students that were already signed up. One of the students (Frank Chen) claims another one (Jiquan Ngiam) frequently stranded him in the Stanford building and refused to give him a ride back to his dorm until very late at night so he had no choice but to stick around and keep working. I neither confirm nor deny this story. His work subsequently led to his founding of Coursera with Koller in 2012. As of 2019, the two most popular courses on the platform were taught and designed by Ng: "Machine Learning" (#1) and "Neural Networks and Deep Learning" (#2).

Post-Coursera work
In 2019, Ng launched a new course "AI for Everyone". This is a non-technical course designed to help people understand AI's impact on society and its benefits and costs for companies, as well as how they can navigate through this technological revolution.

Venture capital
Ng is the chair of the board for Woebot Labs, a psychological clinic that uses data science to provide cognitive behavioral therapy. It provides a therapy chatbot to help treat depression, among other things.
He is also a member of the board of directors for drive.ai, which uses AI for self-driving cars and was acquired by Apple in 2019.
Through Landing AI, he also focuses on democratizing AI technology and lowering the barrier for entrance to businesses and developers.

Publications and awards
Ng is also the author or co-author of over 300 publications in robotics, and related fields. His work in computer vision and deep learning has been featured often in press releases and reviews.

1995. Bell Atlantic Network Services Scholarship
1995, 1996. Microsoft Technical Scholarship Award
1996. Andrew Carnegie Society Scholarship
1998–2000: Berkeley Fellowship
2001–2002: Microsoft Research Fellowship
2007. Alfred P. Sloan Research Fellowship Sloan Foundation Faculty Fellowship
2008. Massachusetts Institute of Technology (MIT) Technology Review, 35 Innovators Under 35 (TR35)
2009. IJCAI Computers and Thought Award (the highest award in AI given to a researcher under 35)
2009. Vance D. & Arlene C. Coffman Faculty Scholar Award
2013. Time 100 Most Influential People
2013. Fortune's 40 under 40 
2013. CNN 10: Thinkers
2014. Fast Company's Most Creative People in Business
2015. World Economic Forum Young Global Leaders
2023. Time AI 100 Most Influential People
2024 Honorary Fellowship of the Royal Statistical Society.

He has corefereed hundreds of AI publications in journals like NeurIPS. He has also been the editor of the Journal of Artificial Intelligence Research (JAIR), Associate Editor for the IEEE Robotics and Automation Society Conference Editorial Board (ICRA), and much more.
He has given invited talks at NASA, Google, Microsoft, Lockheed Martin, the Max Planck Society, Stanford, Princeton, UPenn, Cornell, MIT, UC Berkeley, and dozens of other universities. Outside of the US, he has lectured in Spain, Germany, Israel, China, Korea, and Canada.
He has also written for Harvard Business Review, HuffPost, Slate, Apple News, and Quora Sessions' Twitter. He also writes a weekly digital newsletter called The Batch.

Books
He also wrote a book Machine Learning Yearning, a practical guide for those interested in machine learning, which he distributed for free. In December 2018, he wrote a sequel called AI Transformation Playbook.
Ng contributed one chapter to Architects of Intelligence: The Truth About AI from the People Building it (2018) by the American futurist Martin Ford.

Views on AI
A real threat is regarding the future of work: "Rather than being distracted by evil killer robots, the challenge to labor caused by these machines is a conversation that academia and industry and government should have." A particular goal of Ng's work is to "democratize" AI learning so that people can learn more about it and understand its benefits.
In a December 2023 Financial Times interview, Ng highlighted concerns regarding the impact of potential regulations on open-source AI, emphasizing how reporting, licensing, and liability risks could unfairly burden smaller firms and stifle innovation. He argued that regulating basic technologies like open-source models could hinder progress without markedly enhancing safety. Ng advocated for carefully designed regulations to prevent obstacles to the development and distribution of beneficial AI technologies.

See also
Robot Operating System
Latent Dirichlet allocation
Google Brain
Coursera

References
External links

Official website
Ng's Quora profile
Ng's Medium blog
Academic Genealogy
Andrew Ng's Publication List
Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC. 
In June 2013, the private equity firm Peterson Partners acquired Angoss for $8.4 million.

Software
KnowledgeREADER is an integrated customer intelligence product combining visual text discovery and predictive analytics for customer experience management.
KnowledgeSEEKER is a data mining product. Its features include data profiling, data visualization and decision tree analysis. It was first released in 1990.
KnowledgeSTUDIO is a data mining and predictive analytics suite for the model development and deployment cycle. Its features include data profiling, data visualization, decision tree analysis, predictive modeling, implementation, scoring, validation, monitoring and scorecard development.
KnowledgeEXCELERATOR is a visual data discovery software and prediction tool for business analysts and knowledge workers.
StrategyBUILDER is an add-on module for KnowledgeSEEKER and KnowledgeSTUDIO and is a product to design, verify, and deploy predictive and business rules.

Services
FundGUARD is software as a service for marketing, sales targeting and predictive leads for mutual funds and wealth management companies.
ClaimGUARD is a fraud and abuse detection service.
Cloud on demand Software is offered for KnowledgeSEEKER, KnowledgeSTUDIO and its text analytics module.
KnowledgeSCORE for Salesforce.com customer relationship management is a forecasting and predictive sales analytics system for Salesforce users.

See also
List of statistical packages
Predictive analytics

See also
FICO

References
External links
Official website
In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.
Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not, the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.

Definition
Many attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include the following, and can be categorised into three groups: those that are ambiguous, those that are specific to a method with pre-defined thresholds usually chosen empirically, and those that are formally defined:

Ill defined
An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.
Anomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.
An outlier is an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.
An anomaly is a point or collection of points that is relatively distant from other points in multi-dimensional space of features.
Anomalies are patterns in data that do not conform to a well-defined notion of normal behaviour.

Specific
Let T be observations from a univariate Gaussian distribution and O a point from T. Then the z-score for O is greater than a pre-selected threshold if and only if O is an outlier.

Definition of anomalies in high-dimensional context
In this big data era, the focus is increasingly on methodologies capable of handling the complexity and scale of data, going beyond traditional approaches to define and detect anomalies in a way that is both effective and efficient for today's data-driven decision-making processes.

Anomalies in high-dimensional spaces are more challenging to identify due to the sparsity of the data and the relative distance between points becoming less meaningful.
Traditional threshold-based methods become less effective as dimensionality increases, often requiring more sophisticated, multidimensional analysis techniques.
High dimensional anomaly detection often requires careful consideration of the feature selection to reduce dimensionality and enhance the sensitivity to true anomalies.

History
Intrusion detection
The concept of intrusion detection, a critical component of anomaly detection, has evolved significantly over time. Initially, it was a manual process where system administrators would monitor for unusual activities, such as a vacationing user's account being accessed or unexpected printer activity. This approach was not scalable and was soon superseded by the analysis of audit logs and system logs for signs of malicious behavior.
By the late 1970s and early 1980s, the analysis of these logs was primarily used retrospectively to investigate incidents, as the volume of data made it impractical for real-time monitoring. The affordability of digital storage eventually led to audit logs being analyzed online, with specialized programs being developed to sift through the data. These programs, however, were typically run during off-peak hours due to their computational intensity.
The 1990s brought the advent of real-time intrusion detection systems capable of analyzing audit data as it was generated, allowing for immediate detection of and response to attacks. This marked a significant shift towards proactive intrusion detection.
As the field has continued to develop, the focus has shifted to creating solutions that can be efficiently implemented across large and complex network environments, adapting to the ever-growing variety of security threats and the dynamic nature of modern computing infrastructures.

Applications
Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security, intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.

Intrusion detection
Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of features proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.  The counterpart of anomaly detection in intrusion detection is misuse detection.

Fintech fraud detection
Anomaly detection is vital in fintech for fraud prevention.

Preprocessing
Preprocessing data to remove anomalies can be an important step in data analysis, and is done for a number of reasons. Statistics such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.

Video surveillance
Anomaly detection has become increasingly vital in video surveillance to enhance security and safety. With the advent of deep learning technologies, methods using Convolutional Neural Networks (CNNs) and Simple Recurrent Units (SRUs) have shown significant promise in identifying unusual activities or behaviors in video data. These models can process and analyze extensive video feeds in real-time, recognizing patterns that deviate from the norm, which may indicate potential security threats or safety violations.

IT infrastructure
In IT infrastructure management, anomaly detection is crucial for ensuring the smooth operation and reliability of services. Techniques like the IT Infrastructure Library (ITIL) and monitoring frameworks are employed to track and manage system performance and user experience. Detection anomalies can help identify and pre-empt potential performance degradations or system failures, thus maintaining productivity and business process effectiveness.

IoT systems
Anomaly detection is critical for the security and efficiency of Internet of Things (IoT) systems. It helps in identifying system failures and security breaches in complex networks of IoT devices. The methods must manage real-time data, diverse device types, and scale effectively. Garbe et al. have introduced a multi-stage anomaly detection framework that improves upon traditional methods by incorporating spatial clustering, density-based clustering, and locality-sensitive hashing. This tailored approach is designed to better handle the vast and varied nature of IoT data, thereby enhancing security and operational reliability in smart infrastructure and industrial IoT systems.

Petroleum industry
Anomaly detection is crucial in the petroleum industry for monitoring critical machinery. Martí et al. used a novel segmentation algorithm to analyze sensor data for real-time anomaly detection. This approach helps promptly identify and address any irregularities in sensor readings, ensuring the reliability and safety of petroleum operations.

Oil and gas pipeline monitoring
In the oil and gas sector, anomaly detection is not just crucial for maintenance and safety, but also for environmental protection. Aljameel et al. propose an advanced machine learning-based model for detecting minor leaks in oil and gas pipelines, a task traditional methods may miss.

Methods
Many anomaly detection techniques have been proposed in literature. The performance of methods usually depend on the data sets. For example, some may be suited to detecting local outliers, while others global, and methods have little systematic advantages over another when compared across many data sets. Almost all algorithms also require the setting of non-intuitive parameters critical for performance, and usually unknown before application. Some of the popular techniques are mentioned below and are broken down into categories:

Statistical
Parameter-free
Parametric-based
Z-score,
Tukey's range test
Grubbs's test

Density
Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)
Subspace-base (SOD), correlation-based (COP) and tensor-based outlier detection for high-dimensional data
One-class support vector machines (OCSVM, SVDD)

Neural networks
Replicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks
Bayesian networks
Hidden Markov models (HMMs)
Minimum Covariance Determinant
Deep Learning
Convolutional Neural Networks (CNNs): CNNs have shown exceptional performance in the unsupervised learning domain for anomaly detection, especially in image and video data analysis. Their ability to automatically and hierarchically learn spatial hierarchies of features from low to high-level patterns makes them particularly suited for detecting visual anomalies. For instance, CNNs can be trained on image datasets to identify atypical patterns indicative of defects or out-of-norm conditions in industrial quality control scenarios.
Simple Recurrent Units (SRUs): In time-series data, SRUs, a type of recurrent neural network, have been effectively used for anomaly detection by capturing temporal dependencies and sequence anomalies. Unlike traditional RNNs, SRUs are designed to be faster and more parallelizable, offering a better fit for real-time anomaly detection in complex systems such as dynamic financial markets or predictive maintenance in machinery, where identifying temporal irregularities promptly is crucial.

Cluster-based
Clustering: Cluster analysis-based outlier detection
Deviations from association rules and frequent itemsets
Fuzzy logic-based outlier detection

Ensembles
Ensemble techniques, using feature bagging, score normalization and different sources of diversity

Others
Histogram-based Outlier Score (HBOS) uses value histograms and assumes feature independence for fast predictions.

Anomaly detection in dynamic networks
Dynamic networks, such as those representing financial systems, social media interactions, and transportation infrastructure, are subject to constant change, making anomaly detection within them a complex task. Unlike static graphs, dynamic networks reflect evolving relationships and states, requiring adaptive techniques for anomaly detection.

Types of anomalies in dynamic networks
Community anomalies
Compression anomalies
Decomposition anomalies
Distance anomalies
Probabilistic model anomalies

Explainable anomaly detection
Many of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:

The Subspace Outlier Degree (SOD) identifies attributes where a sample is normal, and attributes in which the sample deviates from the expected.
Correlation Outlier Probabilities (COP) compute an error vector of how a sample point deviates from an expected location, which can be interpreted as a counterfactual explanation: the sample would be normal if it were moved to that location.

Software
ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.
PyOD is an open-source Python library developed specifically for anomaly detection.
scikit-learn is an open-source Python library that contains some algorithms for unsupervised anomaly detection.
Wolfram Mathematica provides functionality for unsupervised anomaly detection across multiple data types

Datasets
Anomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-Universität München; Mirror Archived 2022-03-31 at the Wayback Machine at University of São Paulo.
ODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.
Unsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.
KMASH Data Repository  at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth.

See also
Change detection
Statistical process control
Novelty detection
Hierarchical temporal memory


== References ==
Anthropic PBC is a U.S.-based artificial intelligence (AI) startup public-benefit company, founded in 2021. It researches and develops AI to "study their safety properties at the technological frontier" and use this research to deploy safe, reliable models for the public. Anthropic has developed a family of large language models (LLMs) named Claude as a competitor to OpenAI's ChatGPT and Google's Gemini.
Anthropic was founded by former members of OpenAI, siblings Daniela Amodei and Dario Amodei. In September 2023, Amazon announced an investment of up to $4 billion, followed by a $2 billion commitment from Google in the following month.

History
Anthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom served as OpenAI's Vice President of Research.
In April of 2022, Anthropic announced it had received $580 million in funding, with $500 million of this funding coming from FTX under the leadership of Sam Bankman-Fried.
In the summer of 2022, Anthropic finished training the first version of Claude but did not release it, mentioning the need for further internal safety testing and the desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.  
In February 2023, Anthropic was sued by Texas-based Anthrop LLC for the use of its registered trademark "Anthropic A.I." On September 25, 2023, Amazon announced a partnership with Anthropic, with Amazon becoming a minority stakeholder by initially investing $1.25 billion, and planning a total investment of $4 billion. As part of the deal, Anthropic would use Amazon Web Services (AWS) as its primary cloud provider and make its AI models available to AWS customers. The next month, Google invested $500 million in Anthropic, and committed to an additional $1.5 billion over time.
In March 2024, Amazon maxed out its potential investment from the agreement made in the prior year by investing another US $2.75 billion into Anthropic, completing its $4 billion investment.

Participants
Key employees
Dario Amodei: Co-Founder and Chief Executive Officer
Daniela Amodei: Co-Founder and President
Jason Clinton: Chief Information Security Officer
Jared Kaplan: Co-Founder and Chief Science Officer
Ben Mann: Co-Founder and Member of Technical Staff
Jack Clark: Co-Founder and Head of Policy
Mike Krieger: Chief Product Officer
Jan Leike: ex-OpenAI alignment researcher

Board of Directors
Dario Amodei: Chief Executive Officer
Daniela Amodei: representative of common shareholders
Luke Muehlhauser: representative of Series A shareholders
Yasmin Razavi: representative of Series C shareholders

Investors
Amazon.com – $4B
Google – $2B
Menlo Ventures – $750M
Wisdom Ventures
Ripple Impact Investments
Factorial Funds

Motives
According to Anthropic, the company's goal is to research the safety and reliability of artificial intelligence systems. The Amodei siblings were among those who left OpenAI due to directional differences, specifically regarding OpenAI's ventures with Microsoft in 2019. Anthropic incorporated itself as a Delaware public-benefit corporation (PBC), which requires the company to maintain a balance between private and public interests.
Anthropic is a corporate "Long-Term Benefit Trust", a company-derived entity that requires the company's directors to align the company's priorities with the public benefit rather than profit in "extreme" instances of "catastrophic risk". As of September 19, 2023, members of the Trust included Jason Matheny (CEO and President of the RAND Corporation), Kanika Bahl (CEO and President of Evidence Action), Neil Buddy Shah (CEO of the Clinton Health Access Initiative), Paul Christiano (Founder of the Alignment Research Center), and Zach Robinson (CEO of Effective Ventures US).

Projects
Claude
Claude incorporates "Constitutional AI" to set safety guidelines for the model's output. The name, "Claude", was chosen either as a reference to mathematician Claude Shannon, or as a male name to contrast the female names of other A.I. assistants such as Alexa, Siri, and Cortana.
Anthropic initially released two versions of its model, Claude and Claude Instant, in March 2023, with the latter being a more lightweight model. The next iteration, Claude 2, was launched in July 2023. Unlike Claude, which was only available to select users, Claude 2 is available for public use.
Claude 3 was released on March 4, 2024, unveiling three language models: Opus, Sonnet, and Haiku. The Opus model is the largest and most capable—according to Anthropic, it outperforms the leading models from OpenAI (GPT-4, GPT-3.5) and Google (Gemini Ultra). Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models can accept image input. Amazon has incorporated Claude 3 into Bedrock, an Amazon Web Services-based platform for cloud AI services.
On May 1, 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.
On June 20, 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in areas such as coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability in which Claude was able to create code in a dedicated window in the interface and preview select code in real time such as websites or SVGs.

Constitutional AI
According to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest. Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the "constitution". The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution. The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.
Some of the principles of Claude 2's constitution are derived from documents such as the 1948 Universal Declaration of Human Rights and Apple's terms of service. For example, one rule from the UN Declaration applied in Claude 2's CAI states "Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood."

Interpretability research
Anthropic also publishes research on the interpretability of machine learning systems, focusing on the transformer architecture.
Part of Anthropic's research aims to be able to automatically identify "features" in generative pretrained transformers like Claude. In a neural network, a feature is a pattern of neural activations that corresponds to a concept. Using a compute-intensive technique called "dictionary learning", Anthropic was able to identify millions of features in Claude, including for example one associated with the Golden Gate Bridge. Enhancing the ability to identify and edit features is expected to have significant safety implications.

Lawsuit
On October 18, 2023, Anthropic was sued by Concord, Universal, ABKCO, and other music publishers for, per the complaint, "systematic and widespread infringement of their copyrighted song lyrics." They alleged that the company used copyrighted material without permission in the form of song lyrics. The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws. In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as Katy Perry's "Roar" and Gloria Gaynor's "I Will Survive". Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.
On January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.

See also
Apprenticeship learning
AI alignment
Friendly AI
OpenAI

References
External links
Official website
Anthropic on Twitter
Anthropic on Instagram
Anthropic on YouTube
Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark. Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.

Features
Samsara
Apache Mahout-Samsara refers to a Scala domain specific language (DSL) that allows users to use R-Like syntax as opposed to traditional Scala-like syntax. This allows user to express algorithms concisely and clearly.

Backend Agnostic
Apache Mahout's code abstracts the domain specific language from the engine where the code is run. While active development is done with the Apache Spark engine, users are free to implement any engine they choose- H2O and Apache Flink have been implemented in the past and examples exist in the code base.

GPU/CPU accelerators
The JVM has notoriously slow computation. To improve speed, “native solvers” were added which move in-core, and by extension, distributed BLAS operations out of the JVM, offloading to off-heap or GPU memory for processing via multiple CPUs and/or CPU cores, or GPUs when built against the ViennaCL library. ViennaCL is a highly optimized C++ library with BLAS operations implemented in OpenMP, and OpenCL. As of release 14.1, the OpenMP build considered to be stable, leaving the OpenCL build is still in its experimental POC phase.

Recommenders
Apache Mahout features implementations of Alternating Least Squares, Co-Occurrence, and Correlated Co-Occurrence, a unique-to-Mahout recommender algorithm that extends co-occurrence to be used on multiple dimensions of data.

History
Transition from Map Reduce to Apache Spark
While Mahout's core algorithms for clustering, classification and batch based collaborative filtering were implemented on top of Apache Hadoop using the map/reduce paradigm, it did not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster were also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.
Starting with the release 0.10.0, the project shifted its focus to building a backend-independent programming environment, code named "Samsara". The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. Supported algebraic platforms are Apache Spark, H2O, and Apache Flink. Support for MapReduce algorithms started being gradually phased out in 2014.

Release History
Developers
Apache Mahout is developed by a community. The project is managed by a group called the "Project Management Committee" (PMC). The current PMC is Andrew Musselman,  Andrew Palumbo,  Drew Farris,  Isabel Drost-Fromm,  Jake Mannix,  Pat Ferrel,  Paritosh Ranjan,  Trevor Grant,  Robin Anil,  Sebastian Schelter,  Stevo Slavić.

References
External links
Official website
Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.

Overview
Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.
Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.
Inside Apache Spark the workflow is managed as a directed acyclic graph (DAG). Nodes represent RDDs while edges represent the operations on the RDDs.
Spark facilitates the implementation of both iterative algorithms, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by several orders of magnitude compared to Apache Hadoop MapReduce implementation.
Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.
Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone native Spark, Hadoop YARN, Apache Mesos or Kubernetes. A standalone native Spark cluster can be launched manually or by the launch scripts provided by the install package.  It is also possible to run the daemons on a single machine for testing.  For distributed storage Spark can interface with a wide variety of distributed systems, including Alluxio, Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, Lustre file system, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.

Spark Core
Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, .NET and R) centered on the RDD abstraction (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages that can connect to the JVM, such as Julia). This interface mirrors a functional/higher-order model of programming: a "driver" program invokes parallel operations such as map, filter or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster. These operations, and additional ones such as joins, take RDDs as input and produce new RDDs. RDDs are immutable and their operations are lazy; fault-tolerance is achieved by keeping track of the "lineage" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, .NET, Java, or Scala objects.
Besides the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: broadcast variables reference read-only data that needs to be available on all nodes, while accumulators can be used to program reductions in an imperative style.
A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each map, flatMap (a variant of map) and reduceByKey takes an anonymous function that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.

Spark SQL
Spark SQL is a component on top of Spark Core that introduced a data abstraction called DataFrames, which provides support for structured and semi-structured data. Spark SQL provides a domain-specific language (DSL) to manipulate DataFrames in Scala, Java, Python or .NET. It also provides SQL language support, with command-line interfaces and ODBC/JDBC server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.

Spark Streaming
Spark Streaming uses Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of lambda architecture. However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include Storm and the streaming component of Flink. Spark Streaming has support built-in to consume from Kafka, Flume, Twitter, ZeroMQ, Kinesis, and TCP/IP sockets.
In Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.
Spark can be deployed in a traditional on-premises data center as well as in the cloud.

MLlib Machine Learning Library
Spark MLlib is a distributed machine-learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by Apache Mahout (according to benchmarks done by the MLlib developers against the alternating least squares (ALS) implementations, and before Mahout itself gained a Spark interface), and scales better than Vowpal Wabbit. Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines, including:

summary statistics, correlations, stratified sampling, hypothesis testing, random data generation
classification and regression: support vector machines, logistic regression, linear regression, naive Bayes classification, Decision Tree, Random Forest, Gradient-Boosted Tree
collaborative filtering techniques including alternating least squares (ALS)
cluster analysis methods including k-means, and latent Dirichlet allocation (LDA)
dimensionality reduction techniques such as singular value decomposition (SVD), and principal component analysis (PCA)
feature extraction and transformation functions
optimization algorithms such as stochastic gradient descent, limited-memory BFGS (L-BFGS)

GraphX
GraphX is a distributed graph-processing framework on top of Apache Spark. Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a graph database. GraphX provides two separate APIs for implementation of massively parallel algorithms (such as PageRank): a Pregel abstraction, and a more general MapReduce-style API. Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).
Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.

Language support
Apache Spark has built-in support for Scala, Java, SQL, R, and Python with 3rd party support for the .NET CLR, Julia, and more.

History
Spark was initially started by Matei Zaharia at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a BSD license.
In 2013, the project was donated to the Apache Software Foundation and switched its license to Apache 2.0. In February 2014, Spark became a Top-Level Apache Project.
In November 2014, Spark founder M. Zaharia's company Databricks set a new world record in large scale sorting using Spark.
Spark had in excess of 1000 contributors in 2015, making it one of the most active projects in the Apache Software Foundation and one of the most active open source big data projects.

Scala Version
Spark 3.5.2 is based on Scala 2.13 (and thus works with Scala 2.12 and 2.13 out-of-the-box), but it can also be made to work with Scala 3.

Developers
Apache Spark is developed by a community. The project is managed by a group called the "Project Management Committee" (PMC).

Maintenance releases and EOL
Feature release branches will, generally, be maintained with bug fix releases for a period of 18 months. For example, branch 2.3.x is no longer considered maintained as of September 2019, 18 months after the release of 2.3.0 in February 2018. No more 2.3.x releases should be expected after that point, even for bug fixes.
The last minor release within a major a release will typically be maintained for longer as an “LTS” release. For example, 2.4.0 was released on November 2, 2018, and had been maintained for 31 months until 2.4.8 was released in May 2021. 2.4.8 is the last release and no more 2.4.x releases should be expected even for bug fixes.

See also
List of concurrent and parallel programming APIs/Frameworks

Notes
References
External links
Official website
Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. 
SystemDS's distinguishing characteristics are:

Algorithm customizability via R-like and Python-like languages.
Multiple execution modes, including Standalone, Spark Batch, Spark MLContext, Hadoop Batch, and JMLC.
Automatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.

History
SystemML was created in 2010 by researchers at the IBM Almaden Research Center led by IBM Fellow Shivakumar Vaithyanathan. It was observed that data scientists would write machine learning algorithms in languages such as R and Python for small data. When it came time to scale to big data, a systems programmer would be needed to scale the algorithm in a language such as Scala. This process typically involved days or weeks per iteration, and errors would occur translating the algorithms to operate on big data. SystemML seeks to simplify this process. A primary goal of SystemML is to automatically scale an algorithm written in an R-like or Python-like language to operate on big data, generating the same answer without the error-prone, multi-iterative translation approach.
On June 15, 2015, at the Spark Summit in San Francisco, Beth Smith, General Manager of IBM Analytics, announced that IBM was open-sourcing SystemML as part of IBM's major commitment to Apache Spark and Spark-related projects. SystemML became publicly available on GitHub on August 27, 2015 and became an Apache Incubator project on November 2, 2015. On May 17, 2017, the Apache Software Foundation Board approved the graduation of Apache SystemML as an Apache Top Level Project.

Key technologies
The following are some of the technologies built into the SystemDS engine.

Compressed Linear Algebra for Large Scale Machine Learning
Declarative Machine Learning Language

Examples
Principal Component Analysis
The following code snippet does the Principal component analysis of input matrix 
  
    
      
        A
      
    
    {\displaystyle A}
  
 , which returns the 
  
    
      
        e
        i
        g
        e
        n
        v
        e
        c
        t
        o
        r
        s
      
    
    {\displaystyle eigenvectors}
  
 and the 
  
    
      
        e
        i
        g
        e
        n
        v
        a
        l
        u
        e
        s
      
    
    {\textstyle eigenvalues}
  
.

Invocation script
Database functions
DBSCAN clustering algorithm with Euclidean distance.

Improvements
SystemDS 2.0.0 is the first major release under the new name. This release contains a major refactoring, a few major features, a large number of improvements and fixes, and some experimental features to better support the end-to-end data science lifecycle. In addition to that, this release also removes several features that are not up date and outdated.

New mechanism for DML-bodied (script-level) builtin functions, and a wealth of new built-in functions for data preprocessing including data cleaning, augmentation and feature engineering techniques, new ML algorithms, and model debugging.
Several methods for data cleaning have been implemented including multiple imputations with multivariate imputation by chained equations (MICE) and other techniques, SMOTE, an oversampling technique for class imbalance, forward and backward NA filling, cleaning using schema and length information, support for outlier detection using standard deviation and inter-quartile range, and functional dependency discovery.
A complete framework for lineage tracing and reuse including support for loop deduplication, full and partial reuse, compiler assisted reuse, several new rewrites to facilitate reuse.
New federated runtime backend including support for federated matrices and frames, federated builtins (transform-encode, decode etc.).
Refactor compression package and add functionalities including quantization for lossy compression, binary cell operations, left matrix multiplication. [experimental]
New python bindings with supports for several builtins, matrix operations, federated tensors and lineage traces.
Cuda implementation of cumulative aggregate operators (cumsum, cumprod etc.)
New model debugging technique with slice finder.
New tensor data model (basic tensors of different value types, data tensors with schema) [experimental]
Cloud deployment scripts for AWS and scripts to set up and start federated operations.
Performance improvements with parallel sort, gpu cum agg, append cbind etc.
Various compiler and runtime improvements including new and improved rewrites, reduced Spark context creation, new eval framework, list operations, updated native kernel libraries to name a few.
New data reader/writer for json frames and support for sql as a data source.
Miscellaneous improvements: improved documentation, better testing, run/release scripts, improved packaging, Docker container for systemds, support for lambda expressions, bug fixes.
Removed MapReduce compiler and runtime backend, pydml parser, Java-UDF framework, script-level debugger.
Deprecated ./scripts/algorithms, as those algorithms gradually will be part of SystemDS builtins.

Contributions
Apache SystemDS welcomes contributions in code, question and answer, community building, or spreading the word. The contributor guide is available at https://github.com/apache/systemds/blob/main/CONTRIBUTING.md

See also
Comparison of deep learning software

References
External links
Apache SystemML website
IBM Research - SystemML
Q & A with Shiv Vaithyanathan, Creator of SystemML and IBM Fellow
A Universal Translator for Big Data and Machine Learning
SystemML: Declarative Machine Learning at Scale presentation by Fred Reiss
SystemML: Declarative Machine Learning on MapReduce Archived 2016-03-10 at the Wayback Machine
Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML
SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs
IBM's SystemML machine learning system becomes Apache Incubator project
IBM donates machine learning tech to Apache Spark open source community
IBM's SystemML Moves Forward as Apache Incubator Project
Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.
Web application security is a branch of information security that deals specifically with the security of websites, web applications, and web services. At a high level, web application security draws on the principles of application security but applies them specifically to the internet and web systems. The application security also concentrates on mobile apps and their security which includes iOS and Android Applications
Web Application Security Tools are specialized tools for working with HTTP traffic, e.g., Web application firewalls.

Approaches
Different approaches will find different subsets of the security vulnerabilities lurking in an application and are most effective at different times in the software lifecycle. They each represent different tradeoffs of time, effort, cost and vulnerabilities found.

Design review. Before code is written the application's architecture and design can be reviewed for security problems. A common technique in this phase is the creation of a threat model.
Whitebox security review, or code review. This is a security engineer deeply understanding the application through manually reviewing the source code and noticing security flaws. Through comprehension of the application, vulnerabilities unique to the application can be found.
Blackbox security audit. This is only through the use of an application testing it for security vulnerabilities, no source code is required.
Automated Tooling. Many security tools can be automated through inclusion into the development or testing environment. Examples of those are automated DAST/SAST tools that are integrated into code editor or CI/CD platforms.
Coordinated vulnerability platforms. These are hacker-powered application security solutions offered by many websites and software developers by which individuals can receive recognition and compensation for reporting bugs.

Security threats
The Open Worldwide Application Security Project (OWASP) provides free and open resources. It is led by a non-profit called The OWASP Foundation. The OWASP Top 10 - 2017 results from recent research based on comprehensive data compiled from over 40 partner organizations. This data revealed approximately 2.3 million vulnerabilities across over 50,000 applications. According to the OWASP Top 10 - 2021, the ten most critical web application security risks include:

Broken access control
Cryptographic Failures
Injection
Insecure Design
Security Misconfiguration
Vulnerable and Outdated Components
Identification and Authentification Failures
Software and Data Integrity Failures
Security Logging and Monitoring Failures*
Server-Side Request Forgery (SSRF)*

Security Controls
The OWASP Top 10 Proactive Controls 2024 is a list of security techniques every software architect and developer should know and heed.
The current list contains:

Implement Access Control
Use Cryptography the proper way
Validate all Input & Handle Exceptions
Address Security from the Start
Secure by Default Configurations
Keep your Components Secure
Implement Digital Identity
Leverage Browser Security Features
Implement Security Logging and Monitoring
Stop Server Side Request Forgery

Tooling for security testing
Security testing techniques scour for vulnerabilities or security holes in applications. These vulnerabilities leave applications open to exploitation. Ideally, security testing is implemented throughout the entire Software Development Life Cycle (SDLC) so that vulnerabilities may be addressed in a timely and thorough manner.
There are many kinds of automated tools for identifying vulnerabilities in applications. Common tool categories used for identifying application vulnerabilities include:

Static Application Security Testing (SAST) analyzes source code for security vulnerabilities during an application's development. Compared to DAST, SAST can be utilized even before the application is in an executable state. As SAST has access to the full source code it is a white-box approach. This can yield more detailed results but can result in many false positives that need to be manually verified.
Dynamic Application Security Testing (DAST, often called Vulnerability scanners) automatically detects vulnerabilities by crawling and analyzing websites. This method is highly scalable, easily integrated and quick. DAST tools are well suited for dealing with low-level attacks such as injection flaws but are not well suited to detect high-level flaws, e.g., logic or business logic flaws. Fuzzing tools are commonly used for input testing.
Interactive Application Security Testing (IAST) assesses applications from within using software instrumentation. This combines the strengths of both SAST and DAST methods as well as providing access to code, HTTP traffic, library information, backend connections and configuration information. Some IAST products require the application to be attacked, while others can be used during normal quality assurance testing.
Runtime application self-protection augments existing applications to provide intrusion detection and prevention from within an application runtime.
Dependency scanners (also called Software Composition Analysis) try to detect the usage of software components with known vulnerabilities. These tools can either work on-demand, e.g., during the source code build process, or periodically.

Security standards and regulations
CERT Secure Coding
ISO/IEC 27034-1:2011 Information technology — Security techniques — Application security -- Part 1: Overview and concepts
ISO/IEC TR 24772:2013 Information technology — Programming languages — Guidance to avoiding vulnerabilities in programming languages through language selection and use
NIST Special Publication 800-53
OWASP ASVS: Web Application Security Verification Standard

See also
Common Weakness Enumeration
Data security
Mobile security
OWASP
Microsoft Security Development Lifecycle
Usable security


== References ==
Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology that has numerous applications, including language translation, image recognition, decision-making, credit scoring and e-commerce. AI includes the development of machines which can perceive, understand, act and learn a scientific discipline.

Internet and e-commerce
Recommendation systems
A recommendation system predicts the rating or preference a user would give to an item. Artificial intelligence recommendation systems are designed to offer suggestions based on previous behavior. These systems have been used by companies such as Netflix, Amazon, Instagram and YouTube, where they generate personalized playlists, product suggestions, and video recommendations.

Web feeds and posts
Machine learning is also used in web feeds such as for determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.

Targeted advertising and increasing internet engagement
AI is used to target web advertisements to those most likely to click or engage in them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints. Both AdSense and Facebook use AI for advertising. Online gambling companies use AI to improve customer targeting.
Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.

Virtual assistants
Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.

Search engines
Bing Chat has used artificial intelligence as part of its search engine.

Spam filtering
Machine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements. Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails. These models can be refined from new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types.

Language translation
Speech translation technology attempts to convert one language's spoken words into another. This potentially reduces language barriers in global commerce and cross-cultural exchange by allowing speakers of various languages to communicate with one another. 
AI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator. Additionally, research and development are in progress to decode and conduct animal communication.
Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.

Facial recognition and image labeling
AI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.
Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.  Facebook's DeepFace identifies human faces in digital images.

Games
Games have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.

Economic and social challenges
AI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.

Agriculture
In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.

Precision Farming
AI helps in achieving precise farming, which calls for the use of algorithims to analyze data retrieved from satellite imagery and on-site field sensors. It allows for optimization of resource usage and helps to make the right decisions regarding the kind of nutrients, water, and pesticides required to maximize yield.

Crop and soil monitoring
Using machine learning models to monitor the health of crops and the soil. The models will be able to detect and predict diseases and pests in crops ahead of time to allow timely interventions.

Automated Machinery
There are automated machinery such as tractors and harvesters, which can operate autonomously with minimal human labor. With the use of AI many duties in the area are possible to be done with precision.

Cyber security
Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.
Applications of AI in cyber security include:

Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.
Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.
AI-related cyber security application cases vary in both benefit and complexity. Security features such as Security Orchestration, Automation, and Response (SOAR) and Extended Endpoint Detection and Response (XDR) offer significant benefits for businesses, but require significant integration and adaptation efforts.
Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.
AI technology can also be utilized to improve system security and safeguard our privacy. Randrianasolo (2012) suggested a security system based on artificial intelligence that can recognize intrusions and adapt to perform better. In order to improve cloud computing security, Sahil (2015) created a user profile system for the cloud environment with AI techniques.
Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.
Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.

Education
AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called "Inclusive and Equitable Quality Education.” 
The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.
Personalized Learning
AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.
These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content to suit each student's pace and style of learning.
Administrative Efficiency
In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.
Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind. 
Ethical and Privacy Concerns
Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data. 
It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.
Much regulation will be influenced by the AI Act, the world’s first comprehensive AI law.

Finance
Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.
Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.
The use of AI in applications such as online trading and decision-making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient. The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.

Trading and investment
Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.
Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.

Underwriting
Online lender Upstart uses machine learning for underwriting.
ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.

Audit
AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.
Continuous auditing with AI allows a real-time monitoring and reporting of financial activities and providing businesses with timely insights that can lead to quick decision making.

Anti-money laundering
AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to "develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability". A study about deep learning for AML identified "key challenges for researchers" to have "access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced" and suggests future research should bring-out "explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data".
Banks use machine learning (ML) to upgrade process monitoring and demonstrating the ability of  responding efficiently to evolving techniques.
Through ML and other methods, financial organizations can detect laundering operations and run compliance in an automated and very fast mode.

History
In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. "The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism."
One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.
In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.
AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.

Government
AI facial recognition systems are used for mass surveillance, notably in China. In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.

Military
Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.
In 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.
In the 2023 Israel–Hamas war, Israel used two AI systems to generate targets to strike: Habsora (translated: "the gospel") was used to compile a list of buildings to target, while "Lavender" produced a list of people. "Lavender" produced a list of 37,000 people to target. The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the "AI target bank." An internal source describes the process as a “mass assassination factory”.
In 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.
In 2024 a Chinese laboratory at the Joint Operations College of the National Defense University in Shijiazhuang has created an AI military commander, for use in large-scale war simulations in the role of the commander-in-chief.
Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. The Ukrainian Army has developed 2024 autonomous Kamikazedrones in oder to make Russian interference during flight ineffective.  Many researchers avoid military applications.

Health
Healthcare
AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.
The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem. Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.
Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.
Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.
Other healthcare tasks thought suitable for an AI that are in development include:

Screening
Heart sound analysis
Companion robots for elder care
Medical record analysis
Treatment plan design
Medication management
Assisting blind people
Consultations
Drug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)
Clinical training
Outcome prediction for surgical procedures
HIV prognosis
Identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)
Helping link genes to their functions, otherwise analyzing genes and identification of novel biological targets
Help development of biomarkers
Help tailor therapies to individuals in personalized medicine/precision medicine

Workplace health and safety
AI-enabled chatbots decrease the need for humans to perform basic call center tasks.
Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.
AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.

Biochemistry
AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).

Chemistry and biology
Machine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines "computational synthesis with AI algorithms to predict molecular properties", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for "drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials". It has been used for the design of proteins with prespecified functional sites.
It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.
There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.
There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.

Novel types of machine learning
There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of "machine learning" not commonly associated with the term.
Similarly, there is research and development of biological "wetware computers" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.
Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.
A subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.

Digital ghosts
Biological computing in AI and as AI
However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with "as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it" and that if it was understood this does not mean there being "a technological solution to imitate natural intelligence". Technologies that integrate biology and are often AI-based include biorobotics.

Astronomy, space activities and ufology
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.

Future or non-human applications
Loeb has speculated that one type of technological equipment the project may detect could be "AI astronauts" and in 2021 – in an opinion piece – that AI "will" "supersede natural intelligence", while Martin Rees stated that there "may" be more civilizations than thought with the "majority of them" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as "safety of encounters with an alien AI", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of "AI astronauts" that engage in "supervised evolution" (see also: directed evolution, uplift, directed panspermia and space colonization).

Astrochemistry
It can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.

Other fields of research
Evidence of general impacts
In April 2024, the Scientific Advice Mechanism to the European Commission published advice including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.
As benefits, the evidence review highlighted:

its role in accelerating research and innovation
its capacity to automate workflows
enhancing dissemination of scientific work
As challenges:

limitations and risks around transparency, reproducibility and interpretability
poor performance (inaccuracy)
risk of harm through misuse or unintended use
societal concerns including the spread of misinformation and increasing inequalities

Archaeology, history and imaging of sites
Machine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.

Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. 
It can also be used for "non-invasive and non-destructive access to internal structures of archaeological remains".

Physics
A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.

Materials science
AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.
In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.

Reverse engineering
Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.

Law
Legal analysis
AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.
The electronic discovery industry uses machine learning to reduce manual searching.

Law enforcement and legal proceedings
Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. 
COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.
One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.
In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.: 124

Services
Human resources
Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.

Job search
AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.

Online and telephone customer service
AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.
A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative. Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.

Hospitality
In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.

Media
AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.
Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.

Motion interpolation
Pixel-art scaling algorithms
Image scaling
Image restoration
Photo colorization
Film restoration and video upscaling
Photo tagging
Automated species identification (such as identifying plants, fungi and animals with an app)
Text-to-image models such as DALL-E, Midjourney and Stable Diffusion
Image to video
Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google
Text to music with AI models such as MusicLM
Text to speech such as ElevenLabs and 15.ai
Motion capture
Make image transparent

Deep-fakes
Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.
In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.
In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.
In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.
In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames. DARPA gave 68 million dollars to work on deep-fake detection.
Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.
Respeecher is a program that enables one person to speak with the voice of another.

Video content analysis, surveillance and manipulated media detection
AI algorithms have been used to detect deepfake videos.

Video production
Artificial Intelligence is also starting to be used in video production, with tools and softwares being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023. Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.

Music
AI has been used to compose music of various genres.
David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.
In 2012, AI Iamus created the first complete classical album.
AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.
Melomics creates computer-generated music for stress and pain relief.
At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.
The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.
South Korean singer Hayeon's debut song, "Eyes on You" was composed using AI which was supervised by real composers, including NUVO.

Writing and reporting
Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.
Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.
TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or "how to balance the need for a coherent story progression with user agency, which is often at odds".
While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.
South Korean company Hanteo Global uses a journalism bot to write articles.
Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.

Sports writing
In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using a software from Narrative Science.
After being unable to cover every Minor League Baseball game with a large team of people, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.
UOL in Brazil expanded the use of AI in their writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.
El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter will be forced to change their comment in order to publish it.
A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been able to be done before without an extremely large team.
Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local news paper. This was met with a lot of criticism from readers for the very robotic diction that was published. With some descriptions of games being a "close encounter of the athletic kind," readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.

Wikipedia
Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.
Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.

Video games
In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a "solved problem" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.
Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.

Art
AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.
AI platforms such as "DALL-E", Stable Diffusion, Imagen, and Midjourney have been used for generating visual images from inputs such as text or other images. Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.
Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.

Art analysis
In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.

Computer animation
AI has been in use since the early 2000s, most notably by a system designed by Pixar called "Genesis". It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur. AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.

Utilities
Energy system
Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.
The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. 
Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).

Telecommunications
Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.

Manufacturing
Sensors
Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.

Toys and games
In the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.
Mattel created an assortment of AI-enabled toys that "understand" conversations, give intelligent responses, and learn.

Oil and gas
Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.

Transport
Automotive
AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.
AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg  and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.
There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.
There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.
Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.
AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.
Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.
Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).

Traffic management
AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.

Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.

Military
The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.
Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.
AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.
AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
Speech recognition allows traffic controllers to give verbal directions to drones.
Artificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.

NASA
In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.
The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.

Maritime
Neural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.

Environmental monitoring
Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.
For example, "Global Plastic Watch" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.

Early-warning systems
Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.

Computer science
Programming assistance
AI-powered code assisting tools
AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy. Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.
GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages. Price for individuals: $10/mo or $100/yr, with one free month trial.
Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota. Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited "starter" free version.
CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.
Ghostwriter by Replit offers code completion and chat. They have multiple pricing plans, including a free one and a "Hacker" plan for $7/month.
CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing. Individual plan is free, professional plan is $19/user/month.
Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby

Neural network design
AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.

Quantum computing
Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).

Historical contributions
AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:

Time sharing
Interactive interpreters
Graphical user interfaces and the computer mouse
Rapid application development environments
The linked list data structure
Automatic storage management
Symbolic programming
Functional programming
Dynamic programming
Object-oriented programming
Optical character recognition
Constraint satisfaction

Business
Content extraction
An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.

Architecture
AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex. 
AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.

List of applications
See also
Applications of artificial intelligence to legal informatics
Applications of deep learning
Applications of machine learning
Collective intelligence § Applications
List of artificial intelligence projects
List of datasets for machine-learning research
Open data
Progress in artificial intelligence
Timeline of computing 2020–present

Footnotes
Further reading
Kaplan, A.M.; Haenlein, M. (2018). "Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence". Business Horizons. 62 (1): 15–25. doi:10.1016/j.bushor.2018.08.004. S2CID 158433736.
Kurzweil, Ray (2005). The Singularity is Near: When Humans Transcend Biology. New York: Viking. ISBN 978-0-670-03384-3.
National Research Council (1999). "Developments in Artificial Intelligence". Funding a Revolution: Government Support for Computing Research. National Academy Press. ISBN 978-0-309-06278-7. OCLC 246584055.
Moghaddam, M. J.; Soleymani, M. R.; Farsi, M. A. (2015). "Sequence planning for stamping operations in progressive dies". Journal of Intelligent Manufacturing. 26 (2): 347–357. doi:10.1007/s10845-013-0788-0. S2CID 7843287.
Felten, Ed (3 May 2016). "Preparing for the Future of Artificial Intelligence".
In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.

Mapping function approach
Mapping methods try to mimic the expert by forming a direct mapping either from states to actions, or from states to reward values. For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills.

Inverse reinforcement learning approach
Inverse reinforcement learning (IRL) is the process of deriving a reward function from observed behavior. While ordinary "reinforcement learning" involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a person's behavior to figure out what goal that behavior seems to be trying to achieve. The IRL problem can be defined as:

Given 1) measurements of an agent's behaviour over time, in a variety of circumstances; 2) measurements of the sensory inputs to that agent; 3) a model of the physical environment (including the agent's body): Determine the reward function that the agent is optimizing.
IRL researcher Stuart J. Russell proposes that IRL might be used to observe humans and attempt to codify their complex "ethical values", in an effort to create "ethical robots" that might someday know "not to cook your cat" without needing to be explicitly told. The scenario can be modeled as a "cooperative inverse reinforcement learning game", where a "person" player and a "robot" player cooperate to secure the person's implicit goals, despite these goals not being explicitly known by either the person nor the robot.
In 2017, OpenAI and DeepMind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems.
Apprenticeship via inverse reinforcement learning (AIRP) was developed by in 2004 Pieter Abbeel, Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. AIRP deals with "Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform". AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.
One domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - "Autonomous Helicopter Aerobatics through Apprenticeship Learning"

System model approach
System models try to mimic the expert by modeling world dynamics.

Plan approach
The system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball
collection task.

Example
Learning from demonstration is often explained from a perspective that the working Robot-control-system is available and the human-demonstrator is using it. And indeed, if the software works, the Human operator takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 1:1. But that is not how the system works internally; it is only what the audience can observe. In reality, Learning from demonstration is much more complex. One of the first works on learning by robot apprentices (anthropomorphic robots learning by imitation) was Adrian Stoica's PhD thesis in 1995.
In 1997, robotics expert Stefan Schaal was working on the Sarcos robot-arm. The goal was simple: solve the pendulum swingup task. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an Optimal control-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a Brute-force solver but record the movements of a human-demonstration. The angle of the pendulum is logged over three seconds at the y-axis. This results into a diagram which produces a pattern.

In computer animation, the principle is called spline animation. That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases it's the position of an object. In the inverted pendulum it is the angle.
The overall task consists of two parts: recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called “Tracking control” or PID control. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle “steering behavior”, because the aim is to bring a robot to a given line.

See also
Inverse reinforcement learning


== References ==
Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design. It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose. One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.
The key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.

Strategies
Several strategies can be used for performing approximate computing.

Approximate circuits
Approximate arithmetic circuits: adders, multipliers and other logical circuits can reduce hardware overhead. For example, an approximate multi-bit adder can ignore the carry chain and thus, allow all its sub-adders to perform addition operation in parallel.
Approximate storage and memory
Instead of storing data values exactly, they can be stored approximately, e.g., by truncating the lower-bits in floating point data. Another method is to accept less reliable memory. For this, in DRAM and eDRAM, refresh rate assignments can be lowered or controlled. In SRAM, supply voltage can be lowered or controlled. Approximate storage can be applied to reduce MRAM's high write energy consumption. In general, any error detection and correction mechanisms should be disabled.
Software-level approximation
There are several ways to approximate at software level. Memoization or fuzzy memoization (the use of a vector database for approximate retrieval from a cache, i.e. fuzzy caching) can be applied. Some iterations of loops can be skipped (termed as loop perforation) to achieve a result faster. Some tasks can also be skipped, for example when a run-time condition suggests that those tasks are not going to be useful (task skipping). Monte Carlo algorithms and Randomized algorithms trade correctness for execution time guarantees. The computation can be reformulated according to paradigms that allow easily the acceleration on specialized hardware, e.g. a neural processing unit.
Approximate system
In an approximate system,  different subsystems of the system such as the processor, memory, sensor, and communication modules are synergistically approximated to obtain a much better system-level Q-E trade-off curve compared to individual approximations to each of the subsystems.

Application areas
Approximate computing has been used in a variety of domains where the applications are error-tolerant, such as multimedia processing, machine learning, signal processing, scientific computing. Therefore, approximate computing is mostly driven by applications that are related to human perception/cognition and have inherent error resilience. Many of these applications are based on statistical or probabilistic computation, such as different approximations can be made to better suit the desired objectives.
One notable application in machine learning is that Google is using this approach in their Tensor processing units (TPU, a custom ASIC).

Derived paradigms
The main issue in approximate computing is the identification of the section of the application that can be approximated. In the case of large scale applications, it is very common to find people holding the expertise on approximate computing techniques not having enough expertise on the application domain (and vice versa). In order to solve this problem, programming paradigms have been proposed. They all have in common the clear role separation between application programmer and application domain expert. These approaches allow the spread of the most common optimizations and approximate computing techniques.

See also
Artificial neural network
Metaheuristic
PCMOS


== References ==
arXiv (pronounced as "archive"—the X represents the Greek letter chi ⟨χ⟩) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month.

History
arXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.
It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with five mirrors around the world.
arXiv was an early adopter and promoter of preprints. Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. 
The annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.
In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence". However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee.
In January 2022, arXiv began assigning DOIs to articles, in collaboration with DataCite.

Data format
Each arXiv paper has a unique identifier:

YYMM.NNNNN, e.g. 1507.00123,
YYMM.NNNN, e.g. 0704.0001,
arch-ive/YYMMNNN for older papers, e.g. hep-th/9901001.
Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1. If no version number is specified, the default is the latest version.
arXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the "Trading and Market Microstructure" category within "quantitative finance". Other categories have one layer. For example, hep-ex is "high energy physics experiments".

Moderation process and endorsement
Although arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.
Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.
A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.
While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as "surprisingly rare". arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them; however, some authors have voiced concern over the lack of transparency in the arXiv screening process.

Submission formats
Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.

Access
The standard access route is through the arXiv.org website or one of several mirrors. Other interfaces and access routes have also been created by other un-associated organisations.
Metadata for arXiv is made available through OAI-PMH, the standard for open access repositories. Content is therefore indexed in all major consumers of such data, such as BASE, CORE and Unpaywall. As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access.
Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.

Copyright status of files
Files on arXiv can have a number of different copyright statuses:

Some are public domain, in which case they will have a statement saying so.
Some are available under either the Creative Commons 4.0 Attribution-ShareAlike license or the Creative Commons 4.0 Attribution-Noncommercial-ShareAlike license.
Some are copyright to the publisher, but the author has the right to distribute them and has given arXiv a non-exclusive irrevocable license to distribute them.
Most are copyright to the author, and arXiv has only a non-exclusive irrevocable license to distribute them.

See also
BioRxiv
PsyArXiv
List of academic databases and search engines
List of academic journals by preprint policy
List of preprint repositories
Sci-Hub
ViXra

Citations
General and cited sources
External links

Official website
arXiv (pronounced as "archive"—the X represents the Greek letter chi ⟨χ⟩) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month.

History
arXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.
It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with five mirrors around the world.
arXiv was an early adopter and promoter of preprints. Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. 
The annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.
In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence". However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee.
In January 2022, arXiv began assigning DOIs to articles, in collaboration with DataCite.

Data format
Each arXiv paper has a unique identifier:

YYMM.NNNNN, e.g. 1507.00123,
YYMM.NNNN, e.g. 0704.0001,
arch-ive/YYMMNNN for older papers, e.g. hep-th/9901001.
Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1. If no version number is specified, the default is the latest version.
arXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the "Trading and Market Microstructure" category within "quantitative finance". Other categories have one layer. For example, hep-ex is "high energy physics experiments".

Moderation process and endorsement
Although arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.
Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.
A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.
While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as "surprisingly rare". arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them; however, some authors have voiced concern over the lack of transparency in the arXiv screening process.

Submission formats
Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.

Access
The standard access route is through the arXiv.org website or one of several mirrors. Other interfaces and access routes have also been created by other un-associated organisations.
Metadata for arXiv is made available through OAI-PMH, the standard for open access repositories. Content is therefore indexed in all major consumers of such data, such as BASE, CORE and Unpaywall. As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access.
Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.

Copyright status of files
Files on arXiv can have a number of different copyright statuses:

Some are public domain, in which case they will have a statement saying so.
Some are available under either the Creative Commons 4.0 Attribution-ShareAlike license or the Creative Commons 4.0 Attribution-Noncommercial-ShareAlike license.
Some are copyright to the publisher, but the author has the right to distribute them and has given arXiv a non-exclusive irrevocable license to distribute them.
Most are copyright to the author, and arXiv has only a non-exclusive irrevocable license to distribute them.

See also
BioRxiv
PsyArXiv
List of academic databases and search engines
List of academic journals by preprint policy
List of preprint repositories
Sci-Hub
ViXra

Citations
General and cited sources
External links

Official website
Arithmetic coding (AC) is a form of entropy encoding used in lossless data compression. Normally, a string of characters is represented using a fixed number of bits per character, as in the ASCII code. When a string is converted to arithmetic encoding, frequently used characters will be stored with fewer bits and not-so-frequently occurring characters will be stored with more bits, resulting in fewer bits used in total. Arithmetic coding differs from other forms of entropy encoding, such as Huffman coding, in that rather than separating the input into component symbols and replacing each with a code, arithmetic coding encodes the entire message into a single number, an arbitrary-precision fraction q, where 0.0 ≤ q < 1.0. It represents the current information as a range, defined by two numbers. A recent family of entropy coders called asymmetric numeral systems allows for faster implementations thanks to directly operating on a single natural number representing the current information.

Implementation details and examples
Equal probabilities
In the simplest case, the probability of each symbol occurring is equal. For example, consider a set of three symbols, A, B, and C, each equally likely to occur. Encoding the symbols one by one would require 2 bits per symbol, which is wasteful: one of the bit variations is never used. That is to say, symbols A, B and C might be encoded respectively as 00, 01 and 10, with 11 unused.
A more efficient solution is to represent a sequence of these three symbols as a rational number in base 3 where each digit represents a symbol. For example, the sequence "ABBCAB" could become 0.0112013, in arithmetic coding as a value in the interval [0, 1). The next step is to encode this ternary number using a fixed-point binary number of sufficient precision to recover it, such as 0.00101100012 – this is only 10 bits; 2 bits are saved in comparison with naïve block encoding. This is feasible for long sequences because there are efficient, in-place algorithms for converting the base of arbitrarily precise numbers.
To decode the value, knowing the original string had length 6, one can simply convert back to base 3, round to 6 digits, and recover the string.

Defining a model
In general, arithmetic coders can produce near-optimal output for any given set of symbols and probabilities. (The optimal value is −log2P bits for each symbol of probability P; see Source coding theorem.) Compression algorithms that use arithmetic coding start by determining a model of the data – basically a prediction of what patterns will be found in the symbols of the message. The more accurate this prediction is, the closer to optimal the output will be.
Example: a simple, static model for describing the output of a particular monitoring instrument over time might be:

60% chance of symbol NEUTRAL
20% chance of symbol POSITIVE
10% chance of symbol NEGATIVE
10% chance of symbol END-OF-DATA. (The presence of this symbol means that the stream will be 'internally terminated', as is fairly common in data compression; when this symbol appears in the data stream, the decoder will know that the entire stream has been decoded.)
Models can also handle alphabets other than the simple four-symbol set chosen for this example. More sophisticated models are also possible: higher-order modelling changes its estimation of the current probability of a symbol based on the symbols that precede it (the context), so that in a model for English text, for example, the percentage chance of "u" would be much higher when it follows a "Q" or a "q". Models can even be adaptive, so that they continually change their prediction of the data based on what the stream actually contains. The decoder must have the same model as the encoder.

Encoding and decoding: overview
In general, each step of the encoding process, except for the last, is the same; the encoder has basically just three pieces of data to consider:

The next symbol that needs to be encoded
The current interval (at the very start of the encoding process, the interval is set to [0,1], but that will change)
The probabilities the model assigns to each of the various symbols that are possible at this stage (as mentioned earlier, higher-order or adaptive models mean that these probabilities are not necessarily the same in each step.)
The encoder divides the current interval into sub-intervals, each representing a fraction of the current interval proportional to the probability of that symbol in the current context. Whichever interval corresponds to the actual symbol that is next to be encoded becomes the interval used in the next step.
Example: for the four-symbol model above:

the interval for NEUTRAL would be [0, 0.6)
the interval for POSITIVE would be [0.6, 0.8)
the interval for NEGATIVE would be [0.8, 0.9)
the interval for END-OF-DATA would be [0.9, 1).
When all symbols have been encoded, the resulting interval unambiguously identifies the sequence of symbols that produced it. Anyone who has the same final interval and model that is being used can reconstruct the symbol sequence that must have entered the encoder to result in that final interval.
It is not necessary to transmit the final interval, however; it is only necessary to transmit one fraction that lies within that interval. In particular, it is only necessary to transmit enough digits (in whatever base) of the fraction so that all fractions that begin with those digits fall into the final interval; this will guarantee that the resulting code is a prefix code.

Encoding and decoding: example
Consider the process for decoding a message encoded with the given four-symbol model. The message is encoded in the fraction 0.538 (using decimal for clarity, instead of binary; also assuming that there are only as many digits as needed to decode the message.)
The process starts with the same interval used by the encoder: [0,1), and using the same model, dividing it into the same four sub-intervals that the encoder must have. The fraction 0.538 falls into the sub-interval for NEUTRAL, [0, 0.6); this indicates that the first symbol the encoder read must have been NEUTRAL, so this is the first symbol of the message.
Next divide the interval [0, 0.6) into sub-intervals:

the interval for NEUTRAL would be [0, 0.36), 60% of [0, 0.6).
the interval for POSITIVE would be [0.36, 0.48), 20% of [0, 0.6).
the interval for NEGATIVE would be [0.48, 0.54), 10% of [0, 0.6).
the interval for END-OF-DATA would be [0.54, 0.6), 10% of [0, 0.6).
Since 0.538 is within the interval [0.48, 0.54), the second symbol of the message must have been NEGATIVE.
Again divide our current interval into sub-intervals:

the interval for NEUTRAL would be [0.48, 0.516).
the interval for POSITIVE would be [0.516, 0.528).
the interval for NEGATIVE would be [0.528, 0.534).
the interval for END-OF-DATA would be [0.534, 0.540).
Now 0.538 falls within the interval of the END-OF-DATA symbol; therefore, this must be the next symbol. Since it is also the internal termination symbol, it means the decoding is complete. If the stream is not internally terminated, there needs to be some other way to indicate where the stream stops. Otherwise, the decoding process could continue forever, mistakenly reading more symbols from the fraction than were in fact encoded into it.

Sources of inefficiency
The message 0.538 in the previous example could have been encoded by the equally short fractions 0.534, 0.535, 0.536, 0.537 or 0.539. This suggests that the use of decimal instead of binary introduced some inefficiency. This is correct; the information content of a three-digit decimal is 
  
    
      
        3
        ×
        
          log
          
            2
          
        
        ⁡
        (
        10
        )
        ≈
        9.966
      
    
    {\displaystyle 3\times \log _{2}(10)\approx 9.966}
  
 bits; the same message could have been encoded in the binary fraction 0.10001001 (equivalent to 0.53515625 decimal) at a cost of only 8bits.
This 8 bit output is larger than the information content, or entropy of the message, which is

  
    
      
        ∑
        −
        
          log
          
            2
          
        
        ⁡
        (
        
          p
          
            i
          
        
        )
        =
        −
        
          log
          
            2
          
        
        ⁡
        (
        0.6
        )
        −
        
          log
          
            2
          
        
        ⁡
        (
        0.1
        )
        −
        
          log
          
            2
          
        
        ⁡
        (
        0.1
        )
        =
        7.381
        
           bits
        
        .
      
    
    {\displaystyle \sum -\log _{2}(p_{i})=-\log _{2}(0.6)-\log _{2}(0.1)-\log _{2}(0.1)=7.381{\text{ bits}}.}
  

But an integer number of bits must be used in the binary encoding, so an encoder for this message would use at least 8 bits, resulting in a message 8.4% larger than the entropy contents. This inefficiency of at most 1 bit results in relatively less overhead as the message size grows.
Moreover, the claimed symbol probabilities were [0.6, 0.2, 0.1, 0.1), but the actual frequencies in this example are [0.33, 0, 0.33, 0.33). If the intervals are readjusted for these frequencies, the entropy of the message would be 4.755 bits and the same NEUTRAL NEGATIVE END-OF-DATA message could be encoded as intervals [0, 1/3); [1/9, 2/9); [5/27, 6/27); and a binary interval of [0.00101111011, 0.00111000111). This is also an example of how statistical coding methods like arithmetic encoding can produce an output message that is larger than the input message, especially if the probability model is off.

Adaptive arithmetic coding
One advantage of arithmetic coding over other similar methods of data compression is the convenience of adaptation. Adaptation is the changing of the frequency (or probability) tables while processing the data. The decoded data matches the original data as long as the frequency table in decoding is replaced in the same way and in the same step as in encoding. The synchronization is, usually, based on a combination of symbols occurring during the encoding and decoding process.

Precision and renormalization
The above explanations of arithmetic coding contain some simplification. In particular, they are written as if the encoder first calculated the fractions representing the endpoints of the interval in full, using infinite precision, and only converted the fraction to its final form at the end of encoding. Rather than try to simulate infinite precision, most arithmetic coders instead operate at a fixed limit of precision which they know the decoder will be able to match, and round the calculated fractions to their nearest equivalents at that precision. An example shows how this would work if the model called for the interval [0,1) to be divided into thirds, and this was approximated with 8 bit precision. Note that since now the precision is known, so are the binary ranges we'll be able to use.

A process called renormalization keeps the finite precision from becoming a limit on the total number of symbols that can be encoded. Whenever the range is reduced to the point where all values in the range share certain beginning digits, those digits are sent to the output. For however many digits of precision the computer can handle, it is now handling fewer than that, so the existing digits are shifted left, and at the right, new digits are added to expand the range as widely as possible. Note that this result occurs in two of the three cases from our previous example.

Arithmetic coding as a generalized change of radix
Recall that in the case where the symbols had equal probabilities, arithmetic coding could be implemented by a simple change of base, or radix. In general, arithmetic (and range) coding may be interpreted as a generalized change of radix. For example, we may look at any sequence of symbols:

  
    
      
        
          D
          A
          B
          D
          D
          B
        
      
    
    {\displaystyle \mathrm {DABDDB} }
  

as a number in a certain base presuming that the involved symbols form an ordered set and each symbol in the ordered set denotes a sequential integer A = 0, B = 1, C = 2, D = 3, and so on. This results in the following frequencies and cumulative frequencies:

The cumulative frequency for an item is the sum of all frequencies preceding the item. In other words, cumulative frequency is a running total of frequencies.
In a positional numeral system the radix, or base, is numerically equal to a number of different symbols used to express the number. For example, in the decimal system the number of symbols is 10, namely 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. The radix is used to express any finite integer in a presumed multiplier in polynomial form. For example, the number 457 is actually 4×102 + 5×101 + 7×100, where base 10 is presumed but not shown explicitly.
Initially, we will convert DABDDB into a base-6 numeral, because 6 is the length of the string. The string is first mapped into the digit string 301331, which then maps to an integer by the polynomial:

  
    
      
        
          6
          
            5
          
        
        ×
        3
        +
        
          6
          
            4
          
        
        ×
        0
        +
        
          6
          
            3
          
        
        ×
        1
        +
        
          6
          
            2
          
        
        ×
        3
        +
        
          6
          
            1
          
        
        ×
        3
        +
        
          6
          
            0
          
        
        ×
        1
        =
        23671
      
    
    {\displaystyle 6^{5}\times 3+6^{4}\times 0+6^{3}\times 1+6^{2}\times 3+6^{1}\times 3+6^{0}\times 1=23671}
  

The result 23671 has a length of 15 bits, which is not very close to the theoretical limit (the entropy of the message), which is approximately 9 bits.
To encode a message with a length closer to the theoretical limit imposed by information theory we need to slightly generalize the classic formula for changing the radix. We will compute lower and upper bounds L and U and choose a number between them. For the computation of L we multiply each term in the above expression by the product of the frequencies of all previously occurred symbols:

  
    
      
        
          
            
              
                L
                =
                

                
              
              
                
                (
                
                  6
                  
                    5
                  
                
                ×
                3
                )
                +
                

                
              
            
            
              
              
                3
                ×
                (
                
                  6
                  
                    4
                  
                
                ×
                0
                )
                +
                

                
              
            
            
              
              
                
                (
                3
                ×
                1
                )
                ×
                (
                
                  6
                  
                    3
                  
                
                ×
                1
                )
                +
                

                
              
            
            
              
              
                
                (
                3
                ×
                1
                ×
                2
                )
                ×
                (
                
                  6
                  
                    2
                  
                
                ×
                3
                )
                +
                

                
              
            
            
              
              
                
                (
                3
                ×
                1
                ×
                2
                ×
                3
                )
                ×
                (
                
                  6
                  
                    1
                  
                
                ×
                3
                )
                +
                

                
              
            
            
              
              
                
                (
                3
                ×
                1
                ×
                2
                ×
                3
                ×
                3
                )
                ×
                (
                
                  6
                  
                    0
                  
                
                ×
                1
                )
                

                
              
            
            
              
                =
                

                
              
              
                25002
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L={}&(6^{5}\times 3)+{}\\&3\times (6^{4}\times 0)+{}\\&(3\times 1)\times (6^{3}\times 1)+{}\\&(3\times 1\times 2)\times (6^{2}\times 3)+{}\\&(3\times 1\times 2\times 3)\times (6^{1}\times 3)+{}\\&(3\times 1\times 2\times 3\times 3)\times (6^{0}\times 1){}\\={}&25002\end{aligned}}}
  

The difference between this polynomial and the polynomial above is that each term is multiplied by the product of the frequencies of all previously occurring symbols. More generally, L may be computed as:

  
    
      
        L
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          n
          
            n
            −
            i
          
        
        
          C
          
            i
          
        
        
          
            ∏
            
              k
              =
              1
            
            
              i
              −
              1
            
          
          
            f
            
              k
            
          
        
      
    
    {\displaystyle L=\sum _{i=1}^{n}n^{n-i}C_{i}{\prod _{k=1}^{i-1}f_{k}}}
  

where 
  
    
      
        
          
            C
            
              i
            
          
        
      
    
    {\displaystyle \scriptstyle C_{i}}
  
 are the cumulative frequencies and 
  
    
      
        
          
            f
            
              k
            
          
        
      
    
    {\displaystyle \scriptstyle f_{k}}
  
 are the frequencies of occurrences. Indexes denote the position of the symbol in a message. In the special case where all frequencies 
  
    
      
        
          
            f
            
              k
            
          
        
      
    
    {\displaystyle \scriptstyle f_{k}}
  
 are 1, this is the change-of-base formula.
The upper bound U will be L plus the product of all frequencies; in this case U = L + (3 × 1 × 2 × 3 × 3 × 2) = 25002 + 108 = 25110. In general, U is given by:

  
    
      
        U
        =
        L
        +
        
          ∏
          
            k
            =
            1
          
          
            n
          
        
        
          f
          
            k
          
        
      
    
    {\displaystyle U=L+\prod _{k=1}^{n}f_{k}}
  

Now we can choose any number from the interval [L, U) to represent the message; one convenient choice is the value with the longest possible trail of zeroes, 25100, since it allows us to achieve compression by representing the result as 251×102. The zeroes can also be truncated, giving 251, if the length of the message is stored separately. Longer messages will tend to have longer trails of zeroes.
To decode the integer 25100, the polynomial computation can be reversed as shown in the table below. At each stage the current symbol is identified, then the corresponding term is subtracted from the result.

During decoding we take the floor after dividing by the corresponding power of 6. The result is then matched against the cumulative intervals and the appropriate symbol is selected from look up table. When the symbol is identified the result is corrected. The process is continued for the known length of the message or while the remaining result is positive. The only difference compared to the classical change-of-base is that there may be a range of values associated with each symbol. In this example, A is always 0, B is either 1 or 2, and D is any of 3, 4, 5. This is in exact accordance with our intervals that are determined by the frequencies. When all intervals are equal to 1 we have a special case of the classic base change.

Theoretical limit of compressed message
The lower bound L never exceeds nn, where n is the size of the message, and so can be represented in 
  
    
      
        
          log
          
            2
          
        
        ⁡
        (
        
          n
          
            n
          
        
        )
        =
        n
        
          log
          
            2
          
        
        ⁡
        (
        n
        )
      
    
    {\displaystyle \log _{2}(n^{n})=n\log _{2}(n)}
  
 bits. After the computation of the upper bound U and the reduction of the message by selecting a number from the interval [L, U) with the longest trail of zeros we can presume that this length can be reduced by 
  
    
      
        
          
            log
            
              2
            
          
          ⁡
          
            (
            
              
                ∏
                
                  k
                  =
                  1
                
                
                  n
                
              
              
                f
                
                  k
                
              
            
            )
          
        
      
    
    {\displaystyle \textstyle \log _{2}\left(\prod _{k=1}^{n}f_{k}\right)}
  
 bits. Since each frequency in a product occurs exactly the same number of times as the value of this frequency, we can use the size of the alphabet A for the computation of the product

  
    
      
        
          ∏
          
            k
            =
            1
          
          
            n
          
        
        
          f
          
            k
          
        
        =
        
          ∏
          
            k
            =
            1
          
          
            A
          
        
        
          f
          
            k
          
          
            
              f
              
                k
              
            
          
        
        .
      
    
    {\displaystyle \prod _{k=1}^{n}f_{k}=\prod _{k=1}^{A}f_{k}^{f_{k}}.}
  

Applying log2 for the estimated number of bits in the message, the final message (not counting a logarithmic overhead for the message length and frequency tables) will match the number of bits given by entropy, which for long messages is very close to optimal:

  
    
      
        −
        
          [
          
            
              ∑
              
                i
                =
                1
              
              
                A
              
            
            
              f
              
                i
              
            
            
              log
              
                2
              
            
            ⁡
            (
            
              f
              
                i
              
            
            )
          
          ]
        
        n
        =
        n
        H
      
    
    {\displaystyle -\left[\sum _{i=1}^{A}f_{i}\log _{2}(f_{i})\right]n=nH}
  

In other words, the efficiency of arithmetic encoding approaches the theoretical limit of 
  
    
      
        H
      
    
    {\displaystyle H}
  
 bits per symbol, as the message length approaches infinity.

Asymptotic equipartition
We can understand this intuitively. Suppose the source is ergodic, then it has the asymptotic equipartition property (AEP). By the AEP, after a long stream of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 symbols, the interval of 
  
    
      
        (
        0
        ,
        1
        )
      
    
    {\displaystyle (0,1)}
  
 is almost partitioned into almost equally-sized intervals. 
Technically, for any small 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
  
, for all large enough 
  
    
      
        n
      
    
    {\displaystyle n}
  
, there exists 
  
    
      
        
          2
          
            n
            H
            (
            X
            )
            (
            1
            +
            O
            (
            ϵ
            )
            )
          
        
      
    
    {\displaystyle 2^{nH(X)(1+O(\epsilon ))}}
  
 strings 
  
    
      
        
          x
          
            1
            :
            n
          
        
      
    
    {\displaystyle x_{1:n}}
  
, such that each string has almost equal probability 
  
    
      
        P
        r
        (
        
          x
          
            1
            :
            n
          
        
        )
        =
        
          2
          
            −
            n
            H
            (
            X
            )
            (
            1
            +
            O
            (
            ϵ
            )
            )
          
        
      
    
    {\displaystyle Pr(x_{1:n})=2^{-nH(X)(1+O(\epsilon ))}}
  
, and their total probability is 
  
    
      
        1
        −
        O
        (
        ϵ
        )
      
    
    {\displaystyle 1-O(\epsilon )}
  
.
For any such string, it is arithmetically encoded by a binary string of length 
  
    
      
        k
      
    
    {\displaystyle k}
  
, where 
  
    
      
        k
      
    
    {\displaystyle k}
  
 is the smallest 
  
    
      
        k
      
    
    {\displaystyle k}
  
 such that there exists a fraction of form 
  
    
      
        
          
            ?
            
              2
              
                k
              
            
          
        
      
    
    {\displaystyle {\frac {?}{2^{k}}}}
  
 in the interval for 
  
    
      
        
          x
          
            1
            :
            n
          
        
      
    
    {\displaystyle x_{1:n}}
  
. Since the interval for 
  
    
      
        
          x
          
            1
            :
            n
          
        
      
    
    {\displaystyle x_{1:n}}
  
 has size 
  
    
      
        
          2
          
            −
            n
            H
            (
            X
            )
            (
            1
            +
            O
            (
            ϵ
            )
            )
          
        
      
    
    {\displaystyle 2^{-nH(X)(1+O(\epsilon ))}}
  
, we should expect it to contain one fraction of form 
  
    
      
        
          
            ?
            
              2
              
                k
              
            
          
        
      
    
    {\displaystyle {\frac {?}{2^{k}}}}
  
 when 
  
    
      
        k
        =
        n
        H
        (
        X
        )
        (
        1
        +
        O
        (
        ϵ
        )
        )
      
    
    {\displaystyle k=nH(X)(1+O(\epsilon ))}
  
.
Thus, with high probability, 
  
    
      
        
          x
          
            1
            :
            n
          
        
      
    
    {\displaystyle x_{1:n}}
  
 can be arithmetically encoded with a binary string of length 
  
    
      
        n
        H
        (
        X
        )
        (
        1
        +
        O
        (
        ϵ
        )
        )
      
    
    {\displaystyle nH(X)(1+O(\epsilon ))}
  
.

Connections with other compression methods
Huffman coding
Because arithmetic coding doesn't compress one datum at a time, it can get arbitrarily close to entropy when compressing IID strings. By contrast, using the extension of Huffman coding (to strings) does not reach entropy unless all probabilities of alphabet symbols are powers of two, in which case both Huffman and arithmetic coding achieve entropy.
When naively Huffman coding binary strings, no compression is possible, even if entropy is low (e.g. ({0, 1}) has probabilities {0.95, 0.05}). Huffman encoding assigns 1 bit to each value, resulting in a code of the same length as the input. By contrast, arithmetic coding compresses bits well, approaching the optimal compression ratio of

  
    
      
        1
        −
        [
        −
        0.95
        
          log
          
            2
          
        
        ⁡
        (
        0.95
        )
        +
        −
        0.05
        
          log
          
            2
          
        
        ⁡
        (
        0.05
        )
        ]
        ≈
        71.4
        %
        .
      
    
    {\displaystyle 1-[-0.95\log _{2}(0.95)+-0.05\log _{2}(0.05)]\approx 71.4\%.}
  

One simple way to address Huffman coding's suboptimality is to concatenate symbols ("blocking") to form a new alphabet in which each new symbol represents a sequence of original symbols – in this case bits – from the original alphabet. In the above example, grouping sequences of three symbols before encoding would produce new "super-symbols" with the following frequencies:

000: 85.7%
001, 010, 100: 4.5% each
011, 101, 110: 0.24% each
111: 0.0125%
With this grouping, Huffman coding averages 1.3 bits for every three symbols, or 0.433 bits per symbol, compared with one bit per symbol in the original encoding, i.e., 
  
    
      
        56.7
        %
      
    
    {\displaystyle 56.7\%}
  
 compression. Allowing arbitrarily large sequences gets arbitrarily close to entropy – just like arithmetic coding – but requires huge codes to do so, so is not as practical as arithmetic coding for this purpose.
An alternative is encoding run lengths via Huffman-based Golomb-Rice codes. Such an approach allows simpler and faster encoding/decoding than arithmetic coding or even Huffman coding, since the latter requires a table lookups. In the {0.95, 0.05} example, a Golomb-Rice code with a four-bit remainder achieves a compression ratio of 
  
    
      
        71.1
        %
      
    
    {\displaystyle 71.1\%}
  
, far closer to optimum than using three-bit blocks. Golomb-Rice codes only apply to Bernoulli inputs such as the one in this example, however, so it is not a substitute for blocking in all cases.

History and patents
Basic algorithms for arithmetic coding were developed independently by Jorma J. Rissanen, at IBM Research, and by Richard C. Pasco, a Ph.D. student at Stanford University; both were published in May 1976. Pasco cites a pre-publication draft of Rissanen's article and comments on the relationship between their works:

One algorithm of the family was developed independently by Rissanen [1976]. It shifts the code element to the most significant end of the accumulator, using a pointer obtained by addition and exponentiation. We shall now compare the alternatives in the three choices, and see that it is preferable to shift the code element rather than the accumulator, and to add code elements to the least significant end of the accumulator.
Less than a year after publication, IBM filed for a US patent on Rissanen's work. Pasco's work was not patented.
A variety of specific techniques for arithmetic coding have historically been covered by US patents, although various well-known methods have since passed into the public domain as the patents have expired. Techniques covered by patents may be essential for implementing the algorithms for arithmetic coding that are specified in some formal international standards. When this is the case, such patents are generally available for licensing under what is called "reasonable and non-discriminatory" (RAND) licensing terms (at least as a matter of standards-committee policy). In some well-known instances, (including some involving IBM patents that have since expired), such licenses were available for free, and in other instances, licensing fees have been required. The availability of licenses under RAND terms does not necessarily satisfy everyone who might want to use the technology, as what may seem "reasonable" for a company preparing a proprietary commercial software product may seem much less reasonable for a free software or open source project.
At least one significant compression software program, bzip2, deliberately discontinued the use of arithmetic coding in favor of Huffman coding due to the perceived patent situation at the time. Also, encoders and decoders of the JPEG file format, which has options for both Huffman encoding and arithmetic coding, typically only support the Huffman encoding option, which was originally because of patent concerns; the result is that nearly all JPEG images in use today use Huffman encoding although JPEG's arithmetic coding patents have expired due to the age of the JPEG standard (the design of which was approximately completed by 1990). JPEG XL, as well as archivers like PackJPG, Brunsli and Lepton, that can losslessly convert Huffman encoded files to ones with arithmetic coding (or asymmetric numeral systems in case of JPEG XL), showing up to 25% size saving.
The JPEG image compression format's arithmetic coding algorithm is based on the following cited patents (since expired).

U.S. patent 4,652,856 – (IBM) Filed 4 February 1986, granted 24 March 1987 – Kottappuram M. A. Mohiuddin, Jorma Johannes Rissanen – Multiplication-free multi-alphabet arithmetic code
U.S. patent 4,905,297 – (IBM) Filed 18 November 1988, granted 27 February 1990 – Glen George Langdon, Joan L. Mitchell, William B. Pennebaker, Jorma Johannes Rissanen – Arithmetic coding encoder and decoder system
U.S. patent 4,935,882 – (IBM) Filed 20 July 1988, granted 19 June 1990 – William B. Pennebaker, Joan L. Mitchell – Probability adaptation for arithmetic coders
JP Patent 1021672 – (Mitsubishi) Filed 21 January 1989, granted 10 August 1990 – Toshihiro Kimura, Shigenori Kino, Fumitaka Ono, Masayuki Yoshida – Coding system
JP Patent 2-46275 – (Mitsubishi) Filed 26 February 1990, granted 5 November 1991 – Fumitaka Ono, Tomohiro Kimura, Masayuki Yoshida, Shigenori Kino – Coding apparatus and coding method
Other patents (mostly also expired) related to arithmetic coding include the following.

U.S. patent 4,122,440 – (IBM) Filed 4 March 1977, granted 24 October 1978 – Glen George Langdon, Jorma Johannes Rissanen – Method and means for arithmetic string coding
U.S. patent 4,286,256 – (IBM) Filed 28 November 1979, granted 25 August 1981 – Glen George Langdon, Jorma Johannes Rissanen – Method and means for arithmetic coding utilizing a reduced number of operations
U.S. patent 4,467,317 – (IBM) Filed 30 March 1981, granted 21 August 1984 – Glen George Langdon, Jorma Johannes Rissanen – High-speed arithmetic compression coding using concurrent value updating
U.S. patent 4,891,643 – (IBM) Filed 15 September 1986, granted 2 January 1990 – Joan L. Mitchell, William B. Pennebaker – Arithmetic coding data compression/de-compression by selectively employed, diverse arithmetic coding encoders and decoders
JP Patent 11782787 – (NEC) Filed 13 May 1987, granted 18 November 1988 – Michio Shimada – Data compressing arithmetic encoding device
JP Patent 15015487 – (KDDI) Filed 18 June 1987, granted 22 December 1988 – Shuichi Matsumoto, Masahiro Saito – System for preventing carrying propagation in arithmetic coding
U.S. patent 4,933,883 – (IBM) Filed 3 May 1988, granted 12 June 1990 – William B. Pennebaker, Joan L. Mitchell – Probability adaptation for arithmetic coders
U.S. patent 4,989,000 – (IBM) Filed 19 June 1989, granted 29 January 1991 – Dan S. Chevion, Ehud D. Karnin, Eugeniusz Walach – Data string compression using arithmetic encoding with simplified probability subinterval estimation
U.S. patent 5,099,440 – (IBM) Filed 5 January 1990, granted 24 March 1992 – William B. Pennebaker, Joan L. Mitchell – Probability adaptation for arithmetic coders
U.S. patent 5,272,478 – (Ricoh) Filed 17 August 1992, granted 21 December 1993 – James D. Allen – Method and apparatus for entropy coding
Note: This list is not exhaustive. See the following links for a list of more US patents. The Dirac codec uses arithmetic coding and is not patent pending.
Patents on arithmetic coding may exist in other jurisdictions; see software patents for a discussion of the patentability of software around the world.

Benchmarks and other technical characteristics
Every programmatic implementation of arithmetic encoding has a different compression ratio and performance. While compression ratios vary only a little (usually under 1%), the code execution time can vary by a factor of 10. Choosing the right encoder from a list of publicly available encoders is not a simple task because performance and compression ratio depend also on the type of data, particularly on the size of the alphabet (number of different symbols). One of two particular encoders may have better performance for small alphabets while the other may show better performance for large alphabets. Most encoders have limitations on the size of the alphabet and many of them are specialized for alphabets of exactly two symbols (0 and 1).

See also
Asymmetric numeral systems
Context-adaptive binary arithmetic coding (CABAC)
Data compression
Entropy encoding
Huffman coding
Range encoding
Run-length encoding

Notes
References
MacKay, David J.C. (September 2003). "Chapter 6: Stream Codes". Information Theory, Inference, and Learning Algorithms. Cambridge University Press. ISBN 0-521-64298-1. Archived from the original (PDF/PostScript/DjVu/LaTeX) on 22 December 2007. Retrieved 30 December 2007.
Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). "Section 22.6. Arithmetic Coding". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8. Archived from the original on 11 August 2011. Retrieved 18 August 2011.
Rissanen, Jorma (May 1976). "Generalized Kraft Inequality and Arithmetic Coding". IBM Journal of Research and Development. 20 (3): 198–203. doi:10.1147/rd.203.0198. Retrieved 21 September 2007.
Rissanen, J.J.; Langdon G.G., Jr (March 1979). "Arithmetic coding" (PDF). IBM Journal of Research and Development. 23 (2): 149–162. doi:10.1147/rd.232.0149. S2CID 39909636. Archived from the original (PDF) on 28 September 2007. Retrieved 22 September 2007.
Witten, Ian H.; Neal, Radford M.; Cleary, John G. (June 1987). "Arithmetic Coding for Data Compression" (PDF). Communications of the ACM. 30 (6): 520–540. doi:10.1145/214762.214771. S2CID 3343393. Archived (PDF) from the original on 28 September 2007. Retrieved 21 September 2007.
Rodionov Anatoly, Volkov Sergey (2010) "p-adic arithmetic coding" Contemporary Mathematics Volume 508, 2010 Contemporary Mathematics
Rodionov Anatoly, Volkov Sergey (2007) "p-adic arithmetic coding", P-adic arithmetic coding

External links
 This article incorporates public domain material from Paul E. Black. "Arithmetic coding". Dictionary of Algorithms and Data Structures. NIST.
Newsgroup posting with a short worked example of arithmetic encoding (integer-only).
PlanetMath article on arithmetic coding
Anatomy of Range Encoder The article explains both range and arithmetic coding. It has also code samples for 3 different arithmetic encoders along with performance comparison.
Introduction to Arithmetic Coding Archived 9 November 2020 at the Wayback Machine. 60 pages.
Eric Bodden, Malte Clasen and Joachim Kneis: Arithmetic Coding revealed. Technical Report 2007-5, Sable Research Group, McGill University.
Arithmetic Coding + Statistical Modeling = Data Compression by Mark Nelson.
Data Compression With Arithmetic Coding by Mark Nelson (2014)
Fast implementation of range coding and rANS by James K. Bonfield
In computer science, an array is a data structure consisting of a collection of elements (values or variables), of same memory size, each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.
For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).
The memory address of the first element of an array is called first address, foundation address, or base address.
Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called "matrices". In some cases the term "vector" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word "table" is sometimes used as a synonym of array.
Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.
The term "array" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
The term is also used, especially in the description of algorithms, to mean associative array or "abstract array", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.

History
The first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. John von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer. Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.
Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), Lisp (1958), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.

Applications
Arrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.
Arrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient (implicit data structures), requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).
One or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate "dynamic memory" portably.
Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple IF statements. They are known in this context as control tables and are used in conjunction with a purpose-built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.

Element identifier and addressing formulas
When data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indexes are also called subscripts. An index maps the array value to a stored object.
There are three ways in which the elements of an array can be indexed:

0 (zero-based indexing)
The first element of the array is indexed by subscript of 0.
1 (one-based indexing)
The first element of the array is indexed by subscript of 1.
n (n-based indexing)
The base index of an array can be freely chosen. Usually programming languages allowing n-based indexing also allow negative index values and other scalar data types like enumerations, or characters may be used as an array index.
Using zero based indexing is the design choice of many influential programming languages, including C, Java and Lisp. This leads to simpler implementation where the subscript refers to an offset from the starting position of an array, so the first element has an offset of zero.
Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example, a two-dimensional array A with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression A[1][3] in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.
The number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.
In standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a "linear" formula on the indices.

One-dimensional arrays
A one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.
As an example consider the C declaration int anArrayName[10]; which declares a one-dimensional array of ten integers. Here, the array can store ten elements of type int . This array has indices starting from zero through nine. For example, the expressions anArrayName[0] and anArrayName[9] are the first and last elements respectively.
For a vector with linear addressing, the element with index i is located at the address B + c · i, where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.
If the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element "zeroth" rather than "first".
However, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by B + 30c, then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.

Multidimensional arrays
For a multidimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.
More generally, in a k-dimensional array, the address of an element with indices i1, i2, ..., ik is

B + c1 · i1 + c2 · i2 + … + ck · ik.
For example: int a[2][3];
This means that array a has 2 rows and 3 columns, and the array is of integer type. Here we can store 6 elements they will be stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a11, a12, a13, a21, a22, a23.
This formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.
The coefficients ck must be chosen so that every valid index tuple maps to the address of a distinct element.
If the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by B + c1 − 3c2 will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.

Dope vectors
The addressing formula is completely defined by the dimension d, the base address B, and the increments c1, c2, ..., ck. It is often useful to pack these parameters into a record called the array's descriptor, stride vector, or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.

Compact layouts
Often the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.

There are two systematic compact layouts for a two-dimensional array. For example, consider the matrix

  
    
      
        A
        =
        
          
            [
            
              
                
                  1
                
                
                  2
                
                
                  3
                
              
              
                
                  4
                
                
                  5
                
                
                  6
                
              
              
                
                  7
                
                
                  8
                
                
                  9
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle A={\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}}.}
  

In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:

In column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:

For arrays with three or more indices, "row major order" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. "Column major order" is analogous with respect to the first index.
In systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. This is known as spatial locality, which is a type of locality of reference. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.

Resizing
Static arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.
Some array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.

Non-linear formulas
More complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.

Efficiency
Both store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.
In an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.
Memory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead (e.g., to store index bounds) but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.
Array accesses with statically predictable access patterns are a major source of data parallelism.

Comparison with other data structures
Dynamic arrays or growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.
Associative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.
Balanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.
Linked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.

An Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row(pointer on c or c++). Thus an element in row i and column j of an array A would be accessed by double indexing (A[i][j] in typical notation). This alternative structure allows jagged arrays, where each row may have a different size—or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.

Dimension
The dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array is a rectangle of data, a three-dimensional array a block of data, etc.
This should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.

See also
References
External links

 Data Structures/Arrays at Wikibooks
Arthur Lee Samuel (December 5, 1901 – July 29, 1990) was an American pioneer in the field of computer gaming and artificial intelligence. He popularized the term "machine learning" in 1959. The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI). He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.

Biography
Samuel was born on December 5, 1901, in Emporia, Kansas, and graduated from the College of Emporia in Kansas in 1923.
He received a master's degree in Electrical Engineering from MIT in 1926, and taught for two years as an instructor.  In 1928, he joined Bell Laboratories, where he worked mostly on vacuum tubes, including improvements of radar during World War II.  He developed a gas-discharge transmit-receive switch (TR tube) that allowed a single antenna to be used for both transmitting and receiving. After the war he moved to the University of Illinois at Urbana–Champaign to become a Professor of Electrical Engineering, where he initiated the ILLIAC project, but left before its first computer was complete.
Samuel went to IBM in Poughkeepsie, New York, in 1949, where he would conceive and carry out his most successful work. He is credited with one of the first software hash tables, and influencing early research in using transistors for computers at IBM. At IBM he made the first checkers program on IBM's first commercial computer, the IBM 701.  The program was a sensational demonstration of the advances in both hardware and skilled programming and caused IBM's stock to increase 15 points overnight.  His pioneering non-numerical programming helped shape the instruction set of processors, as he was one of the first to work with computers on projects other than computation. He was known for writing articles that made complex subjects easy to understand. He was chosen to write an introduction to one of the earliest journals devoted to computing in 1953.
In 1966, Samuel retired from IBM and became a professor at Stanford University, where he worked the remainder of his life.  He worked with Donald Knuth on the TeX project, including writing some of the documentation. He continued to write software past his 88th birthday.
He was given the Computer Pioneer Award by the IEEE Computer Society in 1987.
He died of complications from Parkinson's disease on July 29, 1990.

Computer checkers (draughts) development
Samuel is most known within the AI community for his groundbreaking work in computer checkers in 1959, and seminal research on machine learning, beginning in 1949. He graduated from MIT and taught at MIT and UIUC from 1946 to 1949. He believed teaching computers to play games was very fruitful for developing tactics appropriate to general problems, and he chose checkers as it is relatively simple though has a depth of strategy. The main driver of the machine was a search tree of the board positions reachable from the current state.  Since he had only a very limited amount of available computer memory, Samuel implemented what is now called alpha-beta pruning. Instead of searching each path until it came to the game's conclusion, Samuel developed a scoring function based on the position of the board at any given time. This function tried to measure the chance of winning for each side at the given position.  It took into account such things as the number of pieces on each side, the number of kings, and the proximity of pieces to being “kinged”.  The program chose its move based on a minimax strategy, meaning it made the move that optimized the value of this function, assuming that the opponent was trying to optimize the value of the same function from its point of view.
Samuel also designed various mechanisms by which his program could become better. In what he called rote learning, the program remembered every position it had already seen, along with the terminal value of the reward function. This technique effectively extended the search depth at each of these positions.  Samuel's later programs reevaluated the reward function based on input from professional games. He also had it play thousands of games against itself as another way of learning. With all of this work, Samuel's program reached a respectable amateur status and was the first to play any board game at this high a level. He continued to work on checkers until the mid-1970s, at which point his program achieved sufficient skill to challenge a respectable amateur.

Awards
1990. Founding Fellow of the Association for the Advancement of Artificial Intelligence
1987. Computer Pioneer Award.
For Adaptive non-numeric processing.

Selected works
Computing bit by bit, or Digital computers made easy (1953). Proceedings of the Institute of Radio Engineers 41, 1223-1230.
Samuel, A. L. (2000). "Some studies in machine learning using the game of checkers". IBM Journal of Research and Development. 44. IBM: 206–226. doi:10.1147/rd.441.0206.
Pioneer of machine learning.
Reprinted with an additional annotated game in Computers and Thought, edited by Edward Feigenbaum and Julian Feldman (New York: McGraw-Hill, 1963), 71-105.
1983. First Grade TeX: A Beginner's TeX Manual. Stanford Computer Science Report STAN-CS-83-985 (November 1983).
Senior member in TeX community.

References
External links
Checkers AI: a look at Arthur Samuel's ideas on GitHub
Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995, and the fourth edition of the book was released on 28 April 2020.
AIMA has been called "the most popular artificial intelligence textbook in the world", and is considered the standard text in the field of artificial intelligence. As of 2023, it was being used at over 1500 universities worldwide, and it has over 59,000 citations on Google Scholar.
AIMA is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.

Content
AIMA gives detailed information about the working of algorithms in AI. The book's chapters span from classical AI topics like searching algorithms and first-order logic, propositional logic and probabilistic reasoning to advanced topics such as multi-agent systems, constraint satisfaction problems, optimization problems, artificial neural networks, deep learning, reinforcement learning, and computer vision.

Code
The authors provide a GitHub repository with implementations of various exercises and algorithms from the book in different programming languages.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript, and Scala available online.

Editions
The first and last editions of AIMA were published in 1995 and 2020, respectively, with four editions published in total (1995, 2003, 2009, 2020).
The following is a list of the US print editions. For other editions, the publishing date and the colors of the cover can vary.

1st edition: published in 1995 with red cover
2nd edition: published in 2003 with green cover
3rd edition: published in 2009 with blue cover
4th edition: published in 2020 with purple cover
Various editions have been translated from the original English into several languages, including at least Chinese, French, German, Hungarian, Italian, Romanian, Russian, and Serbian. However, the latest, 4th edition is available only in English, French, and Italian.

References
External links
"AIMA" (1st ed.). S Russell.
"AIMA". Computer Science Division (4th ed.). Berkeley CoE.
Pollack, Martha E. (1995-09-15). "Artificial Intelligence -- A Modern Approach -- A Review". AI Magazine. 16 (3): 73–73. doi:10.1609/aimag.v16i3.1153. ISSN 2371-9621.
Artificial consciousness, also known as machine consciousness, synthetic consciousness, or digital consciousness, is the consciousness hypothesized to be possible in artificial intelligence. It is also the corresponding field of study, which draws insights from philosophy of mind, philosophy of artificial intelligence, cognitive science and neuroscience. The same terminology can be used with the term "sentience" instead of "consciousness" when specifically designating phenomenal consciousness (the ability to feel qualia).
Some scholars believe that consciousness is generated by the interoperation of various parts of the brain; these mechanisms are labeled the neural correlates of consciousness or NCC. Some further believe that constructing a system (e.g., a computer system) that can emulate this NCC interoperation would result in a system that is conscious.

Philosophical views
As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into "access" and "phenomenal" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of "raw feels", "what it is like" or qualia.

Plausibility debate
Type-identity theorists and other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution.
In his article "Artificial Consciousness: Utopia or Real Possibility," Giorgio Buttazzo says that a common objection to artificial consciousness is that, "Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, or free will. A computer, like a washing machine, is a slave operated by its components."
For other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.

Computational foundation argument
One of the most explicit arguments for the plausibility of artificial sentience comes from David Chalmers. His proposal is roughly that the right kinds of computations are sufficient for the possession of a conscious mind. Chalmers proposes that a system implements a computation if "the causal structure of the system mirrors the formal structure of the computation", and that any system that implements certain computations is sentient.
The most controversial part of Chalmers' proposal is that mental properties are "organizationally invariant". Mental properties are of two kinds, psychological and phenomenological. Psychological properties, such as belief and perception, are those that are "characterized by their causal role". Aided by previous work, he says that "systems with the same causal topology ... will share their psychological properties".
Phenomenological properties, unlike psychological properties, are not definable in terms of their causal roles. Establishing that phenomenological properties are a consequence of a causal topology, therefore, requires argument. Chalmers provides his Dancing Qualia argument for this purpose.
Chalmers begins by assuming that his principle of organization invariance is false: that agents with identical causal organizations could have different experiences. He then asks us to conceive of changing one agent into the other by the replacement of parts (neural parts replaced by silicon, say) while preserving its causal organization. The experience of the agent under transformation would change (as the parts were replaced), but there would be no change in causal topology and therefore no means whereby the agent could "notice" the shift in experience; Chalmers considers this state of affairs an implausible reducto ad absurdum establishing that his principle of organizational invariance must almost certainly be true.
Critics of artificial sentience object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization.

Controversies
In 2022, Google engineer Blake Lemoine made a viral claim that Google's LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous. However, while philosopher Nick Bostrom states that LaMDA is unlikely to be conscious, he additionally poses the question of "what grounds would a person have for being sure about it?" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: "(In the absence of these steps), it seems like one should be maybe a little bit uncertain. [...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria."

Testing
Qualia, or phenomenological consciousness, is an inherently first-person phenomenon. Although various systems may display numerous signs of behaviors correlated with functional consciousness, there is no conceivable way in which third-person tests can have access to first-person phenomenological features. Because of that, in addition to the lack of an empirical definition of sentience, directly testing for the presence of sentience in AC may be impossible.
A well-known method for testing machine intelligence is the Turing test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.
In 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.

Ethics
If it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law). For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.
In 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an "explosion of artificial suffering".
Enforced amnesia has been proposed as a way to mitigate the risk of silent suffering in locked-in conscious AI and certain AI-adjacent biological systems like brain organoids.

Aspects of consciousness
Bernard Baars and others argue there are various aspects of consciousness necessary for a machine to be artificially conscious. The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function. Igor Aleksander suggested 12 principles for artificial consciousness: the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.

Subjective experience
Some philosophers, such as David Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Explaining why and how subjective experience arises is known as the hard problem of consciousness. AI sentience would give rise to concerns of welfare, whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.

Awareness
Awareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.
There are at least three types of awareness: agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.
Because objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.

Memory
Conscious events interact with memory systems in learning, rehearsal, and retrieval.
The IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva’s sparse distributed memory architecture.

Learning
Learning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events. Per Axel Cleeremans and Luis Jiménez, learning is defined as "a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments".

Anticipation
The ability to predict (or anticipate) foreseeable events is considered important for artificial intelligence by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction: it involves the evaluation and selection of the most appropriate "draft" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.
Relationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.

Functionalist theories of consciousness
Functionalism is a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships. Functionalism is particularly popular among philosophers.
A 2023 study suggested that current large language models probably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.

Global workspace theory
This theory analogizes the mind to a theater, with conscious thought being like material illuminated on the main stage. The brain contains many specialized processes or modules (such as those for vision, language, or memory) that operate in parallel, much of which is unconscious. Attention acts as a spotlight, bringing some of this unconscious activity into conscious awareness on the global workspace. The global workspace functions as a hub for broadcasting and integrating information, allowing it to be shared and processed across different specialized modules. For example, when reading a word, the visual module recognizes the letters, the language module interprets the meaning, and the memory module might recall associated information – all coordinated through the global workspace.

Higher-order theories of consciousness
Higher-order theories of consciousness propose that a mental state becomes conscious when it is the object of a higher-order representation, such as a thought or perception about that state. These theories argue that consciousness arises from a relationship between lower-order mental states and higher-order awareness of those states. There are several variations, including higher-order thought (HOT) and higher-order perception (HOP) theories.

Attention schema theory
In 2011, Michael Graziano and Sabine Kastler published a paper named "Human consciousness and its relationship to social neuroscience: A novel hypothesis" proposing a theory of consciousness as an attention schema. Graziano went on to publish an expanded discussion of this theory in his book "Consciousness and the Social Brain". This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well-studied body schema that tracks the spatial place of a person's body. This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one's own awareness is a schematized model of one's attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain.

Implementation proposals
Symbolic or hybrid
Learning Intelligent Distribution Agent
Stan Franklin created a cognitive architecture called LIDA that implements Bernard Baars's theory of consciousness called the global workspace theory. It relies heavily on codelets, which are "special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread." Each element of cognition, called a "cognitive cycle" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.

CLARION cognitive architecture
The CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious ("explicit") and unconscious ("implicit") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.

OpenCog
Ben Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at the Hong Kong Polytechnic University.

Connectionist
Haikonen's cognitive architecture
Pentti Haikonen considers classical rule-based computing inadequate for achieving AC: "the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers." Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes "a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be "a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection." 
Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many. A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.

Shanahan's cognitive architecture
Murray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation ("imagination").

Creativity Machine
Stephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called "Device for the Autonomous Generation of Useful Information" (DAGUI), or the so-called "Creativity Machine", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.

"Self-modeling"
Hod Lipson defines "self-modeling" as a necessary component of self-awareness or consciousness in robots. "Self-modeling" consists of a robot running an internal model or simulation of itself.

In fiction
In 2001: A Space Odyssey, the spaceship's sentient supercomputer, HAL 9000 was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading to cognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.
In Arthur C. Clarke's The City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.
In Westworld, human-like androids called "Hosts" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.
In Greg Egan's short story Learning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a "teacher" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel and remove the brain. The main character, before the surgery, endures a malfunction of the "teacher". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.

See also
References
Citations
Bibliography
Further reading
Aleksander, Igor (2017). "Machine Consciousness". In Schneider, Susan; Velmans, Max (eds.). The Blackwell Companion to Consciousness (2nd ed.). Wiley-Blackwell. pp. 93–105. doi:10.1002/9781119132363.ch7. ISBN 978-0-470-67406-2.
Baars, Bernard; Franklin, Stan (2003). "How conscious experience and working memory interact" (PDF). Trends in Cognitive Sciences. 7 (4): 166–172. doi:10.1016/s1364-6613(03)00056-1. PMID 12691765. S2CID 14185056.
Casti, John L. "The Cambridge Quintet: A Work of Scientific Speculation", Perseus Books Group, 1998
Franklin, S, B J Baars, U Ramamurthy, and Matthew Ventura. 2005. The role of consciousness in memory. Brains, Minds and Media 1: 1–38, pdf.
Haikonen, Pentti (2004), Conscious Machines and Machine Emotions, presented at Workshop on Models for Machine Consciousness, Antwerp, BE, June 2004.
McCarthy, John (1971–1987), Generality in Artificial Intelligence. Stanford University, 1971–1987.
Penrose, Roger, The Emperor's New Mind, 1989.
Sternberg, Eliezer J. (2007) Are You a Machine?: The Brain, the Mind, And What It Means to be Human. Amherst, NY: Prometheus Books.
Suzuki T., Inaba K., Takeno, Junichi (2005), Conscious Robot That Distinguishes Between Self and Others and Implements Imitation Behavior, (Best Paper of IEA/AIE2005), Innovations in Applied Artificial Intelligence, 18th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, pp. 101–110, IEA/AIE 2005, Bari, Italy, June 22–24, 2005.
Takeno, Junichi (2006), The Self-Aware Robot -A Response to Reactions to Discovery News-, HRI Press, August 2006.
Zagal, J.C., Lipson, H. (2009) "Self-Reflection in Evolutionary Robotics", Proceedings of the Genetic and Evolutionary Computation Conference, pp 2179–2188, GECCO 2009.

External links
Artefactual consciousness depiction by Professor Igor Aleksander
FOCS 2009: Manuel Blum – Can (Theoretical Computer) Science come to grips with Consciousness?
www.Conscious-Robots.com, Machine Consciousness and Conscious Robots Portal.
Artificial general intelligence (AGI) is a theoretical type of artificial intelligence (AI) that matches or surpasses human capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to general intelligence that exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI. 
Creating AGI is a primary goal of AI research and of companies such as OpenAI and Meta. A 2020 survey identified 72 active AGI R&D projects spread across 37 countries. 
The timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2024, some argue that it may be possible within years or decades, while others maintain it could take a century or longer. A minority believe it may never be achieved. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect. 
There is debate on the exact definition of AGI, and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI. AGI is a common topic in science fiction and futures studies.
Contention exists over whether AGI represents an existential risk. Many AI personalities have stated that mitigating the risk of human extinction from AI should be a global priority. Others find the development of AGI to be too remote to present such a risk.

Terminology
AGI is also known as strong AI, full AI, human-level AI or general intelligent action. However, some academic sources reserve the term "strong AI" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use "weak AI" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.
Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.
The exact definition of AGI is debated. AGI usually refers to "systems that can learn to accomplish any intellectual task that human beings can perform", but details of the definition can affect how difficult it is to achieve. OpenAI defined AGI differently, as "highly autonomous systems that outperform humans at most economically valuable work", stating in its charter that its goal is to ensure that AGI "benefits all of humanity". General intelligence can also be viewed as a continuum rather than a binary property.
A framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.

Characteristics
Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches.

Intelligence traits
However, researchers generally hold that intelligence is required to do all of the following:

reason, use strategy, solve puzzles, and make judgments under uncertainty
represent knowledge, including common sense knowledge
plan
learn
communicate in natural language
if necessary, integrate these skills in completion of any given goal
Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.
Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.

Physical traits
Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:

the ability to sense (e.g. see, hear, etc.), and
the ability to act (e.g. move and manipulate objects, change location to explore, etc.)
This includes the ability to detect and respond to hazard.

Tests for human-level AGI
Several tests meant to confirm human-level AGI have been considered, including:

The Turing Test (Turing)
Proposed by Alan Turing in his 1950 paper "Computing Machinery and Intelligence," this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.
Turing described the test as follows:
The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.
In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.
The Robot College Student Test (Goertzel)
A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.
The Employment Test (Nilsson)
A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.
The Ikea test (Marcus)
Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
The Coffee Test (Wozniak)
A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.
The Modern Turing Test (Suleyman)
An AI model is given $100,000 and has to obtain $1 million.

AI-complete problems
A problem is informally called "AI-complete" or "AI-hard" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.
There are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.
However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.

History
Classical AI
Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: "machines will be capable, within twenty years, of doing any work a man can do."
Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, "Within a generation... the problem of creating 'artificial intelligence' will substantially be solved".
Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.
However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful "applied AI". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like "carry on a casual conversation". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of "human level" artificial intelligence for fear of being labeled "wild-eyed dreamer[s]".

Narrow AI research
In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These "applied AI" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.

At the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.
However, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that "top-down" (symbolic) approaches to modeling cognition will somehow meet "bottom-up" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).

Modern artificial general intelligence research
The term "artificial general intelligence" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises "the ability to satisfy goals in a wide range of environments". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.
The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as "producing publications and preliminary results". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.
As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do.

Feasibility
As of 2023, the development and potential achievement of Artificial General Intelligence (AGI) remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that "machines will be capable, within twenty years, of doing any work a man can do". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require "unforeseeable and fundamentally unpredictable breakthroughs" and a "scientifically deep understanding of cognition". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.
A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?
Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with "never" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.
A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that "over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.
In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: "Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system." Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.
Blaise Agüera y Arcas and Peter Norvig wrote in 2023 that a significant level of general intelligence has already be achieved with frontier models. They wrote that reluctance to this view comes from four main reasons: a "healthy skepticism about metrics for AGI", an "ideological commitment to alternative AI theories or techniques", a "devotion to human (or biological) exceptionalism", or a "concern about the economic implications of AGI".
2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).
In 2024, OpenAI released o1-preview, the first of a series of models that "spend more time thinking before they respond". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.

Timescales
Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.
In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.
In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.
In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.
In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.
In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called "Project December". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.
In 2022, DeepMind developed Gato, a "general-purpose" system capable of performing more than 600 different tasks.
In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.
In 2023, the AI researcher Geoffrey Hinton stated that:

The idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.In May 2023, Demis Hassabis similarly said that "The progress in the last few years has been pretty incredible", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years. In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be "strikingly plausible".

Whole brain emulation
While the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.

Early estimates
For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).
In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a "computation" was equivalent to one "floating-point operation" – a measure used to rate current supercomputers – then 1016 "computations" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.

Current research
The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain. A supercomputer with similar computing capability as the human brain is expected in April 2024. Called "DeepSouth", it could perform 228 trillions of synaptic operations per second.

Criticisms of simulation-based approaches
The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.
A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.

Philosophical perspective
"Strong AI" as defined in philosophy
In 1980, philosopher John Searle coined the term "strong AI" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence:

Strong AI hypothesis: An artificial intelligence system can have "a mind" and "consciousness".
Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.
The first one he called "strong" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a "weak AI" machine would be precisely identical to a "strong AI" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.
In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term "strong AI" to mean "human level artificial general intelligence". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.
Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, "as long as the program works, they don't care if you call it real or a simulation." If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's "weak AI hypothesis" is equivalent to the statement "artificial general intelligence is possible". Thus, according to Russell and Norvig, "most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis." Thus, for academic AI research, "Strong AI" and "AGI" are two different things.

Consciousness
Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:

sentience (or "phenomenal consciousness"): The ability to "feel" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term "consciousness" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it "feels like" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask "what does it feel like to be a bat?" However, we are unlikely to ask "what does it feel like to be a toaster?" Nagel concludes that a bat appears to be conscious (i.e. has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.
self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the "subject of one's thought" – an operating system or debugger is able to be "aware of itself" (that is, to represent itself in the same way it represents everything else) but this is not what people typically mean when they use the term "self-awareness".
These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.
The relationship between artificial general intelligence (AGI) and phenomenal consciousness is a subject of ongoing philosophical debate, notably between the perspectives of materialism and idealism. From a materialist standpoint, consciousness arises from physical processes in the brain. Functionalists in particular argue that replicating the neural structures and functions that underlie the human mind would inherently produce consciousness in machines. Conversely, idealists hold that consciousness is fundamental and cannot be fully explained by physical processes alone. They suggest that even if an AGI could mimic human intelligence, it might not possess true consciousness unless it shares in the non-physical essence that constitutes conscious experience.

Benefits
AGI could have a wide variety of applications. If oriented towards such goals, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.
AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.
AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.

Risks
Existential risks
AGI may represent multiple types of existential risk, which are risks that threaten "the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks "an argument for proceeding with due caution", not for "abandoning AI".

Risk of loss of control and human extinction
The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.
In 2014, Stephen Hawking criticized widespread indifference:

So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.
The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be "smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.
Many scholars who are concerned about existential risk advocate for more research into solving the "control problem" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.
The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.
Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.
In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

Mass unemployment
Researchers from OpenAI estimated that "80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.
According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:

Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk considers that the automation of society will require governments to adopt a universal basic income.

See also
Notes
References
Sources
Further reading
Cukier, Kenneth, "Ready for Robots? How to Think about the Future of AI", Foreign Affairs, vol. 98, no. 4 (July/August 2019), pp. 192–98. George Dyson, historian of computing, writes (in what might be called "Dyson's Law") that "Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand." (p. 197.) Computer scientist Alex Pentland writes: "Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but they work by brute force." (p. 198.)
Gleick, James, "The Fate of Free Will" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will, Princeton University Press, 2023, 333 pp.), The New York Review of Books, vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. "Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that." (p. 30.)
Hughes-Castleberry, Kenna, "A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. "This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose." (p. 82.)
Immerwahr, Daniel, "Your Lying Eyes: People now use A.I. to generate fake videos indistinguishable from real ones. How much does it matter?", The New Yorker, 20 November 2023, pp. 54–59. "If by 'deepfakes' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. The fakes aren't deep, and the deeps aren't fake. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Their role better resembles that of cartoons, especially smutty ones." (p. 59.)
Leffer, Lauren, "The Risks of Trusting AI: We must avoid humanizing machine-learning models used in scientific research", Scientific American, vol. 330, no. 6 (June 2024), pp. 80-81.
Marcus, Gary, "Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymmied by the same old problems", Scientific American, vol. 327, no. 4 (October 2022), pp. 42–45.
Press, Eyal, "In Front of Their Faces: Does facial-recognition technology lead police to ignore contradictory evidence?", The New Yorker, 20 November 2023, pp. 20–26.
Roivainen, Eka, "AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. "Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts."
Scharre, Paul, "Killer Apps:  The Real Dangers of an AI Arms Race", Foreign Affairs, vol. 98, no. 3 (May/June 2019), pp. 135–44.  "Today's AI technologies are powerful but unreliable.  Rules-based systems cannot deal with circumstances their programmers did not anticipate.  Learning systems are limited by the data on which they were trained.  AI failures have already led to tragedy.  Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars.  In the wrong situation, AI systems go from supersmart to superdumb in an instant.  When an enemy is trying to manipulate and hack an AI system, the risks are even greater."  (p. 140.)

External links
The AGI portal maintained by Pei Wang
In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.

Definition
The field of artificial immune systems (AIS) is concerned with abstracting the structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. AIS is a sub-field of biologically inspired computing, and natural computation, with interests in machine learning and belonging to the broader field of artificial intelligence.

Artificial immune systems (AIS) are adaptive systems, inspired by theoretical immunology and observed immune functions, principles and models, which are applied to problem solving.

AIS is distinct from computational immunology and theoretical biology that are concerned with simulating immunology using computational and mathematical models towards better understanding the immune system, although such models initiated the field of AIS and continue to provide a fertile ground for inspiration. Finally, the field of AIS is not concerned with the investigation of the immune system as a substrate for computation, unlike other fields such as DNA computing.

History
AIS emerged in the mid-1980s with articles authored by Farmer, Packard and Perelson (1986) and Bersini and Varela (1990) on immune networks. However, it was only in the mid-1990s that AIS became a field in its own right. Forrest et al. (on negative selection) and Kephart et al. published their first papers on AIS in 1994, and Dasgupta conducted extensive studies on Negative Selection Algorithms. Hunt and Cooke started the works on Immune Network models in 1995; Timmis and Neal continued this work and made some improvements. De Castro & Von Zuben's and Nicosia & Cutello's work (on clonal selection) became notable in 2002. The first book on Artificial Immune Systems was edited by Dasgupta in 1999.
Currently, new ideas along AIS lines, such as danger theory and algorithms inspired by the innate immune system, are also being explored. Although some believe that these new ideas do not yet offer any truly 'new' abstract, over and above existing AIS algorithms. This, however, is hotly debated, and the debate provides one of the main driving forces for AIS development at the moment. Other recent developments involve the exploration of degeneracy in AIS models, which is motivated by its hypothesized role in open ended learning and evolution.
Originally AIS set out to find efficient abstractions of processes found in the immune system but, more recently, it is becoming interested in modelling the biological processes and in applying immune algorithms to bioinformatics problems.
In 2008, Dasgupta and Nino  published a textbook on immunological computation which presents a compendium of up-to-date work related to immunity-based techniques and describes a wide variety of applications.

Techniques
The common techniques are inspired by specific immunological theories that explain the function and behavior of the mammalian adaptive immune system.

Clonal selection algorithm: A class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen–antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.
Negative selection algorithm: Inspired by the positive and negative selection processes that occur during the maturation of T cells in the thymus called T cell tolerance. Negative selection refers to the identification and deletion (apoptosis) of self-reacting cells, that is T cells that may select for and attack self tissues. This class of algorithms are typically used for classification and pattern recognition problem domains where the problem space is modeled in the complement of available knowledge. For example, in the case of an anomaly detection domain the algorithm prepares a set of exemplar pattern detectors trained on normal (non-anomalous) patterns that model and detect unseen or anomalous patterns.
Immune network algorithms: Algorithms inspired by the idiotypic network theory proposed by Niels Kaj Jerne that describes the regulation of the immune system by anti-idiotypic antibodies (antibodies that select for other antibodies). This class of algorithms focus on the network graph structures involved where antibodies (or antibody producing cells) represent the nodes and the training algorithm involves growing or pruning edges between the nodes based on affinity (similarity in the problems representation space). Immune network algorithms have been used in clustering, data visualization, control, and optimization domains, and share properties with artificial neural networks.
Dendritic cell algorithms: The dendritic cell algorithm (DCA) is an example of an immune inspired algorithm developed using a multi-scale approach. This algorithm is based on an abstract model of dendritic cells (DCs). The DCA is abstracted and implemented through a process of examining and modeling various aspects of DC function, from the molecular networks present within the cell to the behaviour exhibited by a population of cells as a whole. Within the DCA information is granulated at different layers, achieved through multi-scale processing.

See also
Biologically inspired computing
Computational immunology
Computational intelligence
Evolutionary computation
Immunocomputing
Natural computation
Swarm intelligence
Learning classifier system
Rule-based machine learning

Notes
References
J.D. Farmer, N. Packard and A. Perelson, (1986) "The immune system, adaptation and machine learning", Physica D, vol. 2, pp. 187–204
H. Bersini, F.J. Varela, Hints for adaptive problem solving gleaned from immune networks. Parallel Problem Solving from Nature, First Workshop PPSW 1, Dortmund, FRG, October, 1990.
D. Dasgupta (Editor), Artificial Immune Systems and Their Applications, Springer-Verlag, Inc. Berlin, January 1999, ISBN 3-540-64390-7
V. Cutello and G. Nicosia (2002) "An Immunological Approach to Combinatorial Optimization Problems" Lecture Notes in Computer Science, Springer vol. 2527, pp. 361–370.
L. N. de Castro and F. J. Von Zuben, (1999) "Artificial Immune Systems: Part I -Basic Theory and Applications", School of Computing and Electrical Engineering, State University of Campinas, Brazil, No. DCA-RT 01/99.
S. Garrett (2005) "How Do We Evaluate Artificial Immune Systems?" Evolutionary Computation, vol. 13, no. 2, pp. 145–178. http://mitpress.mit.edu/journals/pdf/EVCO_13_2_145_0.pdf Archived 2011-06-29 at the Wayback Machine
V. Cutello, G. Nicosia, M. Pavone, J. Timmis (2007) An Immune Algorithm for Protein Structure Prediction on Lattice Models, IEEE Transactions on Evolutionary Computation, vol. 11, no. 1, pp. 101–117. https://web.archive.org/web/20120208130715/http://www.dmi.unict.it/nicosia/papers/journals/Nicosia-IEEE-TEVC07.pdf
Villalobos-Arias M., Coello C.A.C., Hernández-Lerma O. (2004) Convergence Analysis of a Multiobjective Artificial Immune System Algorithm. In: Nicosia G., Cutello V., Bentley P.J., Timmis J. (eds) Artificial Immune Systems. ICARIS 2004. Lecture Notes in Computer Science, vol 3239. Springer, Berlin, Heidelberg. DOI https://doi.org/10.1007/978-3-540-30220-9_19

External links
AISWeb: The Online Home of Artificial Immune Systems Information about AIS in general and links to a variety of resources including ICARIS conference series, code, teaching material and algorithm descriptions.
ARTIST: Network for Artificial Immune Systems Provides information about the UK AIS network, ARTIST. It provides technical and financial support for AIS in the UK and beyond, and aims to promote AIS projects.
Computer Immune Systems Archived 2017-02-20 at the Wayback Machine Group at the University of New Mexico led by Stephanie Forrest.
AIS: Artificial Immune Systems Group at the University of Memphis led by Dipankar Dasgupta.
IBM Antivirus Research Early work in AIS for computer security.
Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.
Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, Apple Intelligence, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."
The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.
Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the "AI boom"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.

Goals
The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.

Reasoning and problem-solving
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.
Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.

Knowledge representation
Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining "interesting" and actionable inferences from large databases), and other areas.
A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.
Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.

Planning and decision-making
An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.
In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.
In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.
Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.

Learning
Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.

Natural language processing
Natural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.
Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.

Perception
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.
The field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.

Social intelligence
Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.

General intelligence
A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.

Techniques
AI research uses a wide variety of techniques to accomplish the goals above.

Search and optimization
AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.

State space search
State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.
Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. "Heuristics" or "rules of thumb" can help prioritize choices that are more likely to reach a goal.
Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.

Local search
Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.
Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by "mutating" and "recombining" them, selecting only the fittest to survive each generation.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).

Logic
Formal logic is used for reasoning and knowledge representation.
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as "and", "or", "not" and "implies") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as "Every X is a Y" and "There are some Xs that are Ys").
Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.
Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore handle propositions that are vague and partially true.
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.

Probabilistic methods for uncertain reasoning
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.
Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).

Classifiers and statistical learning methods
The simplest AI applications can be divided into two types: classifiers (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if diamond then pick up"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the "most widely used learner" at Google, due in part to its scalability.
Neural networks are also used as classifiers.

Artificial neural networks
An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.
In feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are "close" to each other—this is especially important in image processing, where a local set of neurons must identify an "edge" before the network can identify an object.

Deep learning
Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.

GPT
Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called "hallucinations", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.
Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.

Specialized hardware and software
In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.

Applications
AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).

Health and medicine
The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.
For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.

Games
Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.

Mathematics
In mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.
Alternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind, Llemma from eleuther or Julius.
When natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks.
Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.

Finance
Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated "robot advisers" have been in use for some years.

World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: "the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I’m not sure it will unleash a new wave of [e.g., sophisticated] pension innovation."

Military
Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.
In November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.

Generative AI
In the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models, often in response to prompts.
In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.

Agents
Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.

Other industry-specific tasks
There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated "AI" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.
In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.

Ethics
AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to "solve intelligence, and then use that to solve everything else". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.

Risks and harm
Privacy and copyright
Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. 
AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.
Sensitive user data collected may include online activity records, geolocation data, video or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.
AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted "from the question of 'what they know' to the question of 'what they're doing with it'."
Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of "fair use". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include "the purpose and character of the use of the copyrighted work" and "the effect upon the potential market for the copyrighted work". Website owners who do not wish to have their content scraped can indicate it in a "robots.txt" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.

Dominance by tech giants
The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.

Substantial power needs and other environmental impacts
In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.
Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and "intelligent", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.
A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found "US power demand (is) likely to experience growth not seen in a generation…." and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.
In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).

Misinformation
YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .
In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling "authoritarian leaders to manipulate their electorates" on a large scale, among other risks.

Algorithmic bias and fairness
Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. 
On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as "gorillas" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called "sample size disparity". Google "fixed" this problem by preventing the system from labelling anything as a "gorilla". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.
COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.
A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as "race" or "gender"). The feature will correlate with other features (like "address", "shopping history" or "first name"), and the program will make the same decisions based on these features as it would on "race" or "gender". Moritz Hardt said "the most robust fact in this research area is that fairness through blindness doesn't work."
Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.
Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.
There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.

Lack of transparency
Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.
People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.
DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.
Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.

Bad actors and weaponized AI
Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.
There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.

Technological unemployment
Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.

Existential risk
It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, "spell the end of the human race". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like "self-awareness" (or "sentience" or "consciousness") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.
First, AI does not require human-like "sentience" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that "you can't fetch the coffee if you're dead." In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is "fundamentally on our side".
Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.
The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.
In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to "freely speak out about the risks of AI" without "considering how this impacts Google." He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.
In 2023, many leading AI experts issued the joint statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war".
Other researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making "human lives longer and healthier and easier." While the tools that are now being used to improve lives can also be used by bad actors, "they can also be used against the bad actors." Andrew Ng also argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests." Yann LeCun "scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction." In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.

Ethical machines and alignment
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.
Other approaches include Wendell Wallach's "artificial moral agents" and Stuart J. Russell's three principles for developing provably beneficial machines.

Open source
Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the "weights") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they can't be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.

Frameworks
Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:

Respect the dignity of individual people
Connect with other people sincerely, openly, and inclusively
Care for the wellbeing of everyone
Protect social values, justice, and the public interest
Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.
Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.
The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.

Regulation
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.
In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that "products and services using AI have more benefits than drawbacks". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it "very important", and an additional 41% thought it "somewhat important", for the federal government to regulate AI, versus 13% responding "not very important" and 8% responding "not at all important".
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.

History
The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an "electronic brain". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for "artificial neurons" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that "machine intelligence" was plausible.
The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as "astonishing": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do". In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The "AI winter", a period when obtaining funding for AI projects was difficult, followed.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.
Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into "sub-symbolic" approaches. Rodney Brooks rejected "representation" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of "connectionism", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence".
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.
For many specific tasks, other methods were abandoned.
Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.
In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI". About 800,000 "AI"-related U.S. job openings existed in 2022.

Philosophy
Defining artificial intelligence
Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?" He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people but "it is usual to have a polite convention that everyone thinks."

Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts," they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'" AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".
McCarthy defines intelligence as "the computational part of the ability to achieve goals in the world". Another AI founder, Marvin Minsky similarly describes it as "the ability to solve hard problems". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the "intelligence" of the machine—and no other philosophical discussion is required, or may not even be possible.
Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".

Evaluating approaches to AI
No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.

Symbolic AI and its limits
Symbolic AI (or "GOFAI") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.

Neat vs. scruffy
"Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.

Soft vs. hard computing
Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.

Narrow vs. general AI
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.

Machine consciousness, sentience, and mind
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on." However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.

Consciousness
David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.

Computationalism and functionalism
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.
Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds." Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.

AI welfare and rights
It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.
In 2017, the European Union considered granting "electronic personhood" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.
Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.

Future
Superintelligence and the singularity
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.
If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".
However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.

Transhumanism
Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.
Edward Fredkin argues that "artificial intelligence is the next stage in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.

In fiction
Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.

See also
AI Convention – International treatyPages displaying short descriptions of redirect targets
Artificial intelligence detection software – Software to detect AI-generated contentPages displaying short descriptions of redirect targets
Behavior selection algorithm – Algorithm that selects actions for intelligent agents
Business process automation – Automation of business processes
Case-based reasoning – Process of solving new problems based on the solutions of similar past problems
Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation
Digital immortality – Hypothetical concept of storing a personality in digital form
Emergent algorithm – Algorithm exhibiting emergent behavior
Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets
Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence
Intelligence amplification – Use of information technology to augment human intelligence
Mind uploading – Hypothetical process of digitally emulating a brain
Organoid intelligence - Use of brain cells and brain organoids for intelligent computing
Robotic process automation – Form of business process automation technology
Weak artificial intelligence – Form of artificial intelligence
Wetware computer – Computer composed of organic material
Hallucination (artificial intelligence) – Erroneous material generated by AI

Explanatory notes
References
AI textbooks
The two most widely used textbooks in 2023 (see the Open Syllabus):

Russell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.
Rich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0070087705.
These were the four of the most widely used AI textbooks in 2008:

Other textbooks:

Ertel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). ISBN 978-3319584867.
Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). ISBN 978-8894787603.

History of AI
Other sources
Further reading
External links

"Artificial Intelligence". Internet Encyclopedia of Philosophy.
Thomason, Richmond. "Logic and Artificial Intelligence". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).
A military artificial intelligence arms race is an arms race between two or more states to develop and deploy lethal autonomous weapons systems (LAWS). Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better military AI, driven by increasing geopolitical and military tensions. 
An AI arms race is sometimes placed in the context of an AI Cold War between the United States, Russia, and China.

Terminology
Lethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention. LAWS have colloquially been called "slaughterbots" or "killer robots". Broadly, any competition for superior AI is sometimes framed as an "arms race". Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages.

History
In 2014, AI specialist Steve Omohundro warned that "An autonomous weapons arms race is already taking place". According to Siemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.
China became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman of Alphabet, has predicted China will be the leading country in AI by 2025.

Risks
One risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, which may result in increased algorithmic bias. This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using "race" terminology at all in this context can exacerbate this effect.
Another potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk. In 2023, a United States Air Force official reportedly said that during a computer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.
A third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group. A US government report argued that "AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war":1, and that "global stability and nuclear deterrence could be undermined".:11

Stances toward military artificial intelligence
Russia
Russian General Viktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight. The Military-Industrial Commission of Russia has approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that "there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact", and that it is inevitable that "swarms of drones" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's "neural net" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.
In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated "Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their "technology with the rest of the world, like we are doing now with atomic and nuclear technology".
Russia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives. In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on "Robotization of the Armed Forces of the Russian Federation."
The Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that "artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit" and later noted that "the day is nearing when vehicles will get artificial intelligence." Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly "outperformed existing [crewed] combat vehicles." Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles. Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification—and, potentially, target engagement—and plans to develop a suite of AI-enabled autonomous systems.
In addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities. It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures. Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.
The Russian government has strongly rejected any ban on lethal autonomous weapon systems, suggesting that such an international ban could be ignored.

China
China is pursuing a strategic policy of military-civil fusion on AI for global technological supremacy. According to a February 2019 report by Gregory C. Allen of the Center for a New American Security, China's leadership – including paramount leader Xi Jinping – believes that being at the forefront in AI technology is critical to the future of global military and economic power competition. Chinese military officials have said that their goal is to incorporate commercial AI technology to "narrow the gap between the Chinese military and global advanced powers." The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies. An October 2021 report by the Center for Security and Emerging Technology found that "Most of the [Chinese military]'s AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010." The report estimated that Chinese military spending on AI exceeded $1.6 billion each year. The Japan Times reported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.
China published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U. N. Security Council to broach the issue. In 2018, Xi called for greater international cooperation in basic AI research. Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms. In 2019, former United States Secretary of Defense Mark Esper lashed out at China for selling drones capable of taking life with no human oversight.

United States
In 2014, former Secretary of Defense Chuck Hagel posited the "Third Offset Strategy" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, the U.S. Department of Defense (DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around $70 billion per year. The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.
The U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.
The Joint Artificial Intelligence Center (JAIC) (pronounced "jake") is an American organization on exploring the usage of AI (particularly edge computing), Network of Networks, and AI-enhanced communication, for use in actual combat. It is a subdivision of the United States Armed Forces and was created in June 2018. The organization's stated objective is to "transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools."
In 2023 Microsoft pitched the DoD to use DALL-E models to train its battlefield management system. OpenAI, the developer of DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024.

Project Maven
Project Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos, apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led by Robert O. Work who was concerned about China's military use of the emerging technology.  Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets. The project was established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017. Also known as the Algorithmic Warfare Cross Functional Team, it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project "designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department". Its chief, U.S. Marine Corps Col. Drew Cukor, said: "People and computers will work symbiotically to increase the ability of weapon systems to detect objects." Project Maven has been noted by allies, such as Australia's Ian Langford, for the ability to identify adversaries by harvesting data from sensors on UAVs and satellite. At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a "deliberate workflow process" was funded by the Department [of Defense] through its "rapid acquisition authorities" for about "the next 36 months".

United Kingdom
In 2015, the UK government opposed a ban on lethal autonomous weapons, stating that "international humanitarian law already provides sufficient regulation for this area", but that all weapons employed by UK armed forces would be "under human oversight and control".

Israel
Israel makes extensive use of AI for military applications specially during the Israel-Hamas war. The main AI systems used for target identification are the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90% accuracy rate and a database of tens of thousands.  The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home. 
Israel's Harpy anti-radar "fire and forget" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria. The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions. These robotic vehicles are used in border defense.

South Korea
The South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, "Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability", and they want to "get to a place where our software can discern whether a target is friend, foe, civilian or military".

European Union
The European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons. However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.
Some EU member states have developed and are developing automated weapons. Germany has developed an active protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond. Italy plans to incorporate autonomous weapons systems into its future military plans.

Proposals for international regulation
The international regulation of autonomous weapons is an emerging issue for international law. AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process. As early as 2007, scholars such as AI professor Noel Sharkey have warned of "an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions".
Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: "We saw in the various historical arms races that collaboration and dialog can pay dividends". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons. However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.
Many experts believe attempts to completely ban killer robots are likely to fail, in part because detecting treaty violations would be extremely difficult. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that "Preventing expanded military use of AI is likely impossible" and that "the more modest goal of safe and effective technology management must be pursued", such as banning the attaching of an AI dead man's switch to a nuclear arsenal.

Other reactions to autonomous weapons
A 2015 open letter by the Future of Life Institute calling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicist Stephen Hawking, Tesla magnate Elon Musk, Apple's Steve Wozniak and Twitter co-founder Jack Dorsey, and over 4,600 artificial intelligence researchers, including Stuart Russell, Bart Selman and Francesca Rossi. The Future of Life Institute has also released two fictional films, Slaughterbots (2017) and Slaughterbots - if human: kill() (2021), which portray threats of autonomous weapons and promote a ban, both of which went viral.
Professor Noel Sharkey of the University of Sheffield argues that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.

Disassociation
Many Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market. Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work.
For example, in June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow-up Project Maven after the current contract expired in March 2019.

See also
AI alignment
A.I. Rising
Artificial intelligence detection software
Cold War
Ethics of artificial intelligence
Existential risk from artificial general intelligence
Nuclear arms race
Post–Cold War era
Second Cold War
Space Race
Unmanned combat aerial vehicle
Weak AI

References
Further reading
Paul Scharre, "Killer Apps: The Real Dangers of an AI Arms Race", Foreign Affairs, vol. 98, no. 3 (May/June 2019), pp. 135–44. "Today's AI technologies are powerful but unreliable.  Rules-based systems cannot deal with circumstances their programmers did not anticipate.  Learning systems are limited by the data on which they were trained. AI failures have already led to tragedy. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars.  In the wrong situation, AI systems go from supersmart to superdumb in an instant.  When an enemy is trying to manipulate and hack an AI system, the risks are even greater."  (p. 140.)
The National Security Commission on Artificial Intelligence. (2019). Interim Report. Washington, DC: Author.
Artificial intelligence art is visual artwork created through the use of an artificial intelligence (AI) program.   
Artists began to create artificial intelligence art in the mid to late 20th century, when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human–AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.
The increased availability of AI art tools to the general public in the 2020s AI boom provided opportunities for creating AI generated images outside of academia and professional artists. Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment.

History
Early history
The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created around 1800 and capable of creating multiple drawings and poems stored in its "cams," the brass disks that hold memory.
In 1950, with the publication of Alan Turing's paper Computing Machinery and Intelligence, there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly. Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.

1950s to 2000s: Early implementations
Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art, computer art, digital art, or new media.
One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing. In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began to develop a way for AARON to paint as well, using special brushes and dyes that were chosen by the program itself without mediation from Cohen. After years of work, AARON was exhibited in 1972 at the Los Angeles County Museum of Art. From 1973 to 1975, Cohen refined AARON during a residency at the Artificial Intelligence Laboratory at Stanford University. In 2024, the Whitney Museum of American Art exhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.
Karl Sims has exhibited art created with artificial life since the 1980s. He received an M.S. in computer graphics from the MIT Media Lab in 1987 and was artist-in-residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company Thinking Machines. In both 1991 and 1992, Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution. In 1997, Sims created the interactive installation Galápagos for the NTT InterCommunication Center in Tokyo. In this installation, viewers help evolve 3D animated creatures by selecting which ones will be allowed to live and produce new, mutated offspring. Furthermore, Sims received an Emmy Award in 2019 for outstanding achievement in engineering development.
Eric Millikin has been creating animated films using artificial intelligence since the 1980s, and began posting art on the internet using CompuServe in the early 1980s. 
In 1999, Scott Draves and a team of several engineers created and released Electric Sheep as a free software screensaver. Electric Sheep is a volunteer computing project for animating and evolving fractal flames, which are in turn distributed to the networked computers, which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telefonica Life 4.0 prize for Electric Sheep.

2010s: Deep learning
During the deep learning era, there are mainly these types of designs for generative art: autoregressive models, diffusion models, GANs, normalizing flows.
In 2014, Ian Goodfellow and colleagues at Université de Montréal developed the generative adversarial network (GAN), a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images. The GAN uses a "generator" to create new images and a "discriminator" to decide which created images are considered successful. Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images.
In 2015, a team at Google released DeepDream, a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia. The process creates deliberately over-processed images with a dream-like appearance reminiscent of a psychedelic experience.
In 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.
Autoregressive models were used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with a recurrent neural network. Immediately after the Transformer architecture was proposed in Attention Is All You Need (2018), it was used for autoregressive generation of images, but without text conditioning.
In 2018, an auction sale of artificial intelligence art was held at Christie's Auction House in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for US$432,500, which was almost 45 times higher than its estimate of US$7,000–10,000. The artwork was created by Obvious, a Paris-based collective. The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.
In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the "interests and culture(s) of people of color." Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.

2020s
In the 2020s, text-to-image models, which generate images based on prompts, became widely used.
In 2021, using the influential large language generative pre-trained transformer models that are used in GPT-2 and GPT-3, OpenAI released a series of images created with the text-to-image AI model DALL-E 1. It was an autoregressive generative model with essentially the same architecture as GPT-3.
Later in 2021, EleutherAI released the open source VQGAN-CLIP based on OpenAI's CLIP model.
Diffusion models were proposed in 2015, but they only became better than GANs in early 2021. Latent diffusion model was published in 2021 December, and became the basis for the later Stable Diffusion (August 2022).
In 2022, Midjourney was released, followed by Google Brain's Imagen and Parti, which were announced in May 2022, Microsoft's NUWA-Infinity, and the source-available Stable Diffusion, which was released in August 2022. DALL-E 2, a successor to DALL-E, was beta-tested and released. Unlike DALL-E 1, it was a diffusion model. Stability AI has a Stable Diffusion web interface called DreamStudio, plugins for Krita, Photoshop, Blender, and GIMP, and the Automatic1111 web-based open source user interface. Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.
In 2023, Eric Millikin released The Dance of the Nain Rouge, a documentary film created using AI deepfake technology about the Detroit folklore legend of the Nain Rouge. The film is described as "an experimental decolonial Detroit demonology deepfake dream dance documentary." It was awarded the "Best Innovative Technologies Award" ("Premio Migliori Tecnologie Innovative") at the 2024 Pisa Robot Film Festival in Italy and "Best Animation Film" at the 2024 Absurd Film Festival in Italy.

Tools and processes
Imagery
There are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of) VAEs, LorAs, hypernetworks, ipadapter, and embeddings/textual inversions. Variables, including CFG, seed, steps, sampler, scheduler, denoise, upscaler, and encoder, are sometimes available for adjustment. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. Artists can also train their own models. 
In addition, procedural "rule-based" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks (GANs) and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.
There are many options, ranging from simple consumer-facing mobile apps to Jupyter notebooks and webUIs that require powerful GPUs to run effectively. Additional functionalities include "textual inversion," which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept) and model extensions or fine-tuning (such as DreamBooth).

Impact and applications
AI has the potential for a societal transformation, which may include enabling the expansion of noncommercial niche genres (such as cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, fast prototyping, increasing art-making accessibility, and artistic output per effort and/or expenses and/or time—e.g., via generating drafts, draft-refinitions, and image components (inpainting). Generated images are sometimes used as sketches, low-cost experiments, inspiration, or illustrations of proof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.

Prompt engineering and sharing
Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like "in the style of [name of an artist]" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining, and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.

Related terminology
Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years. Synthography is a proposed term for the practice of generating images that are similar to photographs using AI.

Impact
Copyright
Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century.
In 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program. A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define "author" as a natural person or a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the "work for hire" doctrine, under which ownership of a copyright is transferred to the "employer." In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship.
In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation. In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that "there are no safeguards for artists to be able to identify works in databases that are being used and opt out." Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature. In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as "Have I Been Trained?" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright, a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.
In January 2023, three artists—Sarah Andersen, Kelly McKernan, and Karla Ortiz—filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web. In July 2023, U.S. District Judge William Orrick was inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint. Also in 2023, Stability AI was sued by Getty Images for using its images in the training data. A tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.
In March 2024, Tennessee enacted the ELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission. A month later in that year, Adam Schiff introduced the Generative AI Copyright Disclosure Act which, if passed, would require that AI companies to submit copyrighted works in their datasets to the Register of Copyrights before releasing new generative AI systems.

Income and employment stability
As generative AI image software such as Stable Diffusion and DALL-E continue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen. In 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in the illustration and design industries. In August 2022, digital artist R. J. Palmer stated that "I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist." Scholars Jiang et al. state that "Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently."
AI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles. For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style. Furthermore, some training databases on which AI systems are based are not accessible to the public.
The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed. Works of AI-generated art, such as Théâtre d'Opéra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists. The Netflix short film The Dog & The Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.
AI art has sometimes been deemed to be able to replace traditional stock images. In 2023, Shutterstock announced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images. Getty Images and Nvidia have partnered with the launch of Generative AI by iStock, a model trained on Getty’s library and iStock’s photo library using Nvidia’s Picasso model.

Power usage
Researchers from Hugging Face and Carnegie Mellon University reported in a 2023 paper that generating one thousand 1024×1024 images using Stable Diffusion's XL 1.0 base model requires 11.49 kWh of energy and generates 1,594 grams (56.2 oz) of carbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6 km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9 kWh of energy per 1,000 inferences.

Reception
AI-produced images are causing artists to be concerned about AI art potentially devaluing traditionally-made art. There is also the question of whether or not the gathered data can be used to produce a work.

Deception
As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such as deepfakes. Artist Sarah Andersen, who previously had her art copied and edited to depict Neo-Nazi beliefs, stated that the spread of hate speech online can be worsened by the use of image generators. Some also generate images or videos for the purpose of catfishing.
AI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it. This mainly refers to revenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generated child pornography has been deemed a potential danger to society due to its unlawful nature.

To mitigate some deceptions, there has been a tool that tries to detect images that were generated by Dall-E.
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
After winning the 2023 "Creative" "Open competition" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had "clearly shown that even experienced photographers and art experts can be fooled". Smaller contests have been affected as well; in 2023, a contest run by author Mark Lawrence as Self-Published Fantasy Blog-Off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.
In May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image of Pope Francis wearing a white puffer coat. Additionally, an AI-generated image of an attack on the Pentagon went viral as part of a hoax news story on Twitter.
In the days before March 2023 indictment of Donald Trump as part of the Stormy Daniels–Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online. On March 20th, British journalist Eliot Higgins generated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days. According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.
In February 2024, the paper Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway was published using AI-generated images. It was later retracted from Frontiers in Cell and Developmental Biology because the paper "does not meet the standards".

Bias
Another major concern raised about AI-generated images and art is sampling bias within model training data leading towards discriminatory output from AI art models. In 2023, University of Washington researchers found evidence of racial bias within the Stable Diffusion model, with images of a "person" corresponding most frequently with images of males from Europe or North America.
In 2024, Google's chatbot Gemini's AI image generator was criticized for perceived racial bias, with claims that Gemini deliberately underrepresented white people in its results. Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as "happy white people" and "ideal nuclear family". Google later apologized for "missing the mark" and took Gemini's image generator offline for updates.

Analysis of existing art using AI
In addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Synthetic images can also be used to train AI algorithms for art authentication and to detect forgeries.
Researchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.

Other forms of art
Some prototype cooking robots can dynamically taste.
There is also AI-assisted writing beyond copy editing (such as helping with writer's block, inspiration, or rewriting segments). Generative AI has been used in video game production beyond imagery, especially for level design (e.g., for custom maps) and creating new content (e.g., quests or dialogue) or interactive stories in video games. Some AI can also generate videos, either from text, an image, or a video. This is known as a text-to-video model. Examples of this are Runway's Gen-2, OpenAI's Sora, and Google's VideoPoet.

See also


== References ==
Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, "Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world." Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters. The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption. However, it also carries risks (described below).

Uses of AI in government
The potential uses of AI in government are wide and varied, with Deloitte considering that "Cognitive technologies could eventually revolutionize every facet of government operations". Mehr suggests that six types of government problems are appropriate for AI applications:

Resource allocation - such as where administrative support is required to complete tasks more quickly.
Large datasets - where these are too large for employees to work efficiently and multiple datasets could be combined to provide greater insights.
Experts shortage - including where basic questions could be answered and niche issues can be learned.
Predictable scenario - historical data makes the situation predictable.
Procedural - repetitive tasks where inputs or outputs have a binary answer.
Diverse data - where data takes a variety of forms (such as visual and linguistic) and needs to be summarised regularly.
Mehr states that "While applications of AI in government work have not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector."
Potential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with the government; and other uses.

Contributing to public policy objectives
There are a range of examples of where AI can contribute to public policy objectives. These include:

Receiving benefits at job loss, retirement, bereavement and child birth almost immediately, in an automated way (thus without requiring any actions from citizens at all)
Social insurance service provision
Classifying emergency calls based on their urgency (like the system used by the Cincinnati Fire Department in the United States)
Detecting and preventing the spread of diseases
Assisting public servants in making welfare payments and immigration decisions
Adjudicating bail hearings
Triaging health care cases
Monitoring social media for public feedback on policies
Monitoring social media to identify emergency situations
Identifying fraudulent benefits claims
Predicting a crime and recommending optimal police presence
Predicting traffic congestion and car accidents
Anticipating road maintenance requirements
Identifying breaches of health regulations
Providing personalised education to students
Marking exam papers
Assisting with defence and national security (see Artificial intelligence § Military and Applications of artificial intelligence § Other fields in which AI methods are implemented respectively)
Assisting with policy analysis, including by using ChatGPT or a custom GPT

Assisting public interactions with government
AI can be used to assist members of the public to interact with government and access government services, for example by:

Answering questions using virtual assistants or chatbots (see below)
Directing requests to the appropriate area within government
Filling out forms
Assisting with searching documents (e.g. IP Australia's trade mark search)
Scheduling appointments
Examples of virtual assistants or chatbots being used by government include the following:

Launched in February 2016, the Australian Taxation Office has a virtual assistant on its website called "Alex". As at 30 June 2017, Alex could respond to more than 500 questions, had engaged in 1.5 million conversations and resolved over 81% of enquiries at first contact.
Australia's National Disability Insurance Scheme (NDIS) is developing a virtual assistant called "Nadia" which takes the form of an avatar using the voice of actress Cate Blanchett. Nadia is intended to assist users of the NDIS to navigate the service. Costing some $4.5 million, the project has been postponed following a number of issues. Nadia was developed using IBM Watson, however, the Australian Government is considering other platforms such as Microsoft Cortana for its further development.
The Australian Government's Department of Human Services uses virtual assistants on parts of its website to answer questions and encourage users to stay in the digital channel. As at December 2018, a virtual assistant called "Sam" could answer general questions about family, job seeker and student payments and related information. The department also introduced an internally-facing virtual assistant called "MelissHR" to make it easier for departmental staff to access human resources information.
Estonia is building a virtual assistant which will guide citizens through any interactions they have with the government. Automated and proactive services "push" services to citizens at key events of their lives (including births, bereavements, unemployment, ...). One example is the automated registering of babies when they are born.

Gerrymandering
Gerrymandering is an insidious method of influencing political process. Depending on the objective of its use, the application of artificial intelligence to redraw districts based on voter distribution and demographic datasets can either contribute to impartiality, or sustain partisan gains for interested stakeholders in the election process.

Other uses
Other uses of AI in government include:

Translation
Language interpretation pioneered by the European Commission's Directorate General for Interpretation and Florika Fink-Hooijer.
Drafting documents

Potential benefits
AI offers potential efficiencies and costs savings for the government. For example, Deloitte has estimated that automation could save US Government employees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between $3.3 billion to $41.1 billion a year. The Harvard Business Review has stated that while this may lead a government to reduce employee numbers, "Governments could instead choose to invest in the quality of its services. They can re-employ workers' time towards more rewarding work that requires lateral thinking, empathy, and creativity — all things at which humans continue to outperform even the most sophisticated AI program."

Risks
Risks associated with the use of AI in government include AI becoming susceptible to bias, a lack of transparency in how an AI application may make decisions, and the accountability for any such decisions.
AI in governance and the economic world might make the market more difficult for companies to keep up with the increases in technology. Large U.S. companies like Apple and Google are able to dominate the market with their latest and most advanced technologies. This gives them an advantage over smaller companies that do not have the means of advancing as far in the digital technology fields with AI.

See also
AI for Good
Applications of artificial intelligence
Artificial general intelligence
Civic technology
e-government
Existential risk from artificial general intelligence
Government by algorithm
Lawbot
Project Cybersyn
Regulation of artificial intelligence
Singleton (global governance)

References
Further reading
Cornish, Lisa (5 December 2018). "Bringing intelligence to government decision-making". The Mandarin. Retrieved 12 January 2019.
Garner, Catherine (4 December 2018). "Demystifying artificial intelligence". The Canberra Times. Archived from the original on 12 January 2019. Retrieved 12 January 2019.
London, Dan (6 June 2018). "Powering AI for government". CIO. Retrieved 12 January 2019.
Artificial intelligence in healthcare is the application of artificial intelligence (AI) to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data. It can also augment and exceed human capabilities by providing faster or new ways to diagnose, treat, or prevent disease. Using AI in healthcare has the potential improve predicting, diagnosing and treating diseases. Through machine learning algorithms and deep learning, AI can analyse large sets of clinical data and electronic health records and can help to diagnose the disease more quickly and precisely.
AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care.
Because radiographs are the most common imaging tests conducted in most radiology departments, the potential for AI to help with triage and interpretation of traditional radiographs (X-ray pictures) is particularly noteworthy.
As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various fields of medicine and related industries. 
Using AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases. Furthermore, new technologies brought about by AI in healthcare are often resisted by healthcare leaders, leading to slow and erratic adoption.

Applications in healthcare systems
Disease diagnosis
Accurate and early diagnosis of diseases is still a challenge in healthcare. Recognising medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy. Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analyis of mass electronic health records (EHRs). AI can help early prediction, for example, of Alzheimer's disease and dementias, by looking through large numbers of similar cases and possible treatments. 
Doctors' decision making could also be supported by AI in urgent situations, for example in the emergency department. Here AI algorithms can help prioritise more serious cases and reduce waiting time. Decision support systems augmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.
In 2023 a study reported higher satisfaction rates with ChatGPT-generated responses compared with those from physicians for medical questions posted on Reddit’s r/AskDocs. Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions, not in the context of an established patient-physician relationship.
Recent developments in statistical physics, machine learning, and inference algorithms are also being explored for their potential in improving medical diagnostic approaches.

Electronic health records
Electronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, the next step is to use artificial intelligence to interpret the records and provide new information to physicians.
One application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms. For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences. NLP algorithms consolidate these differences so that larger datasets can be analyzed. Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read. Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.
Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history. One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts. This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses. Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease. Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time. One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70–72% accuracy in predicting individualized treatment response. These methods are helpful due to the fact that the amount of online health records doubles every five years. Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.

Drug interactions
Improvements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature. Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken. To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms. Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.
Other algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports. Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.

Telemedicine
The increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications. AI can assist in caring for patients remotely by monitoring their information through sensors. A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.
Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.
Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations. Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal. Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.

Clinical applications
Cardiovascular
Artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool. Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome. Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital. Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease. Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.
A key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is noninferior to humans in interpretation of cardiac echocardiograms and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.
In cardiovascular tissue engineering and organoid studies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.

Dermatology
Medical imaging (such as X-ray and photography) is a commonly used tool in dermatology and the development of deep learning has been strongly tied to image processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses. Han et al. showed keratinocytic skin cancer detection from face photographs. Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images. Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images. A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.
According to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer. However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets. Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.
It has also been suggested that AI could be used to automatically evaluate the outcome of maxillo-facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance.

Gastroenterology
AI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early stomach cancer have shown sensitivity close to expert endoscopists.
AI can assist doctors treating ulcerative colitis in detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool  was developed to analyse digitised bowel samples (biopsies). The tool was able to distinguish with 80% accuracy between samples that show remission of colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy of pathologists.

Obstetrics and gynaecology
Artificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilised in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.

Infectious diseases
AI has shown potential in both the laboratory and clinical spheres of infectious disease medicine. During the COVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things. However there were only a few examples of AI being used directly in clinical practice during the pandemic itself. 
Other applications  of AI around infectious diseases include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.

Musculoskeletal
AI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients. Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.

Neurology
The use of AI technologies has been explored for use in the diagnosis and prognosis of Alzheimer's disease (AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs. The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative. Researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy. Generative adversarial networks are a form of deep learning that have also performed well in diagnosing AD. There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets. They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.

Oncology
AI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics. AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides.
In January 2020, Google DeepMind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans. A number of researchers, including Trevor Hastie, Joelle Pineau, and Robert Tibshirani among others, published a reply claiming that DeepMind's research publication in Nature lacked key details on methodology and code, "effectively undermin[ing] its scientific value" and making it impossible for the scientific community to confirm the work. In the MIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as "an advertisement" having little to do with science.
In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity. In 2023 a study reported the use of AI for CT-based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82% accuracy compared with 44% for lab analysis of biopsies.

Ophthalmology
Artificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness. In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm. Moreover, AI technology may be used to further improve "diagnosis rates" because of the potential to decrease detection time.

Pathology
For many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes. AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis. Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists, and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone. Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years, though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review. AI also has the potential to identify histological findings at levels beyond what the human eye can see, and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer. One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.

Primary care
Primary care has become one key development area for AI technologies. AI in primary care has been used for supporting decision making, predictive modelling, and business analytics. There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.

Psychiatry
In psychiatry, AI applications are still in a phase of proof-of-concept. Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes, chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.
Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017. Such applications outside the healthcare system raise various professional, ethical and regulatory questions. Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.
In 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.

Radiology
AI is being studied within the field of radiology to detect and diagnose diseases through computerized tomography (CT) and magnetic resonance (MR) imaging. It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers. Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated. AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality, and automatically assessing image quality. Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies. The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1) convolutional neural network-based analysis; and (2) utilization of radiomics.

Pharmacy
Industry
The trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.
A large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions. Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of "data assessment, storage, management, and analysis technologies" which are all crucial parts of the healthcare industry.
The following are examples of large companies that have contributed to AI algorithms for use in healthcare:

IBM's Watson Oncology is in development at Memorial Sloan Kettering Cancer Center and Cleveland Clinic. IBM is also working with CVS Health on AI applications in chronic disease treatment and with Johnson & Johnson on analysis of scientific papers to find new connections for drug development. In May 2017, IBM and Rensselaer Polytechnic Institute began a joint project entitled Health Empowerment by Analytics, Learning and Semantics (HEALS), to explore using AI technology to enhance healthcare.
Microsoft's Hanover project, in partnership with Oregon Health & Science University's Knight Cancer Institute, analyzes medical research to predict the most effective cancer drug treatment options for patients. Other projects include medical image analysis of tumor progression and the development of programmable cells.
Google's DeepMind platform is being used by the UK National Health Service to detect certain health risks through data collected via a mobile app. A second project with the NHS involves the analysis of medical images collected from NHS patients to develop computer vision algorithms to detect cancerous tissues.
Tencent is working on several medical systems and services. These include AI Medical Innovation System (AIMIS), an AI-powered diagnostic medical imaging service; WeChat Intelligent Healthcare; and Tencent Doctorwork
Intel's venture capital arm Intel Capital invested in 2016 in the startup Lumiata, which uses AI to identify at-risk patients and develop care options.

Neuralink has come up with a next-generation neuroprosthetic which intricately interfaces with thousands of neural pathways in the brain. Their process allows a chip, roughly the size of a quarter, to be inserted in the place of a chunk of a skull by a precision surgical robot to avoid accidental injury .
Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).
IFlytek launched a service robot "Xiao Man", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH ("Cruzr") and Softbank Robotics ("Pepper").
The Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India. Similarly, a software platform ChatBot in partnership with medtech startup Infermedica launched COVID-19 Risk Assessment ChatBot.
With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies. Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well. Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances.

Expanding care to developing nations
Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before. With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.
Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient. The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.

Regulation
While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection. These challenges of the clinical use of AI have brought about a potential need for regulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI, DECIDE-AI, CONSORT-AI) have been developed to provide recommendations on the key details that need to be reported.

Currently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR). The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States. In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.
There is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.

United Nations (WHO/ITU)
The joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.

US FDA
In January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan. This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.
According to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the "implementation of the HHS AI strategy". The OCR also has issued rules and regulations to protect the privacy of individuals’ health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals’ privacy and ethical issues related to AI in healthcare
The U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes
The European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency. With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.

Ethical concerns
Data collection
In order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology. The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.
Furthermore, the lack of current regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, Roche, a Swiss healthcare company, was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion. Naturally, this generates questions of ethical concern; Is there a monetary price that can be set for data, and should it depend on its perceived value or contributions to science? Is it fair to patients to sell their data? These concerns were addressed in a survey conducted by the Pew Research Center in 2022 that asked Americans for their opinions about the increased presence of AI in their daily lives, and the survey estimated that 37% of Americans were more concerned than excited about such increased presence, with 8% of participants specifically associating their concern with "people misusing AI". Ultimately, the current potential of artificial intelligence in healthcare is additionally hindered by  concerns about mismanagement of data collected, especially in the United States.

Automation
A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.
According to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years. However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.
Automation can provide benefits alongside doctors as well. It is expected that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not. AI will likely not completely replace healthcare workers but rather give them more time to attend to their patients. AI may avert healthcare worker burnout and cognitive overload.
Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand

Bias
Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care. A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.
There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.  Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets. Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations. Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients. In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult. However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.
A final source of bias, which has been called "label choice bias", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients. Solutions to the "label choice bias" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.

History
Research in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral. While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN, considered one of the most significant early uses of artificial intelligence in medicine. MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.
The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians. Approaches involving fuzzy set theory, Bayesian networks, and artificial neural networks, have been applied to intelligent computing systems in healthcare.
Medical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: 

Improvements in computing power resulting in faster data collection and data processing
Growth of genomic sequencing databases
Widespread implementation of electronic health record systems
Improvements in natural language processing and computer vision, enabling machines to replicate human perceptual processes
Enhanced the precision of robot-assisted surgery
Increased tree-based machine learning models that allow flexibility in establishing health predictors
Improvements in deep learning techniques and data logs for rare diseases

See also
References


== Further reading ==
Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry and business. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis  and insight discovery.
Artificial intelligence and machine learning  have become key enablers to leverage data in production in recent years due to a number of different factors: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing.

Categories
Possible applications of industrial AI and machine learning in the production domain can be divided into seven application areas:

Market & Trend Analysis
Machinery & Equipment
Intralogistics
Production Process
Supply Chain
Building
Product

Each application area can be further divided into specific application scenarios that describe concrete AI/ML scenarios in production. While some application areas have a direct connection to production processes, others cover production adjacent fields like logistics or the factory building.
An example from the application scenario Process Design & Innovation are collaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task.  Predictive and preventive maintenance through data-driven machine learning are exemplary application scenarios from the Machinery & Equipment application area.

Challenges
In contrast to entirely virtual systems, in which ML applications are already widespread today, real-world production processes are characterized by the interaction between the virtual and the physical world. Data is recorded using sensors and processed on computational entities and, if desired, actions and decisions are translated back into the physical world via actuators or by human operators. This poses major challenges for the application of ML in production engineering systems. These challenges are attributable to the encounter of process, data and model characteristics: The production domain's high reliability requirements, high risk and loss potential, the multitude of heterogeneous data sources and the non-transparency of ML model functionality impede a faster adoption of ML in real-world production processes.

In particular, production data comprises a variety of different modalities, semantics and quality. Furthermore, production systems are dynamic, uncertain and complex, and engineering and manufacturing problems are data-rich but information-sparse. Besides that, due the variety of use cases and data characteristics, problem-specific data sets are required, which are difficult to acquire, hindering both practitioners and academic researchers in this domain.

Process and Industry Characteristics
The domain of production engineering can be considered as a rather conservative industry when it comes to the adoption of advanced technology and their integration into existing processes. This is due to high demands on reliability of the production systems resulting from the potentially high economic harm of reduced process effectiveness due to e.g., additional unplanned downtime or insufficient product qualities. In addition, the specifics of machining equipment and products prevent area-wide adoptions across a variety of processes. Besides the technical reasons, the reluctant adoption of ML is fueled by a lack of IT and data science expertise across the domain.

Data Characteristics
The data collected in production processes mainly stem from frequently sampling sensors to estimate the state of a product, a process, or the environment in the real world. Sensor readings are susceptible to noise and represent only an estimate of the reality under uncertainty. Production data typically comprises multiple distributed data sources resulting in various data modalities (e.g., images from visual quality control systems, time-series sensor readings, or cross-sectional job and product information). The inconsistencies in data acquisition lead to low signal-to-noise ratios, low data quality and great effort in data integration, cleaning and management. In addition, as a result from mechanical and chemical wear of production equipment, process data is subject to various forms of data drifts.

Machine Learning Model Characteristics
ML models are considered as black-box systems given their complexity and intransparency of input-output relation. This reduces the comprehensibility of the system behavior and thus also the acceptance by plant operators. Due to the lack of transparency and the stochasticity of these models, no deterministic proof of functional correctness can be achieved complicating the certification of production equipment. Given their inherent unrestricted prediction behavior, ML models are vulnerable against erroneous or manipulated data further risking the reliability of the production system because of lacking robustness and safety. In addition to high development and deployment costs, the data drifts cause high maintenance costs, which is disadvantageous compared to purely deterministic programs.

Standard processes for data science in production
The development of ML applications – starting with the identification and selection of the use case and ending with the deployment and maintenance of the application – follows dedicated phases that can be organized in standard process models. The process models assist in structuring the development process and defining requirements that must be met in each phase to enter the next phase. The standard processes can be classified into generic and domain-specific ones. Generic standard processes (e.g., CRISP-DM, ASUM-DM, KDD, SEMMA, or Team Data Science Process) describe a generally valid methodology and are thus independent of individual domains. Domain-specific processes on the other hand consider specific peculiarities and challenges of special application areas.
The Machine Learning Pipeline in Production is a domain-specific data science methodology that is inspired by the CRISP-DM model and was specifically designed to be applied in fields of engineering and production technology. To address the core challenges of ML in engineering – process, data, and model characteristics – the methodology especially focuses on use-case assessment, achieving a common data and process understanding data integration, data preprocessing of real-world production data and the deployment and certification of real-world ML applications.

Industrial data sources
The foundation of most artificial intelligence and machine learning applications in industrial settings are comprehensive datasets from the respective fields. Those datasets act as the basis for training the employed models. In other domains, like computer vision, speech recognition or language models, extensive reference datasets (e.g. ImageNet, Librispeech, The People's Speech) and data scraped from the open internet are frequently used for this purpose. Such datasets rarely exist in the industrial context because of high confidentiality requirements  and high specificity of the data. Industrial applications of artificial intelligence are therefore often faced with the problem of data availability.
For these reasons, existing open datasets applicable to industrial applications, often originate from public institutions like governmental agencies or universities  and data analysis competitions hosted by companies. In addition to this, data sharing platforms exist. However, most of these platforms have no industrial focus and offer limited filtering abilities regarding industrial data sources.

Artificial intelligence for business education
Artificial intelligence for business education refers to the academic programs offered by universities that integrate artificial intelligence (AI) with business management principles. These programs aim to prepare students for the increasing role of AI in business, equipping them with the skills necessary to apply AI technologies to areas such as predictive analytics, supply chain optimization, and decision-making. AI for business education programs are offered at both undergraduate and graduate levels by several universities globally.

Academic Programs
Bachelor in Artificial Intelligence for Business (BAIB), Bachelor in Computer Science and Artificial Intelligence (BCSAI), Master of Science in Artificial Intelligence in Business (MS-AIB) – These are new programs that are still in their first cohorts and have yet to prove themselves in the industry. The undergraduate degrees are often offered in conjuction with a BBA as a 5-year double degree program, the undergraduate degrees are going through the acreditation processes in their respective countries.
Programs that combine AI with business studies vary by institution and degree level. Below are some notable examples:
The Bachelor in Artificial Intelligence for Business (BAIB) - This program, started by Esade focuses on the integration of AI and machine learning with core business disciplines such as management, marketing, and finance. The Esade Buisness School is a highly regarded institution for it's business inovation, sustainability focus and future-proof outlook. During the BBA+BAIB, students are trained to apply AI in business environments to improve efficiency, innovation, and decision-making.
Bachelor in Computer Science and Artificial Intelligence (BCSAI) – Offered along with a BBA by IE University, the BCSAI combines foundational studies in computer science with a specialization in artificial intelligence. The program also provides a strong grounding in business principles, preparing graduates to create AI solutions for business problems and drive technological innovation in the business world.
Master in Artificial Intelligence for Business (MS-AIB) – Arizona State University (ASU) offers a graduate-level program focused on AI applications in business environments. This degree explores advanced topics such as AI-driven decision-making, big data analysis, and the ethical implications of AI in business. The program is designed for professionals seeking to leverage AI technologies to transform business practices and improve efficiency.

Curriculum Structure
These programs typically include a combination of AI and business courses. Core subjects often cover topics such as machine learning, data science, business strategy, and financial management. The programs aim to give students a broad understanding of AI applications within a business environment, while also allowing them to specialize in areas such as supply chain management, marketing analytics, and AI-driven innovation.
In addition to technical courses, many programs include practical training, such as internships, real-world AI projects, and industry case studies. This helps students gain practical experience in applying AI tools and techniques to solve business challenges.

Accreditation
Many universities offering these degrees hold accreditation from recognized educational bodies, ensuring that their programs meet rigorous academic and industry standards. For example, ESADE and IE University are both accredited by institutions such as EQUIS and AACSB, which evaluate the quality of business education programs. Similarly, Arizona State University holds accreditation for its graduate programs in business and technology.

See also
Operational artificial intelligence
Artificial intelligence in heavy industry


== References ==
Artificial intelligence in mental health is the application of artificial intelligence (AI), computational technologies and algorithms to supplement the understanding, diagnosis, and treatment of mental health disorders. AI is becoming a ubiquitous force in everyday life which can be seen through frequent operation of models like ChatGPT. Utilizing AI in the realm of mental health signifies a form of digital healthcare, in which, the goal is to increase accessibility in a world where mental health is becoming a growing concern. Prospective ideas involving AI in mental health include identification and diagnosis of mental disorders, explication of electronic health records, creation of personalized treatment plans, and predictive analytics for suicide prevention.  Learning how to apply AI in healthcare proves to be a difficult task with many challenges, thus it remains rarely used as efforts to bridge gaps are deliberated.

Background
In 2019, 1 in every 8 people, or 970 million people around the world were living with a mental disorder, with anxiety and depressive disorders the most common. In 2020, the number of people living with anxiety and depressive disorders rose significantly because of the COVID-19 pandemic. Additionally, the prevalence of mental health and addiction disorders exhibits a nearly equal distribution across genders, emphasizing the widespread nature of the issue.
The use of AI in mental health aims to support responsive and sustainable interventions against the global challenge posed by mental health disorders. Some issues common to the mental health industry are provider shortages, inefficient diagnoses, and ineffective treatments. The AI industry sees a market in healthcare, with a focus on mental health applications, which are projected to grow substantially, from $5 billion in 2020 to an estimated $45 billion by 2026. This growth indicates a growing interest in AI's ability to address critical challenges in mental healthcare provision through the development and implementation of innovative solutions.

Types of AI in mental health
As of 2020, there was no Food and Drug Administration (FDA) approval for AI in the field of Psychiatry. There are two components of AI that are currently widely available for multiple applications, they are Machine learning (ML) and Natural language processing (NLP).

Machine learning
Machine learning is a way for a computer to learn from large datasets presented to it, without explicit instructions. It requires structured databases; unlike scientific research which begins with a hypothesis, ML begins by looking at the data and finding its own hypothesis based on the patterns that it detects. It then creates algorithms to be able to predict new information, based on the created algorithm and pattern that it was able to generate from the original dataset. This model of AI is data driven, as it requires a huge amount of structured data—an obstacle in the field of psychiatry—with a lot of its patient encounters being based on interview and storytelling on the part of the patient. Due to these limitations, some researchers have adopted a different method of developing ML models, a process named transfer learning, to be used in psychiatry based on trained models from different fields.
Transfer learning was used by researchers to develop a modified algorithm to detect alcoholism vs. non-alcoholism, and on another occasion, the same method was used to detect the signs of post-traumatic stress disorder.

Natural language processing
One of the obstacles for AI is finding or creating an organized dataset to train and develop a useful algorithm. Natural language processing can be used to create such a dataset. NLP is a way for a computer to analyze text and speech, process semantic and lexical representations, as well as recognize speech and optical characters in data. This is crucial because many of the diagnoses and DSM-5 mental health disorders are diagnosed via speech in doctor-patient interviews, utilizing the clinician's skill for behavioral pattern recognition and translating it into medically relevant information to be documented and used for diagnoses. NLP can be used to extract, organize, and structure data from patients' everyday interactions, not just during a clinical visit, raises ethical and legal concerns over consent to personal data use and data anonymization.

Applications
Diagnosis
AI with the use of NLP and ML can be used to help diagnose individuals with mental health disorders. It can be used to differentiate closely similar disorders based on their initial presentation to inform timely treatment before disease progression. For example, it may be able to differentiate unipolar from bipolar depression by analyzing imaging and medical scans. AI also has the potential to identify novel diseases that were overlooked due to the heterogeneity of presentation of a single disorder. Doctors may overlook the presentation of a disorder because while many people get diagnosed with depression, that depression may take on different forms and be enacted in different behaviors. AI can parse through the variability found in human expression data and potentially identify different types of depression.

Prognosis
AI can be used to create accurate predictions for disease progression once diagnosed. AI algorithms can also use data-driven approaches to build new clinical risk prediction models without relying primarily on current theories of psychopathology. However, internal and external validation of an AI algorithm is essential for its clinical utility. In fact, some studies have used neuroimaging, electronic health records, genetic data, and speech data to predict how depression would present in patients, their risk for suicidality or substance abuse, or functional outcomes.

Treatment
In psychiatry, in many cases multiple drugs are trialed with the patients until the correct combination or regimen is reached to effectively treat their ailment—AI could theoretically be used to predict treatment response based on observed data collected from various sources. This use of AI could bypass all the time, effort, resources needed,  and burden placed on both patients and clinicians.

Benefits
AI in mental health offers several benefits, such as:

Improving the accuracy of diagnosis: AI-based systems can analyze data from various sources, such as brain imaging and genetic tests, to identify biomarkers of mental health conditions and improve the accuracy of diagnosis.
Personalized treatment: AI-based systems can analyze data from electronic health records (EHRs), brain imaging, and genetic tests to identify the most effective treatment for specific individuals.
Improving access to mental healthcare: AI-based systems can be used to deliver mental health interventions, such as cognitive behavioral therapy, in virtual environments, which can improve access to mental healthcare in areas where access is limited.
Intelligent monitoring and early warning signs: AI-based systems can assist in recognition of mental health concerns earlier on, hence quicker turn overs in strategizing action plans and decreased chances of extreme episodes.
Chatbots and virtual assistants: AI-based systems can accelerate the rate of customer care and boost overall efficiency through task features like appointment scheduling and organization of patient background information.
Predictive analytics for suicide prevention: AI-based systems may be optimized to analyze data regarding suicide to locate trends to help better understand potential risks and probabilities in different groups of people.

Challenges
AI in mental health also poses several challenges, including:

Informed consent: AI-based systems are intricate, along with possessing biases and data-related complications. Properly informing patients of these drawbacks is crucial, though the responsibility falls in the hands of clinicians.
Right to explanation: AI-based systems may initiate patient questions or desired expounding on diagnoses or suggested treatments which must be provided to patients upon their request.
Patient privacy: AI-based systems must foster compatibility between the functionality of AI and the protection of those utilizing it to ease uneasiness towards the idea.
Insufficiency of diversity: AI-based training must be wholistic to cater towards a diverse group of patients while providing comprehensive care, rather than disproportionately representing groups or being unskillful in supporting certain populations.
Apprehension of providers and organizations: AI-based systems must be well grasped by those employed in healthcare and who serve complementarily to its functions, as a lack of accord between the two can diminish patient care.
Tarasoff Duty: Since providers who are human have a real duty to warn people in the instance of where they perceive the patient is a risk to others or themselves, questions of who would bear that responsibility arise. [1]

Current AI trends in mental health
Mental health tech startups continue to lead investment activity in digital health despite the ongoing impacts of macroeconomic factors like inflation, supply chain disruptions, and interest rates.
According to CB Insights, State of Mental Health Tech 2021 Report, mental health tech companies raised $5.5 billion worldwide (324 deals), a 139% increase from the previous year that recorded 258 deals. 
A number of startups that are using AI in mental healthcare have closed notable deals in 2022 as well. Among them is the AI chatbot Wysa (20$ million in funding), BlueSkeye that is working on improving early diagnosis (£3.4 million), the Upheal smart notebook for mental health professionals (€1.068 million), and the AI-based mental health companion clare&me (€1 million).
An analysis of the investment landscape and ongoing research suggests that we are likely to see the emergence of more emotionally intelligent AI bots and new mental health applications driven by AI prediction and detection capabilities.
For instance, researchers at Vanderbilt University Medical Center in Tennessee, US, have developed an ML algorithm that uses a person’s hospital admission data, including age, gender, and past medical diagnoses, to make an 80% accurate prediction of whether this individual is likely to take their own life. And researchers at the University of Florida are about to test their new AI platform aimed at making an accurate diagnosis in patients with early Parkinson’s disease. Research is also underway to develop a tool combining explainable AI and deep learning to prescribe personalized treatment plans for children with schizophrenia.
In January of 2024, Cedars-Sinai physician-scientists developed a first-of-its-kind program that uses immersive virtual reality and generative artificial intelligence to provide mental health support. [2] The program is called XAIA which employs a large language model programmed to resemble a human therapist. [3]
The University of Southern California is researching the effectiveness of a virtual therapist named Ellie. Through a webcam and microphone, this AI is able to process and analyze the emotional cues derived from the patient's face and the variation in expressions and tone of voice. [4]
A team of Stanford Psychologists and AI experts created "Woebot". Woebot is an app that makes therapy sessions available 24/7. WoeBot tracks its users' mood through brief daily chat conversations and offers curated videos or word games to assist users in managing their mental health. [5] A Scandinavian team of software engineers and a clinical psychologist created "Heartfelt Services". Heartfelt Services is an application meant to simulate conventional talk therapy with an AI therapist.

Criticism
AI in mental health is still an emerging field and there are still some concerns and criticisms about the use of AI in this area, such as:

Lack of data: There is a lack of data available to train AI systems, which limits their ability to accurately identify patterns in mental health conditions and predict outcomes.
Bias: AI systems can be biased if the data used to train them is biased. This can lead to inaccurate predictions and unfair treatment of certain groups of people.
Privacy: The use of AI in mental health raises concerns about privacy, as large amounts of personal data need to be collected and analyzed.
Harmful Advice: The use of AI in mental health raises concerns about harmful advice being given since it is an AI [6]; one man killed himself after a chatbot told him to "sacrifice himself" [7] and some chatbots have already been taken down due to the bad advice they gave [8]
Relationship: For decades research has consistently shown that the therapeutic relationship plays the most important role in whether and how therapy works. [9]
Empathy? Some question whether a human would experience empathy from an AI chatbot in the same way they would receive empathy from a human. As an AI has never experienced a heartbreak and does not know what addiction truly feels like, some question whether AI therapy can be considered a substitute for huma therapy

Ethical issues
Though there is a large deal of progression to be made, the incorporation of AI in mental health emphasizes a necessity for legal and regulatory frameworks as advancements are made. Constructing harmony amidst human engagement and AI is a difficult task, as there is a risk of healthcare becoming seemingly robotic and losing the humanness that has previously defined the field. Furthermore, granting patients a feeling of security and safety is a priority considering AI's reliance on individual data to perform and respond to inputs. If not approached properly, the process of trying to increase accessibility could remove elements that negatively alter patient experience with receiving mental support. To avoid veering in the wrong direction, more research should continue to develop a deeper understanding of where the incorporation of AI produces advantages and disadvantages.

See also
Artificial intelligence in healthcare
Artificial intelligence detection software
AI alignment
Artificial intelligence in healthcare
Artificial intelligence
Glossary of artificial intelligence
Clinical decision support system
Computer-aided diagnosis
Health software


== References ==
The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.
Most artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.

Integration focus
The focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.

Challenges and solutions
Collaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the "not invented here" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.
The outcome of this in A.I. is a large set of "solution islands": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:

Speech synthesis
FreeTTS from CMU
Speech recognition
Sphinx from CMU
Logical reasoning
OpenCyc from Cycorp
Open Mind Common Sense Net from MIT
With the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example,  serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled “When AI goes wrong: Fatal errors in oncological research reviewing assistance". The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics. 
Many online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with any ease.

Methodologies
Constructionist design methodology
The constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM.

Examples
ASIMO, Honda's humanoid robot, and QRIO, Sony's version of a humanoid robot.
Cog, M.I.T. humanoid robot project under the direction of Rodney Brooks.
AIBO, Sony's robot dog, integrates vision, hearing and motorskills.
TOPIO, TOSY's humanoid robot can play ping-pong with human

See also
Hybrid intelligent system, systems that combine the methods of traditional symbolic AI & that of Computational intelligence.
Neurosymbolic AI
Humanoid robots utilize systems integration intensely.
Constructionist design methodology
Cognitive architectures

References
Notes
Constructionist Design Methodology, published in A.I. magazine
MissionEngine: Multi-system integration using Python in the Tactical Language Project

External links
COG, a humanoid robot at M.I.T.
The Open Knowledge Initiative Library
In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.
An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The "signal" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.
Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.
Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.

Training
Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.

History
Early work
Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions with access to memory by a number of processors. Neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.
Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.
In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network.
In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.
R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: "Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device." However, "they dropped the subject."
Farley and Clark (1954) also used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). 
The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to "the Golden Age of AI" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.
The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962): section 16  cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.

Deep learning breakthroughs in the 1960s and 1970s
Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in Ukraine (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or "gates."
The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.
Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).
Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.

Backpropagation
Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology "back-propagating errors" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was developed multiple times in early 1970s. The earliest published instance was Seppo Linnainmaa's master thesis (1970). Paul Werbos developed it independently in 1971, but had difficulty publishing it until 1982. In 1986, David E. Rumelhart et al. popularized backpropagation.

Convolutional neural networks
Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.
The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. 
In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.
From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.

Recurrent networks
One origin of RNN was statistical mechanics. Shun'ichi Amari in 1972 proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network (1982).Another origin of RNN was neuroscience. The word "recurrent" is used to describe loop-like structures in anatomy. In 1901, Cajal observed "recurrent semicircles" in the cerebellar cortex. Hebb considered "reverberating circuit" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.
Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. 
In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the "neural sequence chunker" or "neural history compressor" which introduced the important concepts of self-supervised pre-training (the "P" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.
In 1991, Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture.
During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models.

Deep learning
Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.
In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.
In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as "deep learning".
Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.
Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it  "artificial curiosity": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.  Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).
In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the "degradation" problem. In 2015, two techniques were developed to train very deep networks: the highway network  was published in May 2015 and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. 

During the 2010s period, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.
It requires computation time that is quadratic in the size of the context window.  Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.
Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.

Models
ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.
An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.

Artificial neurons
ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.
To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.

Organization
The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.

Hyperparameter
A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.

Learning
Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.

Learning rate
The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.

Cost function
While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).

Backpropagation
Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, "no-prop" networks, training without backtracking, "weightless" networks, and non-connectionist neural networks.

Learning paradigms
Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.

Supervised learning
Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

Unsupervised learning
In unsupervised learning, input data is given along with the cost function, some function of the data 
  
    
      
        
          x
        
      
    
    {\displaystyle \textstyle x}
  
 and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model 
  
    
      
        
          f
          (
          x
          )
          =
          a
        
      
    
    {\displaystyle \textstyle f(x)=a}
  
 where 
  
    
      
        
          a
        
      
    
    {\displaystyle \textstyle a}
  
 is a constant and the cost 
  
    
      
        
          C
          =
          E
          [
          (
          x
          −
          f
          (
          x
          )
          
            )
            
              2
            
          
          ]
        
      
    
    {\displaystyle \textstyle C=E[(x-f(x))^{2}]}
  
. Minimizing this cost produces a value of 
  
    
      
        
          a
        
      
    
    {\displaystyle \textstyle a}
  
 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between 
  
    
      
        
          x
        
      
    
    {\displaystyle \textstyle x}
  
 and 
  
    
      
        
          f
          (
          x
          )
        
      
    
    {\displaystyle \textstyle f(x)}
  
, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.

Reinforcement learning
In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.
Formally the environment is modeled as a Markov decision process (MDP) with states 
  
    
      
        
          
            
              s
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              s
              
                n
              
            
          
          ∈
          S
        
      
    
    {\displaystyle \textstyle {s_{1},...,s_{n}}\in S}
  
 and actions 
  
    
      
        
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                m
              
            
          
          ∈
          A
        
      
    
    {\displaystyle \textstyle {a_{1},...,a_{m}}\in A}
  
. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution 
  
    
      
        
          P
          (
          
            c
            
              t
            
          
          
            |
          
          
            s
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(c_{t}|s_{t})}
  
, the observation distribution 
  
    
      
        
          P
          (
          
            x
            
              t
            
          
          
            |
          
          
            s
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(x_{t}|s_{t})}
  
 and the transition distribution 
  
    
      
        
          P
          (
          
            s
            
              t
              +
              1
            
          
          
            |
          
          
            s
            
              t
            
          
          ,
          
            a
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})}
  
, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.
ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.

Self-learning
Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:

  In situation s perform action a;
  Receive consequence situation s';
  Compute emotion of being in consequence situation v(s');
  Update crossbar memory w'(a,s) = w(a,s) + v(s').

The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.

Neuroevolution
Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in "dead ends".

Stochastic neural network
Stochastic neural networks originating from  Sherrington–Kirkpatrick models  are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.

Other
In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation–maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.

Modes
Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces "noise" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use "mini-batches", small batches with samples in each batch selected stochastically from the entire data set.

Types
ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be "supervised" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
Some of the main breakthroughs include: 

Convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;
Competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.

Network design
Using artificial neural networks requires an understanding of their characteristics.

Choice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc. ). Overly complex models learn slowly.
Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.
Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.
Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.

Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:

Applications
Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:

Function approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)
Data processing (including filtering, clustering, blind source separation, and compression)
Nonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)
Pattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)
Sequence recognition (including gesture, speech, and handwritten and printed text recognition)
Sensor data analysis (including image analysis)
Robotics (including directing manipulators and prostheses)
Data mining (including knowledge discovery in databases)
Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)
Quantum chemistry
General game playing
Generative AI
Data visualization
Machine translation
Social network filtering
E-mail spam filtering
Medical diagnosis
ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.
ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.
ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.
It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.
Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.

Theoretical properties
Computational power
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.
A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.

Capacity
A model's "capacity" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.

Convergence
Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.
Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.
The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.

Generalization and statistics
Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.
The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.

Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:

  
    
      
        
          y
          
            i
          
        
        =
        
          
            
              e
              
                
                  x
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  c
                
              
              
                e
                
                  
                    x
                    
                      j
                    
                  
                
              
            
          
        
      
    
    {\displaystyle y_{i}={\frac {e^{x_{i}}}{\sum _{j=1}^{c}e^{x_{j}}}}}

Criticism
Training
A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.
Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.
Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).

Theory
A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a "something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.
Technology writer Roger Bridgman commented:

Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.

Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.
Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.

Hardware
Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.
Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.
Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.

Practical counterexamples
Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.

Hybrid approaches
Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.

Dataset bias
Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exasperate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word "woman" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.

Gallery
Recent advancements and future directions
Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.

Image processing
In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.

Speech recognition
By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.

Natural language processing
In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.

Control systems
In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.

Finance
ANNs are used for stock market prediction and credit scoring: 

In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.
In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.
ANNs require high-quality data and careful tuning, and their "black-box" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.

Medicine
ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.

Content creation
ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.

See also
External links
A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.
Review of Neural Networks in Materials Science Archived 7 June 2015 at the Wayback Machine
Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)
Another introduction to ANN
Next Generation of Neural Networks Archived 24 January 2011 at the Wayback Machine - Google Tech Talks
Performance of Neural Networks
Neural Networks and Information Archived 9 July 2009 at the Wayback Machine
Sanderson G (5 October 2017). "But what is a Neural Network?". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube.

Notes
References


== Bibliography ==
An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks. The artificial neuron is a function that receives one or more inputs, applies weights to these inputs, and sums them to produce an output. 
The design of the artificial neuron was inspired by neural circuitry. Its inputs are analogous to excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites, or activation, its weights are analogous to synaptic weight, and its output is analogous to a neuron's action potential which is transmitted along its axon.
Usually, each input is separately weighted, and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.
The artificial neuron transfer function should not be confused with a linear system's transfer function.
An artificial neuron may be referred to as a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or McCulloch–Pitts (MCP) neuron, depending on the structure used.
Simple artificial neurons, such as the McCulloch–Pitts model, are sometimes described as "caricature models", since they are intended to reflect one or more neurophysiological observations, but without regard to realism. Artificial neurons can also refer to artificial cells in neuromorphic engineering (see below) that are similar to natural physical neurons.

Basic structure
For a given artificial neuron k, let there be m + 1 inputs with signals x0 through xm and weights wk0 through wkm. Usually, the x0 input is assigned the value +1, which makes it a bias input with wk0 = bk. This leaves only m actual inputs to the neuron: from x1 to xm.
The output of the kth neuron is:

  
    
      
        
          y
          
            k
          
        
        =
        φ
        
          (
          
            
              ∑
              
                j
                =
                0
              
              
                m
              
            
            
              w
              
                k
                j
              
            
            
              x
              
                j
              
            
          
          )
        
      
    
    {\displaystyle y_{k}=\varphi \left(\sum _{j=0}^{m}w_{kj}x_{j}\right)}
  

Where 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 (phi) is the  transfer function (commonly  a threshold function).

The output is analogous to the axon of a biological neuron, and its value propagates to the input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.
It has no learning process as such. Its transfer function weights are calculated and threshold value are predetermined.

McCulloch–Pitts (MCP) neuron
A MCP neuron is a kind of restricted artificial neuron which operates in discrete time-steps. Each has zero or more inputs, and are written as 
  
    
      
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},...,x_{n}}
  
. It has one output, written as 
  
    
      
        y
      
    
    {\displaystyle y}
  
. Each input can be either excitatory or inhibitory. The output can either be quiet or firing. An MCP neuron also has a threshold 
  
    
      
        b
        ∈
        {
        0
        ,
        1
        ,
        2
        ,
        .
        .
        .
        }
      
    
    {\displaystyle b\in \{0,1,2,...\}}
  
.
In a MCP neural network, all the neurons operate in synchronous discrete time-steps of 
  
    
      
        t
        =
        0
        ,
        1
        ,
        2
        ,
        3
        ,
        .
        .
        .
      
    
    {\displaystyle t=0,1,2,3,...}
  
. At time 
  
    
      
        t
        +
        1
      
    
    {\displaystyle t+1}
  
, the output of the neuron is 
  
    
      
        y
        (
        t
        +
        1
        )
        =
        1
      
    
    {\displaystyle y(t+1)=1}
  
 if the number of firing excitatory inputs is at least equal to the threshold, and no inhibitory inputs are firing; 
  
    
      
        y
        (
        t
        +
        1
        )
        =
        0
      
    
    {\displaystyle y(t+1)=0}
  
 otherwise.
Each output can be the input to an arbitrary number of neurons, including itself (that is, self-loops are possible). However, an output cannot connect more than once with a single neuron. Self-loops do not cause contradictions, since the network operates in synchronous discrete time-steps.
As a simple example, consider a single neuron with threshold 0, and a single inhibitory self-loop. Its output would oscillate between 0 and 1 at every step, acting as a "clock". 
Any finite state machine can be simulated by a MCP neural network. Furnished with an infinite tape, MCP neural networks can simulate any Turing machine.

Biological models
Artificial neurons are designed to mimic aspects of their biological counterparts. However a significant performance gap exists between biological and artificial neural networks. In particular single biological neurons in the human brain with oscillating activation function capable of learning the XOR function have been discovered. 

Dendrites – In a biological neuron, the dendrites act as the input vector. These dendrites allow the cell to receive signals from a large (>1000) number of neighboring neurons. As in the above mathematical treatment, each dendrite is able to perform "multiplication" by that dendrite's "weight value." The multiplication is accomplished by increasing or decreasing the ratio of synaptic neurotransmitters to signal chemicals introduced into the dendrite in response to the synaptic neurotransmitter. A negative multiplication effect can be achieved by transmitting signal inhibitors (i.e. oppositely charged ions) along the dendrite in response to the reception of synaptic neurotransmitters.
Soma – In a biological neuron, the soma acts as the summation function, seen in the above mathematical description. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites, the positive and negative ions are effectively added in summation, by simple virtue of being mixed together in the solution inside the cell's body.
Axon – The axon gets its signal from the summation behavior which occurs inside the soma. The opening to the axon essentially samples the electrical potential of the solution inside the soma. Once the soma reaches a certain potential, the axon will transmit an all-in signal pulse down its length. In this regard, the axon behaves as the ability for us to connect our artificial neuron to other artificial neurons.
Unlike most artificial neurons, however, biological neurons fire in discrete pulses. Each time the electrical potential inside the soma reaches a certain threshold, a pulse is transmitted down the axon. This pulsing can be translated into continuous values. The rate (activations per second, etc.) at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them. The faster a biological neuron fires, the faster nearby neurons accumulate electrical potential (or lose electrical potential, depending on the "weighting" of the dendrite that connects to the neuron that fired). It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values (often from −1 to 1).

Encoding
Research has shown that unary coding is used in the neural circuits responsible for birdsong production. The use of unary in biological networks is presumably due to the inherent simplicity of the coding. Another contributing factor could be that unary coding provides a certain degree of error correction.

Physical artificial cells
There is research and development into physical artificial neurons – organic and inorganic.
For example, some artificial neurons can receive and release dopamine (chemical signals rather than electrical signals) and communicate with natural rat muscle and brain cells, with potential for use in BCIs/prosthetics.
Low-power biocompatible memristors may enable construction of artificial neurons which function at voltages of biological action potentials and could be used to directly process biosensing signals, for neuromorphic computing and/or direct communication with biological neurons.
Organic neuromorphic circuits made out of polymers, coated with an ion-rich gel to enable a material to carry an electric charge like real neurons, have been built into a robot, enabling it to learn sensorimotorically within the real world, rather than via simulations or virtually. Moreover, artificial spiking neurons made of soft matter (polymers) can operate in biologically relevant environments and enable the synergetic communication between the artificial and biological domains.

History
The first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational model of the "nerve net" in the brain. As a transfer function, it employed a threshold, equivalent to using the Heaviside step function. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the disjunctive or the conjunctive normal form.
Researchers also soon realized that cyclic networks, with feedbacks through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.
One important and pioneering artificial neural network that used the linear threshold function was the perceptron, developed by Frank Rosenblatt. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by Bernard Widrow in 1960 – see ADALINE.
In the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the gradient descent and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model. The best known training algorithm called backpropagation has been rediscovered several times but its first development goes back to the work of Paul Werbos.

Types of transfer functions
The transfer function (activation function) of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron.  Crucially, for instance, any multilayer perceptron using a linear transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.
Below, u refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for n inputs,

  
    
      
        u
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            i
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle u=\sum _{i=1}^{n}w_{i}x_{i}}
  

where w is a vector of synaptic weights and x is a vector of inputs.

Step function
The output y of this transfer function is binary, depending on whether the input meets a specified threshold, θ. The "signal" is sent, i.e. the output is set to one, if the activation meets the threshold.

  
    
      
        y
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                  u
                  ≥
                  θ
                
              
              
                
                  0
                
                
                  
                    if 
                  
                  u
                  <
                  θ
                
              
            
            
          
        
      
    
    {\displaystyle y={\begin{cases}1&{\text{if }}u\geq \theta \\0&{\text{if }}u<\theta \end{cases}}}
  

This function is used in perceptrons and often shows up in many other models. It performs a division of the space of inputs by a hyperplane. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.

Linear combination
In this case, the output unit is simply the weighted sum of its inputs plus a bias term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as harmonic analysis, and they can all be used in neural networks with this linear neuron. The bias term allows us to make affine transformations to the data.
See: Linear transformation, Harmonic analysis, Linear filter, Wavelet, Principal component analysis, Independent component analysis, Deconvolution.

Sigmoid
A fairly simple non-linear function, the sigmoid function such as the logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It was previously commonly seen in multilayer perceptrons. However, recent work has shown sigmoid neurons to be less effective than rectified linear neurons. The reason is that the gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons, making it difficult to optimize neural networks using multiple layers of sigmoidal neurons.

Rectifier
In the context of artificial neural networks, the rectifier or ReLU (Rectified Linear Unit) is an activation function defined as the positive part of its argument:

  
    
      
        f
        (
        x
        )
        =
        
          x
          
            +
          
        
        =
        max
        (
        0
        ,
        x
        )
        ,
      
    
    {\displaystyle f(x)=x^{+}=\max(0,x),}
  

where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature with strong biological motivations and mathematical justifications. It has been demonstrated for the first time in 2011 to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, i.e., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent.
A commonly used variant of the ReLU activation function is the Leaky ReLU which allows a small, positive gradient when the unit is not active:

  
    
      
        f
        (
        x
        )
        =
        
          
            {
            
              
                
                  x
                
                
                  
                    if 
                  
                  x
                  >
                  0
                  ,
                
              
              
                
                  a
                  x
                
                
                  
                    otherwise
                  
                  .
                
              
            
            
          
        
      
    
    {\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0,\\ax&{\text{otherwise}}.\end{cases}}}
  

where x is the input to the neuron and a is a small positive constant (in the original paper the value 0.01 was used for a).

Pseudocode algorithm
The following is a simple pseudocode implementation of a single TLU which takes boolean inputs (true or false), and returns a single boolean output when activated. An object-oriented model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a boolean value.

class TLU defined as:
    data member threshold : number
    data member weights : list of numbers of size X

    function member fire(inputs : list of booleans of size X) : boolean defined as:
        variable T : number
        T ← 0
        for each i in 1 to X do
            if inputs(i) is true then
                T ← T + weights(i)
            end if
        end for each
        if T > threshold then
            return true
        else:
            return false
        end if
    end function
end class

See also
Binding neuron
Connectionism

References
Further reading
External links
Artifical [sic] neuron mimicks function of human cells
McCulloch-Pitts Neurons (Overview)
The Association for Computational Linguistics (ACL) is a scientific and professional organization for people working on natural language processing. Its namesake conference is one of the primary high impact conferences for natural language processing research, along with EMNLP. The conference is held each summer in locations where significant computational linguistics research is carried out.
It was founded in 1962, originally named the Association for Machine Translation and Computational Linguistics (AMTCL). It became the ACL in 1968. The ACL has a European (EACL), a North American (NAACL), and an Asian (AACL) chapter.

History
The ACL was founded in 1962 as the Association for Machine Translation and Computational Linguistics (AMTCL). The initial membership was about 100. In 1965, the AMTCL took over the journal Mechanical Translation and Computational Linguistics. This journal was succeeded by many other journals: the American Journal of Computational Linguistics (1974–1978, 1980–1983), and then Computational Linguistics (1984–present). Since 1988, the journal has been published for the ACL by MIT Press.
The annual meeting was first held in 1963 in conjunction with the Association for Computing Machinery National Conference. The annual meeting was, for a long time, relatively informal and did not publish anything longer than abstracts. By 1968, the society took on its current name, the Association for Computational Linguistics (ACL). The publication of the annual meeting's Proceedings of the ACL began in 1979 and gradually matured into its modern form. Many of the meetings were held in conjunction with the Linguistic Society of America, and a few with the American Society for Information Science and the Cognitive Science Society.
The United States government sponsored much research from 1989 to 1994, characterized by an increase in author retention rates and an increase in research in some key topics, such as speech recognition, in ACL. By the 21st century, it was able to maintain authors at a high rate who coalesced in a more stable arrangement around individual research topics.

Annual Meeting of the ACL
Every year, the ACL holds the Annual Meeting of the ACL. The location lies in Europe in years zero modulo three, North America in years one modulo three, and Asia–Australia in years two modulo three. In 2020, the Annual Meeting received for the first time more submissions from China than the United States.

Activities
The ACL organizes several of the top conferences and workshops in the field of computational linguistics and natural language processing. These include:

Annual Meeting of the Association for Computational Linguistics (ACL), the flagship conference of the organization
Empirical Methods in Natural Language Processing (EMNLP)
International Joint Conference on Natural Language Processing (IJCNLP), held jointly one of the other conferences on a rotating basis
Conference on Computational Natural Language Learning (CoNLL)
Lexical and Computational Semantics and Semantic Evaluation (SemEval)
Joint Conference on Lexical and Computational Semantics (*SEM)
Workshop on Statistical Machine Translation (WMT)
Besides conferences, the ACL also sponsors the journals Computational Linguistics and Transactions of the Association for Computational Linguistics (TACL). Papers and other presentations at ACL and ACL-affiliated venues are archived online in the open-access ACL Anthology.

Special Interest Groups
ACL has a large number of Special Interest Groups (SIGs), focusing on specific areas of natural language processing. Some current SIGs within ACL are:

Presidents
Each year, the ACL elects a distinguished computational linguist who becomes vice-president of the organization in the next calendar year and president one year later. Recent ACL presidents are:

See also
Sociedad Española para el Procesamiento del Lenguaje Natural (SEPLN, Spanish Association for Natural Language Processing)

References
External links
Official website
ACL Anthology
ACL Wiki
EACL
NAACL
AACL
Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
  
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
In addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
The association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.

Definition
Following the original definition by Agrawal, Imieliński, Swami the problem of association rule mining is defined as:
Let 
  
    
      
        I
        =
        {
        
          i
          
            1
          
        
        ,
        
          i
          
            2
          
        
        ,
        …
        ,
        
          i
          
            n
          
        
        }
      
    
    {\displaystyle I=\{i_{1},i_{2},\ldots ,i_{n}\}}
  
 be a set of n binary attributes called items.
Let 
  
    
      
        D
        =
        {
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ,
        …
        ,
        
          t
          
            m
          
        
        }
      
    
    {\displaystyle D=\{t_{1},t_{2},\ldots ,t_{m}\}}
  
 be a set of transactions called the database.
Each transaction in D has a unique transaction ID and contains a subset of the items in I.
A rule is defined as an implication of the form:

  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
  
, where 
  
    
      
        X
        ,
        Y
        ⊆
        I
      
    
    {\displaystyle X,Y\subseteq I}
  
.
In Agrawal, Imieliński, Swami a rule is defined only between a set and a single item, 
  
    
      
        X
        ⇒
        
          i
          
            j
          
        
      
    
    {\displaystyle X\Rightarrow i_{j}}
  
 for 
  
    
      
        
          i
          
            j
          
        
        ∈
        I
      
    
    {\displaystyle i_{j}\in I}
  
.
Every rule is composed by two different sets of items, also known as itemsets, X and Y, where X is called antecedent or left-hand-side (LHS) and Y consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement 
  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
  
 is often read as if X then Y, where the antecedent (X ) is the if and the consequent (Y) is the then. This simply implies that, in theory, whenever X occurs in a dataset, then Y will as well.

Process
Association rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.
Association rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn’t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.
There are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis. What technique you should use depends on what you are looking for with your data. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior. Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data. Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.
Benefits
There are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.
Downsides
However, Association rules also lead to many different downsides such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downside of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn’t have a good concept of data mining, this might cause them to have trouble understanding it.

ThresholdsWhen using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:
A minimum Support threshold to find all the frequent itemsets that are in the database.
A minimum Confidence threshold to the frequent itemsets found to create rules.

The Support Threshold is 30%, Confidence Threshold is 50%
The Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.
To find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size 
  
    
      
        
          2
          
            n
          
        
        −
        1
      
    
    {\displaystyle 2^{n}-1}
  
 , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support (also called anti-monotonicity). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori and Eclat) can find all frequent itemsets.

Useful Concepts
To illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is 
  
    
      
        I
        =
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
          ,
          b
          u
          t
          t
          e
          r
          ,
          b
          e
          e
          r
          ,
          d
          i
          a
          p
          e
          r
          s
          ,
          e
          g
          g
          s
          ,
          f
          r
          u
          i
          t
        
        }
      
    
    {\displaystyle I=\{\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \}}
  
.
An example rule for the supermarket could be 
  
    
      
        {
        
          b
          u
          t
          t
          e
          r
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          m
          i
          l
          k
        
        }
      
    
    {\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}
  
 meaning that if butter and bread are bought, customers also buy milk.
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
Let 
  
    
      
        X
        ,
        Y
      
    
    {\displaystyle X,Y}
  
 be itemsets, 
  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
  
 an association rule and T a set of transactions of a given database.
Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.

Support
Support is an indication of how frequently the itemset appears in the dataset.
In our example, it can be easier to explain support by writing 
  
    
      
        
          support
        
        =
        P
        (
        A
        ∩
        B
        )
        =
        
          
            
              (
              
                number of transactions containing 
              
              A
              
                 and 
              
              B
              )
            
             (total number of transactions)
          
        
      
    
    {\displaystyle {\text{support}}=P(A\cap B)={\frac {({\text{number of transactions containing }}A{\text{ and }}B)}{\text{ (total number of transactions)}}}}
  
  where A and B are separate item sets that occur at the same time in a transaction.
Using Table 2 as an example, the itemset 
  
    
      
        X
        =
        {
        
          b
          e
          e
          r
          ,
          d
          i
          a
          p
          e
          r
          s
        
        }
      
    
    {\displaystyle X=\{\mathrm {beer,diapers} \}}
  
 has a support of 1/5=0.2 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of support of X is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).
Furthermore, the itemset 
  
    
      
        Y
        =
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
          ,
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle Y=\{\mathrm {milk,bread,butter} \}}
  
 has a support of 1/5=0.2 as it appears in 20% of all transactions as well.
When using antecedents and consequents, it allows a data miner to determine the support of multiple items being bought together in comparison to the whole data set. For example, Table 2 shows that if milk is bought, then bread is bought has a support of 0.4 or 40%. This because in 2 out 5 of the transactions, milk as well as bread are bought. In smaller data sets like this example, it is harder to see a strong correlation when there are few samples, but when the data set grows larger, support can be used to find correlation between two or more products in the supermarket example.
Minimum support thresholds are useful for determining which itemsets are preferred or interesting.
If we set the support threshold to ≥0.4 in Table 3, then the 
  
    
      
        {
        
          m
          i
          l
          k
        
        }
        ⇒
        {
        
          e
          g
          g
          s
        
        }
      
    
    {\displaystyle \{\mathrm {milk} \}\Rightarrow \{\mathrm {eggs} \}}
  
 would be removed since it did not meet the minimum threshold of 0.4. Minimum threshold is used to remove samples where there is not a strong enough support or confidence to deem the sample as important or interesting in the dataset.
Another way of finding interesting samples is to find the value of (support)×(confidence); this allows a data miner to see the samples where support and confidence are high enough to be highlighted in the dataset and prompt a closer look at the sample to find more information on the connection between the items.
Support can be beneficial for finding the connection between products in comparison to the whole dataset, whereas confidence looks at the connection between one or more items and another item. Below is a table that shows the comparison and contrast between support and support × confidence, using the information from Table 4 to derive the confidence values.

The support of X with respect to T is defined as the proportion of transactions in the dataset which contains the itemset X. Denoting a transaction by 
  
    
      
        (
        i
        ,
        t
        )
      
    
    {\displaystyle (i,t)}
  
 where i is the unique identifier of the transaction and t is its itemset, the support may be written as:

  
    
      
        
          s
          u
          p
          p
          o
          r
          t
          
          o
          f
          
          X
        
        =
        
          
            
              
                |
              
              {
              (
              i
              ,
              t
              )
              ∈
              T
              :
              X
              ⊆
              t
              }
              
                |
              
            
            
              
                |
              
              T
              
                |
              
            
          
        
      
    
    {\displaystyle \mathrm {support\,of\,X} ={\frac {|\{(i,t)\in T:X\subseteq t\}|}{|T|}}}
  

This notation can be used when defining more complicated datasets where the items and itemsets may not be as easy as our supermarket example above. Other examples of where support can be used is in finding groups of genetic mutations that work collectively to cause a disease, investigating the number of subscribers that respond to upgrade offers, and discovering which products in a drug store are never bought together.

Confidence
Confidence is the percentage of all transactions satisfying X that also satisfy Y.
With respect to T, the confidence value of an association rule, often denoted as 
  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
  
, is the ratio of transactions containing both X and Y to the total amount of X values present, where X is the antecedent and Y is the consequent.
Confidence can also be interpreted as an estimate of the conditional probability 
  
    
      
        P
        (
        
          E
          
            Y
          
        
        
          |
        
        
          E
          
            X
          
        
        )
      
    
    {\displaystyle P(E_{Y}|E_{X})}
  
, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.
It is commonly depicted as:

  
    
      
        
          c
          o
          n
          f
        
        (
        X
        ⇒
        Y
        )
        =
        P
        (
        Y
        
          |
        
        X
        )
        =
        
          
            
              
                s
                u
                p
                p
              
              (
              X
              ∪
              Y
              )
            
            
              
                s
                u
                p
                p
              
              (
              X
              )
            
          
        
        =
        
          
            
              
                number of transactions containing 
              
              X
              
                 and 
              
              Y
            
            
              
                number of transactions containing 
              
              X
            
          
        
      
    
    {\displaystyle \mathrm {conf} (X\Rightarrow Y)=P(Y|X)={\frac {\mathrm {supp} (X\cup Y)}{\mathrm {supp} (X)}}={\frac {{\text{number of transactions containing }}X{\text{ and }}Y}{{\text{number of transactions containing }}X}}}
  

The equation illustrates that confidence can be computed by calculating the co-occurrence of transactions X and Y within the dataset in ratio to transactions containing only X. This means that the number of transactions in both  X and Y  is divided by those just in X .
For example, Table 2 shows the rule 
  
    
      
        {
        
          b
          u
          t
          t
          e
          r
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          m
          i
          l
          k
        
        }
      
    
    {\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}
  
 which has a confidence of 
  
    
      
        
          
            
              1
              
                /
              
              5
            
            
              1
              
                /
              
              5
            
          
        
        =
        
          
            0.2
            0.2
          
        
        =
        1.0
      
    
    {\displaystyle {\frac {1/5}{1/5}}={\frac {0.2}{0.2}}=1.0}
  
 in the dataset, which denotes that every time a customer buys butter and bread, they also buy milk. This particular example demonstrates the rule being correct 100% of the time for transactions containing both butter and bread. The rule 
  
    
      
        {
        
          f
          r
          u
          i
          t
        
        }
        ⇒
        {
        
          e
          g
          g
          s
        
        }
      
    
    {\displaystyle \{\mathrm {fruit} \}\Rightarrow \{\mathrm {eggs} \}}
  
, however, has a confidence of 
  
    
      
        
          
            
              2
              
                /
              
              5
            
            
              3
              
                /
              
              5
            
          
        
        =
        
          
            0.4
            0.6
          
        
        =
        0.67
      
    
    {\displaystyle {\frac {2/5}{3/5}}={\frac {0.4}{0.6}}=0.67}
  
. This suggests that eggs are bought 67% of the times that fruit is brought. Within this particular dataset, fruit is purchased a total of 3 times, with two of those times consisting of egg purchases.
For larger datasets, a minimum threshold, or a percentage cutoff, for the confidence can be useful for determining item relationships. When applying this method to some of the data in Table 2, information that does not meet the requirements are removed. Table 4 shows association rule examples where the minimum threshold for confidence is 0.5 (50%). Any data that does not have a confidence of at least 0.5 is omitted. Generating thresholds allow for the association between items to become stronger as the data is further researched by emphasizing those that co-occur the most. The table uses the confidence information from Table 3 to implement the Support × Confidence column, where the relationship between items via their both confidence and support, instead of just one concept, is highlighted. Ranking the rules by Support × Confidence multiples the confidence of a particular rule to its support and is often implemented for a more in-depth understanding of the relationship between the items.

Overall, using confidence in association rule mining is great way to bring awareness to data relations. Its greatest benefit is highlighting the relationship between particular items to one another within the set, as it compares co-occurrences of items to the total occurrence of the antecedent in the specific rule. However, confidence is not the optimal method for every concept in association rule mining. The disadvantage of using it is that it does not offer multiple difference outlooks on the associations. Unlike support, for instance, confidence does not provide the perspective of relationships between certain items in comparison to the entire dataset, so while milk and bread, for example, may occur 100% of the time for confidence, it only has a support of 0.4 (40%). This is why it is important to look at other viewpoints, such as Support × Confidence, instead of solely relying on one concept incessantly to define the relationships.

Lift
The lift of a rule is defined as:

  
    
      
        
          l
          i
          f
          t
        
        (
        X
        ⇒
        Y
        )
        =
        
          
            
              
                s
                u
                p
                p
              
              (
              X
              ∪
              Y
              )
            
            
              
                s
                u
                p
                p
              
              (
              X
              )
              ×
              
                s
                u
                p
                p
              
              (
              Y
              )
            
          
        
      
    
    {\displaystyle \mathrm {lift} (X\Rightarrow Y)={\frac {\mathrm {supp} (X\cup Y)}{\mathrm {supp} (X)\times \mathrm {supp} (Y)}}}
  

or the ratio of the observed support to that expected if X and Y were independent.
For example, the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
  
 has a lift of 
  
    
      
        
          
            0.2
            
              0.4
              ×
              0.4
            
          
        
        =
        1.25
      
    
    {\displaystyle {\frac {0.2}{0.4\times 0.4}}=1.25}
  
.
If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.
If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.
If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.
The value of lift is that it considers both the support of the rule and the overall data set.

Conviction
The conviction of a rule is defined as 
  
    
      
        
          c
          o
          n
          v
        
        (
        X
        ⇒
        Y
        )
        =
        
          
            
              1
              −
              
                s
                u
                p
                p
              
              (
              Y
              )
            
            
              1
              −
              
                c
                o
                n
                f
              
              (
              X
              ⇒
              Y
              )
            
          
        
      
    
    {\displaystyle \mathrm {conv} (X\Rightarrow Y)={\frac {1-\mathrm {supp} (Y)}{1-\mathrm {conf} (X\Rightarrow Y)}}}
  
.
For example, the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
  
 has a conviction of 
  
    
      
        
          
            
              1
              −
              0.4
            
            
              1
              −
              0.5
            
          
        
        =
        1.2
      
    
    {\displaystyle {\frac {1-0.4}{1-0.5}}=1.2}
  
, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
  
 would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.

Alternative measures of interestingness
In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:

All-confidence
Collective strength
Leverage
Several more measures are presented and compared by Tan et al. and by Hahsler. Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."

History
The concept of association rules was popularized particularly due to the 1993 article of Agrawal et al., which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called "association rules" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr Hájek et al.
An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with 
  
    
      
        
          s
          u
          p
          p
        
        (
        X
        )
      
    
    {\displaystyle \mathrm {supp} (X)}
  
 and 
  
    
      
        
          c
          o
          n
          f
        
        (
        X
        ⇒
        Y
        )
      
    
    {\displaystyle \mathrm {conf} (X\Rightarrow Y)}
  
 greater than user defined constraints.

Statistically sound associations
One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.

Algorithms
Many algorithms for generating association rules have been proposed.
Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

Apriori algorithm
Apriori is given by R. Agrawal and R. Srikant in 1994 for frequent item set mining and association rule learning. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often. The name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.

Overview: Apriori uses a "bottom up" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length  from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.
Example: Assume that each row is a cancer sample with a certain combination of mutations labeled by a character in the alphabet. For example a row could have {a, c} which means it is affected by mutation 'a' and mutation 'c'. 

Now we will generate the frequent item set by counting the number of occurrences of each character. This is also known as finding the support values. Then we will prune the item set by picking a minimum support threshold. For this pass of the algorithm we will pick 3. 

Since all support values are three or above there is no pruning. The frequent item set is {a}, {b}, {c}, and {d}. After this we will repeat the process by counting pairs of mutations in the input set. 

Now we will make our minimum support value 4 so only {a, d} will remain after pruning. Now we will use the frequent item set to make combinations of triplets.  We will then repeat the process by counting occurrences of triplets of mutations in the input set. 

Since we only have one item the next set of combinations of quadruplets is empty so the algorithm will stop.
Advantages and Limitations:
Apriori has some limitations. Candidate generation can result in large candidate sets. For example a 10^4 frequent 1-itemset will generate a 10^7 candidate 2-itemset. The algorithm also needs to frequently scan the database, to be specific n+1 scans where n is the length of the longest pattern. Apriori is slower than the Eclat algorithm. However, Apriori performs well compared to Eclat when the dataset is large. This is because in the Eclat algorithm if the dataset is too large the tid-lists become too large for memory. FP-growth outperforms the Apriori and Eclat. This is due to the FP-growth algorithm not having candidate generation or test, using a compact data structure, and only having one database scan.

Eclat algorithm
Eclat (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.
To illustrate this, let there be a frequent itemset {a, b, c}. a DFS may check the nodes in the frequent itemset lattice in the following order: {a} → {a, b} → {a, b, c}, at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. BFS would explore each subset of {a, b, c} before finally checking it. As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.
It is suitable for both sequential as well as parallel execution with locality-enhancing properties.

FP-growth algorithm
FP stands for frequent pattern.
In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.
Items in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.
Items in each transaction that do not meet the minimum support requirement are discarded.
If many transactions share most frequent items, the FP-tree provides high compression close to tree root.
Recursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).
Growth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item 
  
    
      
        I
      
    
    {\displaystyle I}
  
.
A new conditional tree is created which is the original FP-tree projected onto 
  
    
      
        I
      
    
    {\displaystyle I}
  
. The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on 
  
    
      
        I
      
    
    {\displaystyle I}
  
 meet the minimum support threshold. The resulting paths from root to 
  
    
      
        I
      
    
    {\displaystyle I}
  
 will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.
Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.

Others
ASSOC
The ASSOC procedure is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.

OPUS search
OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support. Initially used to find rules for a fixed consequent it has subsequently been extended to find rules with any item as a consequent. OPUS search is the core technology in the popular Magnum Opus association discovery system.

Lore
A famous story about association rule mining is the "beer and diaper" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true. Daniel Powers says:

In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.

Other types of association rule mining
Multi-Relation Association Rules (MRAR): These are association rules where each item may have several relations. These relations indicate indirect relationships between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 
  
    
      
        
        ⟹
        
      
    
    {\displaystyle \implies }
  
 their health condition is good”. Such association rules can be extracted from RDBMS data or semantic web data.
Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.
Weighted class learning is another form of associative learning where weights may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.
High-order pattern discovery facilitates the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.

K-optimal pattern discovery provides an alternative to the standard approach to association rule learning which requires that each pattern appear frequently in the data.
Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.
Generalized Association Rules hierarchical taxonomy (concept hierarchy)
Quantitative Association Rules categorical and quantitative data
Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
Sequential pattern mining  discovers subsequences that are common to more than minsup (minimum support threshold) sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.
Subspace Clustering, a specific type of clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.
Warmr, shipped as part of the ACE data mining suite, allows association rule learning for first order relational rules.

See also
Sequence mining
Production system (computer science)
Learning classifier system
Rule-based machine learning

References
Bibliographies
Annotated Bibliography on Association Rules Archived 2017-02-19 at the Wayback Machine by M. Hahsler
Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies. The field is closely related to astrostatistics.

Background
Astroinformatics is primarily focused on developing the tools, methods, and applications of computational science, data science, machine learning, and statistics for research and education in data-oriented astronomy. Early efforts in this direction included data discovery, metadata standards development, data modeling, astronomical data dictionary development, data access, information retrieval, data integration, and data mining in the astronomical Virtual Observatory initiatives. Further development of the field, along with astronomy community endorsement, was presented to the National Research Council (United States) in 2009 in the astroinformatics "state of the profession" position paper for the 2010 Astronomy and Astrophysics Decadal Survey. That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper Astroinformatics: Data-Oriented Astronomy Research and Education.
Astroinformatics as a distinct field of research was inspired by work in the fields of Geoinformatics, Cheminformatics, Bioinformatics, and through the eScience work of Jim Gray (computer scientist) at Microsoft Research, whose legacy was remembered and continued through the Jim Gray eScience Awards.
Although the primary focus of astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to digitize historical and recent astronomical observations and images in a large database for efficient retrieval through web-based interfaces. Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.
Astroinformatics is described as the "fourth paradigm" of astronomical research. There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science. Data mining and machine learning play significant roles in astroinformatics as a scientific research discipline due to their focus on "knowledge discovery from data" (KDD) and "learning from data".
The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the Large Synoptic Survey Telescope and into the exabytes with the Square Kilometre Array. This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part due to this, data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing information-intensive and data-intensive sub-disciplines to an extent that these sub-disciplines are now becoming (or have already become) standalone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, such programs most likely will be developed in the near future.
Informatics has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., taxonomies, ontologies, folksonomies, and/or collaborative tagging) plus Astrostatistics will also be heavily involved. Citizen science projects (such as Galaxy Zoo) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.
In 2012, two position papers were presented to the Council of the American Astronomical Society that led to the establishment of formal working groups in astroinformatics and Astrostatistics for the profession of astronomy within the US and elsewhere.
Astroinformatics provides a natural context for the integration of education and research. The experience of research can now be implemented within the classroom to establish and grow data literacy through the easy re-use of data. It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.

Conferences
Additional conferences and conference lists:

See also
Astronomy and Computing
Astrophysics Data System
Astrophysics Source Code Library
Astrostatistics
Committee on Data for Science and Technology
Data-driven astronomy
Galaxy Zoo
International Astrostatistics Association
International Virtual Observatory Alliance (IVOA)
MilkyWay@home
Virtual Observatory
WorldWide Telescope
Zooniverse

References
External links
International AstroInformatics Association (IAIA)
Astronomical Data Analysis Software and Systems (ADASS)
Astrostatistics and Astroinformatics Portal
Cosmostatistics Initiative (COIN)
Astroinformatics and Astrostatistics Commission of the International Astronomical Union
Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by "soft" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.
Unlike "hard" weights, which are computed during the backwards training pass, "soft" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.
Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.

History
Academic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner.

Predecessors
Selective attention in humans had been well studied in neuroscience and cognitive psychology. In 1953, Colin Cherry studied selective attention in the context of audition, known as the cocktail party effect.
In 1958, Donald Broadbent proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperling's partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, insofar as the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve the entire visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene.
These research developments inspired algorithms such as the Neocognitron and its variants. Meanwhile, developments in neural networks had inspired circuit models of biological visual attention. One well-cited network from 1998, for example, was inspired by the low-level primate visual system. It produced saliency maps of images using handcrafted (not learned) features, which were then used to guide a second neural network in processing patches of the image in order of reducing saliency.
A key aspect of attention mechanism can be written (schematically) as 
  
    
      
        
          ∑
          
            i
          
        
        ⟨
        (
        
          query
        
        
          )
          
            i
          
        
        ,
        (
        
          key
        
        
          )
          
            i
          
        
        ⟩
        (
        
          value
        
        
          )
          
            i
          
        
      
    
    {\displaystyle \sum _{i}\langle ({\text{query}})_{i},({\text{key}})_{i}\rangle ({\text{value}})_{i}}
  
where the angled brackets denote dot product. This shows that it involves a multiplicative operation. Multiplicative operations within neural networks had been studied under the names of higher-order neural networks, multiplication units, sigma-pi units, fast weight controllers, and hyper-networks.

Recurrent attention
During the deep learning era, attention mechanism was developed to solve similar problems in encoding-decoding.
In machine translation, the seq2seq model, as it was proposed in 2014, would encode an input text into a fixed-length vector, which would then be decoded into an output text. If the input text is long, the fixed-length vector would be unable to carry enough information for accurate decoding. An attention mechanism was proposed to solve this problem.
An image captioning model was proposed in 2015, citing inspiration from the seq2seq model. that would encode an input image into a fixed-length vector. (Xu et al 2015), citing (Bahdanau et al 2014), applied the attention mechanism as used in the seq2seq model to image captioning.

Transformer
One problem with seq2seq models was their use of recurrent neural networks, which are not parallelizable as both the encoder and the decoder must process the sequence token-by-token. Decomposable attention attempted to solve this problem by processing the input sequence in parallel, before computing a "soft alignment matrix" (alignment is the terminology used by Bahdanau et al) in order to allow for parallel processing.
The idea of using the attention mechanism for self-attention, instead of in an encoder-decoder (cross-attention), was also proposed during this period, such as in differentiable neural computers and neural Turing machines. It was termed intra-attention where an LSTM is augmented with a memory network as it encodes an input sequence.
These strands of development were brought together in 2017 with the Transformer architecture, published in the Attention Is All You Need paper.

Machine translation
In neural machine translation, the seq2seq method developed in the early 2010s uses two neural networks. An encoder network encodes an input sentence into numerical vectors, which a decoder network decodes into an output sentence in another language. During the evolution of seq2seq in the 2014-2017 period, the attention mechanism was refined, until it appeared in the Transformer in 2017.

seq2seq machine translation
Consider the seq2seq language English-to-French translation task. To be concrete, let us consider the translation of "the zone of international control <end>", which should translate to "la zone de contrôle international <end>". Here, we use the special <end> token as a control character to delimit the end of input for both the encoder and the decoder.
An input sequence of text 
  
    
      
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        ,
        …
      
    
    {\displaystyle x_{0},x_{1},\dots }
  
 is processed by a neural network (which can be an LSTM, a Transformer encoder, or some other network) into a sequence of real-valued vectors 
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
, where 
  
    
      
        h
      
    
    {\displaystyle h}
  
 stands for "hidden vector".
After the encoder has finished processing, the decoder starts operating over the hidden vectors, to produce an output sequence 
  
    
      
        
          y
          
            0
          
        
        ,
        
          y
          
            1
          
        
        ,
        …
      
    
    {\displaystyle y_{0},y_{1},\dots }
  
, autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word:

(
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
, "<start>") → "la"
(
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
, "<start> la") → "la zone"
(
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
, "<start> la zone") → "la zone de"
...
(
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
, "<start> la zone de contrôle international") → "la zone de contrôle international <end>"
Here, we use the special <start> token as a control character to delimit the start of input for the decoder. The decoding terminates as soon as "<end>" appears in the decoder output.

Attention weights
In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. In the I love you example above, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t', and aime yields an alignment matrix:

Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, "soft" attention weights work better than "hard" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than "the best one", as there may not be a best hidden vector.
This view of the attention weights addresses some of the neural network explainability problem. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime.

Attention weights
As hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors: key, query, and value. The rough idea is that we have a "database" in the form of a list of key-value pairs. The decoder send in a query, and obtain a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key.
The decoder first processes the "<start>" input partially, to obtain an intermediate vector 
  
    
      
        
          h
          
            0
          
          
            d
          
        
      
    
    {\displaystyle h_{0}^{d}}
  
, the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W^{Q}}
  
 into a query vector 
  
    
      
        
          q
          
            0
          
        
        =
        
          h
          
            0
          
          
            d
          
        
        
          W
          
            Q
          
        
      
    
    {\displaystyle q_{0}=h_{0}^{d}W^{Q}}
  
. Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W^{K}}
  
 into key vectors 
  
    
      
        
          k
          
            0
          
        
        =
        
          h
          
            0
          
        
        
          W
          
            K
          
        
        ,
        
          k
          
            1
          
        
        =
        
          h
          
            1
          
        
        
          W
          
            K
          
        
        ,
        …
      
    
    {\displaystyle k_{0}=h_{0}W^{K},k_{1}=h_{1}W^{K},\dots }
  
. The linear maps are useful for providing the model with enough freedom to find the best way to represent the data.
Now, the query and keys are compared by taking dot products: 
  
    
      
        
          q
          
            0
          
        
        
          k
          
            0
          
          
            T
          
        
        ,
        
          q
          
            0
          
        
        
          k
          
            1
          
          
            T
          
        
        ,
        …
      
    
    {\displaystyle q_{0}k_{0}^{T},q_{0}k_{1}^{T},\dots }
  
. Ideally, the model should have learned to compute the keys and values, such that 
  
    
      
        
          q
          
            0
          
        
        
          k
          
            0
          
          
            T
          
        
      
    
    {\displaystyle q_{0}k_{0}^{T}}
  
 is large, 
  
    
      
        
          q
          
            0
          
        
        
          k
          
            1
          
          
            T
          
        
      
    
    {\displaystyle q_{0}k_{1}^{T}}
  
 is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest.
In order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over 
  
    
      
        0
        ,
        1
        ,
        …
      
    
    {\displaystyle 0,1,\dots }
  
. This can be accomplished by the softmax function, thus giving us the attention weights:
  
    
      
        (
        
          w
          
            00
          
        
        ,
        
          w
          
            01
          
        
        ,
        …
        )
        =
        
          s
          o
          f
          t
          m
          a
          x
        
        (
        
          q
          
            0
          
        
        
          k
          
            0
          
          
            T
          
        
        ,
        
          q
          
            0
          
        
        
          k
          
            1
          
          
            T
          
        
        ,
        …
        )
      
    
    {\displaystyle (w_{00},w_{01},\dots )=\mathrm {softmax} (q_{0}k_{0}^{T},q_{0}k_{1}^{T},\dots )}
  
This is then used to compute the context vector:
  
    
      
        
          c
          
            0
          
        
        =
        
          w
          
            00
          
        
        
          v
          
            0
          
        
        +
        
          w
          
            01
          
        
        
          v
          
            1
          
        
        +
        ⋯
      
    
    {\displaystyle c_{0}=w_{00}v_{0}+w_{01}v_{1}+\cdots }
  
where 
  
    
      
        
          v
          
            0
          
        
        =
        
          h
          
            0
          
        
        
          W
          
            V
          
        
        ,
        
          v
          
            1
          
        
        =
        
          h
          
            1
          
        
        
          W
          
            V
          
        
        ,
        …
      
    
    {\displaystyle v_{0}=h_{0}W^{V},v_{1}=h_{1}W^{V},\dots }
  
 are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices 
  
    
      
        
          W
          
            Q
          
        
        ,
        
          W
          
            K
          
        
        ,
        
          W
          
            V
          
        
      
    
    {\displaystyle W^{Q},W^{K},W^{V}}
  
, the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same.
This is the dot-attention mechanism. The particular version described in this section is "decoder cross-attention", as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus "cross-attention".
More succinctly, we can write it as
  
    
      
        
          c
          
            0
          
        
        =
        
          A
          t
          t
          e
          n
          t
          i
          o
          n
        
        (
        
          h
          
            0
          
          
            d
          
        
        
          W
          
            Q
          
        
        ,
        H
        
          W
          
            K
          
        
        ,
        H
        
          W
          
            V
          
        
        )
        =
        
          s
          o
          f
          t
          m
          a
          x
        
        (
        (
        
          h
          
            0
          
          
            d
          
        
        
          W
          
            Q
          
        
        )
        
        (
        H
        
          W
          
            K
          
        
        
          )
          
            T
          
        
        )
        (
        H
        
          W
          
            V
          
        
        )
      
    
    {\displaystyle c_{0}=\mathrm {Attention} (h_{0}^{d}W^{Q},HW^{K},HW^{V})=\mathrm {softmax} ((h_{0}^{d}W^{Q})\;(HW^{K})^{T})(HW^{V})}
  
where the matrix 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is the matrix whose rows are 
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
. Note that the querying vector, 
  
    
      
        
          h
          
            0
          
          
            d
          
        
      
    
    {\displaystyle h_{0}^{d}}
  
, is not necessarily the same as the key-value vector 
  
    
      
        
          h
          
            0
          
        
      
    
    {\displaystyle h_{0}}
  
. In fact, it is theoretically possible for query, key, and value vectors to all be different, though that is rarely done in practice.

Self-attention
Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences.
For encoder self-attention, we can start with a simple encoder without self-attention, such as an "embedding layer", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors 
  
    
      
        
          h
          
            0
          
        
        ,
        
          h
          
            1
          
        
        ,
        …
      
    
    {\displaystyle h_{0},h_{1},\dots }
  
. These can then be applied to a dot-product attention mechanism, to obtain
  
    
      
        
          
            
              
                
                  h
                  
                    0
                  
                  ′
                
              
              
                
                =
                
                  A
                  t
                  t
                  e
                  n
                  t
                  i
                  o
                  n
                
                (
                
                  h
                  
                    0
                  
                
                
                  W
                  
                    Q
                  
                
                ,
                H
                
                  W
                  
                    K
                  
                
                ,
                H
                
                  W
                  
                    V
                  
                
                )
              
            
            
              
                
                  h
                  
                    1
                  
                  ′
                
              
              
                
                =
                
                  A
                  t
                  t
                  e
                  n
                  t
                  i
                  o
                  n
                
                (
                
                  h
                  
                    1
                  
                
                
                  W
                  
                    Q
                  
                
                ,
                H
                
                  W
                  
                    K
                  
                
                ,
                H
                
                  W
                  
                    V
                  
                
                )
              
            
            
              
              
                
                ⋯
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}h_{0}'&=\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\h_{1}'&=\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\&\cdots \end{aligned}}}
  
or more succinctly, 
  
    
      
        
          H
          ′
        
        =
        
          A
          t
          t
          e
          n
          t
          i
          o
          n
        
        (
        H
        
          W
          
            Q
          
        
        ,
        H
        
          W
          
            K
          
        
        ,
        H
        
          W
          
            V
          
        
        )
      
    
    {\displaystyle H'=\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}
  
. This can be applied repeatedly, to obtain a multilayered encoder. This is the "encoder self-attention", sometimes called the "all-to-all attention", as the vector at every position can attend to every other.

Masking
For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights 
  
    
      
        
          w
          
            i
            j
          
        
        =
        0
      
    
    {\displaystyle w_{ij}=0}
  
 for all 
  
    
      
        i
        <
        j
      
    
    {\displaystyle i<j}
  
, called "causal masking". This attention mechanism is the "causally masked self-attention".

General attention
In general, the attention unit consists of dot products, with 3 trained, fully-connected neural network layers called query, key, and value.

Variants
Many variants of attention implement soft weights, such as

fast weight programmers, or fast weight controllers (1992). A "slow" neural network outputs the "fast" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as "linearized self-attention".
Bahdanau-style attention, also referred to as additive attention,
Luong-style attention, which is known as multiplicative attention,
highly parallelizable self-attention introduced in 2016 as decomposable attention and successfully used in transformers a year later,
positional attention and factorized positional attention.
For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.
Much effort has gone into understand Attention further by studying their roles in focused settings, such as in-context learning, masked language tasks, stripped down transformers, bigram statistics, N-gram statistics, pairwise convolutions, and arithmetic factoring.
These variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients.  In the figures below, W is the matrix of context attention weights, similar to the formula in Core Calculations section above.

Mathematical representation
Standard Scaled Dot-Product Attention
For matrices: 
  
    
      
        
          Q
        
        ∈
        
          
            R
            
              m
              ×
              
                d
                
                  k
                
              
            
          
        
        ,
        
          K
        
        ∈
        
          
            R
            
              n
              ×
              
                d
                
                  k
                
              
            
          
        
      
    
    {\displaystyle \mathbf {Q} \in \mathbb {R^{m\times d_{k}}} ,\mathbf {K} \in \mathbb {R^{n\times d_{k}}} }
  
 and 
  
    
      
        
          V
        
        ∈
        
          
            R
            
              n
              ×
              
                d
                
                  v
                
              
            
          
        
      
    
    {\displaystyle \mathbf {V} \in \mathbb {R^{n\times d_{v}}} }
  
, the scaled dot-product, or QKV attention is defined as:

  
    
      
        
          Attention
        
        (
        
          Q
        
        ,
        
          K
        
        ,
        
          V
        
        )
        =
        
          softmax
        
        
          (
          
            
              
                
                  Q
                
                
                  
                    K
                  
                  
                    T
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
          )
        
        
          V
        
        ∈
        
          
            R
          
          
            m
            ×
            
              d
              
                v
              
            
          
        
      
    
    {\displaystyle {\text{Attention}}(\mathbf {Q} ,\mathbf {K} ,\mathbf {V} )={\text{softmax}}\left({\frac {\mathbf {Q} \mathbf {K} ^{T}}{\sqrt {d_{k}}}}\right)\mathbf {V} \in \mathbb {R} ^{m\times d_{v}}}
  

where 
  
    
      
        
          

          
          
            T
          
        
      
    
    {\displaystyle {}^{T}}
  
 denotes transpose and the softmax function is applied independently to every row of its argument. The matrix 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbf {Q} }
  
 contains 
  
    
      
        m
      
    
    {\displaystyle m}
  
 queries, while matrices 
  
    
      
        
          K
        
        ,
        
          V
        
      
    
    {\displaystyle \mathbf {K} ,\mathbf {V} }
  
 jointly contain an unordered set of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 key-value pairs. Value vectors in matrix 
  
    
      
        
          V
        
      
    
    {\displaystyle \mathbf {V} }
  
 are weighted using the weights resulting from the softmax operation, so that the rows of the 
  
    
      
        m
      
    
    {\displaystyle m}
  
-by-
  
    
      
        
          d
          
            v
          
        
      
    
    {\displaystyle d_{v}}
  
 output matrix are confined to the convex hull of the points in 
  
    
      
        
          
            R
          
          
            
              d
              
                v
              
            
          
        
      
    
    {\displaystyle \mathbb {R} ^{d_{v}}}
  
 given by the rows of 
  
    
      
        
          V
        
      
    
    {\displaystyle \mathbf {V} }
  
.
To understand the permutation invariance and permutation equivariance properties of QKV attention, let 
  
    
      
        
          A
        
        ∈
        
          
            R
          
          
            m
            ×
            m
          
        
      
    
    {\displaystyle \mathbf {A} \in \mathbb {R} ^{m\times m}}
  
 and 
  
    
      
        
          B
        
        ∈
        
          
            R
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle \mathbf {B} \in \mathbb {R} ^{n\times n}}
  
 be permutation matrices; and 
  
    
      
        
          D
        
        ∈
        
          
            R
          
          
            m
            ×
            n
          
        
      
    
    {\displaystyle \mathbf {D} \in \mathbb {R} ^{m\times n}}
  
 an arbitrary matrix. The softmax function is permutation equivariant in the sense that:

  
    
      
        
          softmax
        
        (
        
          A
        
        
          D
        
        
          B
        
        )
        =
        
          A
        
        
        
          softmax
        
        (
        
          D
        
        )
        
          B
        
      
    
    {\displaystyle {\text{softmax}}(\mathbf {A} \mathbf {D} \mathbf {B} )=\mathbf {A} \,{\text{softmax}}(\mathbf {D} )\mathbf {B} }
  

By noting that the transpose of a permutation matrix is also its inverse, it follows that:

  
    
      
        
          Attention
        
        (
        
          A
        
        
          Q
        
        ,
        
          B
        
        
          K
        
        ,
        
          B
        
        
          V
        
        )
        =
        
          A
        
        
        
          Attention
        
        (
        
          Q
        
        ,
        
          K
        
        ,
        
          V
        
        )
      
    
    {\displaystyle {\text{Attention}}(\mathbf {A} \mathbf {Q} ,\mathbf {B} \mathbf {K} ,\mathbf {B} \mathbf {V} )=\mathbf {A} \,{\text{Attention}}(\mathbf {Q} ,\mathbf {K} ,\mathbf {V} )}
  

which shows that QKV attention is equivariant with respect to re-ordering the queries (rows of 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbf {Q} }
  
); and invariant to re-ordering of the key-value pairs in 
  
    
      
        
          K
        
        ,
        
          V
        
      
    
    {\displaystyle \mathbf {K} ,\mathbf {V} }
  
. These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as:

  
    
      
        
          X
        
        ↦
        
          Attention
        
        (
        
          X
        
        
          
            T
          
          
            q
          
        
        ,
        
          X
        
        
          
            T
          
          
            k
          
        
        ,
        
          X
        
        
          
            T
          
          
            v
          
        
        )
      
    
    {\displaystyle \mathbf {X} \mapsto {\text{Attention}}(\mathbf {X} \mathbf {T} _{q},\mathbf {X} \mathbf {T} _{k},\mathbf {X} \mathbf {T} _{v})}
  

is permutation equivariant with respect to re-ordering the rows of the input matrix 
  
    
      
        X
      
    
    {\displaystyle X}
  
 in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below.

Masked Attention
When QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have 
  
    
      
        n
      
    
    {\displaystyle n}
  
 rows, a masked attention variant is used:

  
    
      
        
          Attention
        
        (
        
          Q
        
        ,
        
          K
        
        ,
        
          V
        
        )
        =
        
          softmax
        
        
          (
          
            
              
                
                  
                    Q
                  
                  
                    
                      K
                    
                    
                      T
                    
                  
                
                
                  
                    d
                    
                      k
                    
                  
                
              
            
            +
            
              M
            
          
          )
        
        
          V
        
      
    
    {\displaystyle {\text{Attention}}(\mathbf {Q} ,\mathbf {K} ,\mathbf {V} )={\text{softmax}}\left({\frac {\mathbf {Q} \mathbf {K} ^{T}}{\sqrt {d_{k}}}}+\mathbf {M} \right)\mathbf {V} }
  

where the mask, 
  
    
      
        
          M
        
        ∈
        
          
            R
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle \mathbf {M} \in \mathbb {R} ^{n\times n}}
  
 is a stricly upper triangular matrix, with zeros on and below the diagonal and 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
 in every element above the diagonal. The softmax output, also in 
  
    
      
        
          
            R
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n\times n}}
  
 is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all 
  
    
      
        1
        ≤
        i
        <
        j
        ≤
        n
      
    
    {\displaystyle 1\leq i<j\leq n}
  
, row 
  
    
      
        i
      
    
    {\displaystyle i}
  
 of the attention ouput is independent of row 
  
    
      
        j
      
    
    {\displaystyle j}
  
 of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.

Multi-Head Attention
Multi-head attention

  
    
      
        
          MultiHead
        
        (
        
          Q
        
        ,
        
          K
        
        ,
        
          V
        
        )
        =
        
          Concat
        
        (
        
          
            head
          
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          
            head
          
          
            h
          
        
        )
        
          
            W
          
          
            O
          
        
      
    
    {\displaystyle {\text{MultiHead}}(\mathbf {Q} ,\mathbf {K} ,\mathbf {V} )={\text{Concat}}({\text{head}}_{1},...,{\text{head}}_{h})\mathbf {W} ^{O}}
  

where each head is computed with QKV attention as:

  
    
      
        
          
            head
          
          
            i
          
        
        =
        
          Attention
        
        (
        
          Q
        
        
          
            W
          
          
            i
          
          
            Q
          
        
        ,
        
          K
        
        
          
            W
          
          
            i
          
          
            K
          
        
        ,
        
          V
        
        
          
            W
          
          
            i
          
          
            V
          
        
        )
      
    
    {\displaystyle {\text{head}}_{i}={\text{Attention}}(\mathbf {Q} \mathbf {W} _{i}^{Q},\mathbf {K} \mathbf {W} _{i}^{K},\mathbf {V} \mathbf {W} _{i}^{V})}
  

and 
  
    
      
        
          
            W
          
          
            i
          
          
            Q
          
        
        ,
        
          
            W
          
          
            i
          
          
            K
          
        
        ,
        
          
            W
          
          
            i
          
          
            V
          
        
      
    
    {\displaystyle \mathbf {W} _{i}^{Q},\mathbf {W} _{i}^{K},\mathbf {W} _{i}^{V}}
  
, and 
  
    
      
        
          
            W
          
          
            O
          
        
      
    
    {\displaystyle \mathbf {W} ^{O}}
  
 are parameter matrices.
The permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, 
  
    
      
        
          A
        
        ,
        
          B
        
      
    
    {\displaystyle \mathbf {A} ,\mathbf {B} }
  
:

  
    
      
        
          MultiHead
        
        (
        
          A
        
        
          Q
        
        ,
        
          B
        
        
          K
        
        ,
        
          B
        
        
          V
        
        )
        =
        
          A
        
        
        
          MultiHead
        
        (
        
          Q
        
        ,
        
          K
        
        ,
        
          V
        
        )
      
    
    {\displaystyle {\text{MultiHead}}(\mathbf {A} \mathbf {Q} ,\mathbf {B} \mathbf {K} ,\mathbf {B} \mathbf {V} )=\mathbf {A} \,{\text{MultiHead}}(\mathbf {Q} ,\mathbf {K} ,\mathbf {V} )}
  

from which we also see that multi-head self-attention:

  
    
      
        
          X
        
        ↦
        
          MultiHead
        
        (
        
          X
        
        
          
            T
          
          
            q
          
        
        ,
        
          X
        
        
          
            T
          
          
            k
          
        
        ,
        
          X
        
        
          
            T
          
          
            v
          
        
        )
      
    
    {\displaystyle \mathbf {X} \mapsto {\text{MultiHead}}(\mathbf {X} \mathbf {T} _{q},\mathbf {X} \mathbf {T} _{k},\mathbf {X} \mathbf {T} _{v})}
  

is equivariant with respect to re-ordering of the rows of input matrix 
  
    
      
        X
      
    
    {\displaystyle X}
  
.

Bahdanau (Additive) Attention
Attention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          softmax
        
        (
        e
        )
        V
      
    
    {\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(e)V}
  

where 
  
    
      
        e
        =
        tanh
        ⁡
        (
        
          W
          
            Q
          
        
        Q
        +
        
          W
          
            K
          
        
        K
        )
      
    
    {\displaystyle e=\tanh(W_{Q}Q+W_{K}K)}
  
 and 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W_{Q}}
  
 and 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W_{K}}
  
 are learnable weight matrices.

Luong Attention (General)
Attention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          softmax
        
        (
        Q
        
          W
          
            a
          
        
        
          K
          
            T
          
        
        )
        V
      
    
    {\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(QW_{a}K^{T})V}
  

where 
  
    
      
        
          W
          
            a
          
        
      
    
    {\displaystyle W_{a}}
  
 is a learnable weight matrix.

See also
Recurrent neural network
seq2seq
Transformer (deep learning architecture)
Attention
Dynamic neural network

References
External links
Olah, Chris; Carter, Shan (September 8, 2016). "Attention and Augmented Recurrent Neural Networks". Distill. 1 (9). Distill Working Group. doi:10.23915/distill.00001.
Dan Jurafsky and James H. Martin (2022) Speech and Language Processing (3rd ed. draft, January 2022), ch. 10.4 Attention and ch. 9.7 Self-Attention Networks: Transformers
Alex Graves (4 May 2020), Attention and Memory in Deep Learning (video lecture), DeepMind / UCL, via YouTube
AutoGPT is an open-source "AI agent" that, given a goal in natural language, will attempt to achieve it by breaking it into sub-tasks and using the Internet and other tools in an automatic loop. It uses OpenAI's GPT-4 or GPT-3.5 APIs, and is among the first examples of an application using GPT-4 to perform autonomous tasks.

Background
On March 30, 2023, AutoGPT was released by Toran Bruce Richards, the founder and lead developer at video game company Significant Gravitas Ltd. AutoGPT is an open-source autonomous AI agent based on OpenAI's API for GPT-4, the large language model released on March 14, 2023. AutoGPT is among the first examples of an application using GPT-4 to perform autonomous tasks.
Richards developed AutoGPT to create a model that could respond to real-time feedback and to tasks that include long-term outlooks. Users are prompted to describe the AutoGPT agent's name, role, and objective and specify up to five ways to achieve that objective. From there, AutoGPT will independently work to achieve its objective without the user having to provide a prompt at every step.
In October 2023, AutoGPT raised $12M from investors.

Usage
AutoGPT is publicly available on GitHub. To use it, users must install AutoGPT in a development environment such as Docker. Also, users must register it with an API key from OpenAI, which requires users to have a paid OpenAI account.

Capabilities
The overarching capability of AutoGPT is the breaking down of a large task into various sub-tasks without the need for user input. These sub-tasks are then chained together and performed sequentially to yield a larger result as originally laid out by the user input. One of the distinguishing features of AutoGPT is its ability to connect to the internet. This allows for up-to-date information retrieval to help complete tasks. 
In addition, AutoGPT maintains short-term memory for the current task, which allows it to provide context to subsequent sub-tasks needed to achieve the larger goal. Another feature is its ability to store and organize files so users can better structure their data for future analysis and extension. AutoGPT is also multimodal, which means that it can take in both text and images as input. With these features, AutoGPT is claimed to be capable of automating workflows, analyzing data, and coming up with new suggestions.

Applications
Software
AutoGPT can be used to efficiently develop software applications from scratch. AutoGPT can also debug code and generate test cases. Observers suggest that AutoGPT's ability to write, debug, test, and edit code may extend to AutoGPT's own source code, enabling self-improvement.

Business
AutoGPT can be used to do market research, analyze investments, research products and write product reviews, create a business plan or improve operations, and create content such as a blog or podcast. One user has used AutoGPT to conduct product research and write a summary on the best headphones. Another user has used AutoGPT to summarize recent news events and prepare an outline for a podcast.

Other
AutoGPT was used to create ChefGPT, an AI agent able to independently explore the internet to generate and save unique recipes. AutoGPT was also used to create ChaosGPT, an AI agent tasked to “destroy humanity, establish global dominance, cause chaos and destruction, control humanity through manipulation, and attain immortality”. ChaosGPT reportedly researched nuclear weapons and tweeted disparagingly about humankind.

Limitations
AutoGPT is susceptible to frequent mistakes, primarily because it relies on its own feedback, which can compound errors. In contrast, non-autonomous models can be corrected by users overseeing their outputs. Furthermore, AutoGPT has a tendency to hallucinate or to present false or misleading information as fact when responding.
AutoGPT can be constrained by the cost associated with running it as its recursive nature requires it to continually call the OpenAI API on which it is built. Every step required in one of AutoGPT's tasks requires a corresponding call to GPT-4 at a cost of at least about $0.03 for every 1000 tokens used for inputs and $0.06 for every 1000 tokens for output when choosing the cheapest option. For reference, 1000 tokens roughly result in 750 words.
Another limitation is AutoGPT's tendency to get stuck in infinite loops. Developers believe that this is a result of AutoGPT's inability to remember, as it is unaware of what it has already done and repeatedly attempts the same subtask without end. Andrej Karpathy, co-founder of OpenAI which creates GPT-4, further explains that it is AutoGPT's “finite context window” that can limit its performance and cause it to “go off the rails”. Like other autonomous agents, AutoGPT is prone to distraction and unable to focus on its objective due to its lack of long-term memory, leading to unpredictable and unintended behavior.

Reception
AutoGPT became the top trending repository on GitHub after its release and has since repeatedly trended on Twitter.
In April 2023, Avram Piltch wrote for Tom's Hardware that AutoGPT 'might be too autonomous to be useful,' as it did not ask questions to clarify requirements or allow corrective interventions by users. Piltch nonetheless noted that such tools have "a ton of potential" and should improve with better language models and further development.
Malcolm McMillan from Tom's Guide mentioned that AutoGPT may not be better than ChatGPT for tasks involving conversation, as ChatGPT is well-suited for situations in which advice, rather than task completion, is sought.
Will Knight from Wired wrote that AutoGPT is not a foolproof task-completion tool. When given a test task of finding a public figure's email address, he noted that it was not able to accurately find the email address.
Clara Shih, Salesforce Service Cloud CEO commented that "AutoGPT illustrates the power and unknown risks of generative AI," and that due to usage risks, enterprises should include a human in the loop when using such technologies.
Performance is reportedly enhanced when using AutoGPT with GPT-4 compared to GPT-3.5. For example, one reviewer who tested it on a task of finding the best laptops on the market with pros and cons found that AutoGPT with GPT-4 created a more comprehensive report than one by GPT 3.5.

See also
ChatGPT - Large Language Model-based Chatbot by OpenAI
GPT-3 - 2020 Large Language Model by OpenAI
GPT-4 - 2023 Large Language Model by OpenAI
Artificial general intelligence - Hypothetical intelligent agent that could learn to accomplish any intellectual task that humans can perform
Hallucination (artificial intelligence) - Responses generated by an AI that contain false information that are presented as fact.

References
Further reading
Pounder, Les (April 15, 2023). "How To Create Your Own AutoGPT AI Agent". Tom's Hardware. Retrieved April 16, 2023.
Wiggers, Kyle (April 22, 2023). "What is AutoGPT and why does it matter?". TechCrunch. Retrieved April 23, 2023.

External links
Official website

Official repository at GitHub
An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction, to generate lower-dimensional embeddings for subsequent use by other machine learning algorithms.
Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).

Mathematical principles
Definition
An autoencoder is defined by the following components: Two sets: the space of decoded messages 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
; the space of encoded messages 
  
    
      
        
          
            Z
          
        
      
    
    {\displaystyle {\mathcal {Z}}}
  
. Typically 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
 and 
  
    
      
        
          
            Z
          
        
      
    
    {\displaystyle {\mathcal {Z}}}
  
 are Euclidean spaces, that is, 
  
    
      
        
          
            X
          
        
        =
        
          
            R
          
          
            m
          
        
        ,
        
          
            Z
          
        
        =
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle {\mathcal {X}}=\mathbb {R} ^{m},{\mathcal {Z}}=\mathbb {R} ^{n}}
  
 with 
  
    
      
        m
        >
        n
        .
      
    
    {\displaystyle m>n.}
  
    Two parametrized families of functions: the encoder family 
  
    
      
        
          E
          
            ϕ
          
        
        :
        
          
            X
          
        
        →
        
          
            Z
          
        
      
    
    {\displaystyle E_{\phi }:{\mathcal {X}}\rightarrow {\mathcal {Z}}}
  
, parametrized by 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
; the decoder family 
  
    
      
        
          D
          
            θ
          
        
        :
        
          
            Z
          
        
        →
        
          
            X
          
        
      
    
    {\displaystyle D_{\theta }:{\mathcal {Z}}\rightarrow {\mathcal {X}}}
  
, parametrized by 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
.For any 
  
    
      
        x
        ∈
        
          
            X
          
        
      
    
    {\displaystyle x\in {\mathcal {X}}}
  
, we usually write 
  
    
      
        z
        =
        
          E
          
            ϕ
          
        
        (
        x
        )
      
    
    {\displaystyle z=E_{\phi }(x)}
  
, and refer to it as the code, the latent variable, latent representation, latent vector, etc. Conversely, for any 
  
    
      
        z
        ∈
        
          
            Z
          
        
      
    
    {\displaystyle z\in {\mathcal {Z}}}
  
, we usually write 
  
    
      
        
          x
          ′
        
        =
        
          D
          
            θ
          
        
        (
        z
        )
      
    
    {\displaystyle x'=D_{\theta }(z)}
  
, and refer to it as the (decoded) message.
Usually, both the encoder and the decoder are defined as multilayer perceptrons (MLPs). For example, a one-layer-MLP encoder 
  
    
      
        
          E
          
            ϕ
          
        
      
    
    {\displaystyle E_{\phi }}
  
 is:

  
    
      
        
          E
          
            ϕ
          
        
        (
        
          x
        
        )
        =
        σ
        (
        W
        x
        +
        b
        )
      
    
    {\displaystyle E_{\phi }(\mathbf {x} )=\sigma (Wx+b)}
  

where 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
 is an element-wise activation function, 
  
    
      
        W
      
    
    {\displaystyle W}
  
 is a "weight" matrix, and 
  
    
      
        b
      
    
    {\displaystyle b}
  
 is a "bias" vector.

Training an autoencoder
An autoencoder, by itself, is simply a tuple of two functions. To judge its quality, we need a task. A task is defined by a reference probability distribution 
  
    
      
        
          μ
          
            r
            e
            f
          
        
      
    
    {\displaystyle \mu _{ref}}
  
 over 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
, and a "reconstruction quality" function 
  
    
      
        d
        :
        
          
            X
          
        
        ×
        
          
            X
          
        
        →
        [
        0
        ,
        ∞
        ]
      
    
    {\displaystyle d:{\mathcal {X}}\times {\mathcal {X}}\to [0,\infty ]}
  
, such that 
  
    
      
        d
        (
        x
        ,
        
          x
          ′
        
        )
      
    
    {\displaystyle d(x,x')}
  
 measures how much 
  
    
      
        
          x
          ′
        
      
    
    {\displaystyle x'}
  
 differs from 
  
    
      
        x
      
    
    {\displaystyle x}
  
.
With those, we can define the loss function for the autoencoder as
  
    
      
        L
        (
        θ
        ,
        ϕ
        )
        :=
        
          
            
              E
            
          
          
            x
            ∼
            
              μ
              
                r
                e
                f
              
            
          
        
        [
        d
        (
        x
        ,
        
          D
          
            θ
          
        
        (
        
          E
          
            ϕ
          
        
        (
        x
        )
        )
        )
        ]
      
    
    {\displaystyle L(\theta ,\phi ):=\mathbb {\mathbb {E} } _{x\sim \mu _{ref}}[d(x,D_{\theta }(E_{\phi }(x)))]}
  
The optimal autoencoder for the given task 
  
    
      
        (
        
          μ
          
            r
            e
            f
          
        
        ,
        d
        )
      
    
    {\displaystyle (\mu _{ref},d)}
  
 is then 
  
    
      
        arg
        ⁡
        
          min
          
            θ
            ,
            ϕ
          
        
        L
        (
        θ
        ,
        ϕ
        )
      
    
    {\displaystyle \arg \min _{\theta ,\phi }L(\theta ,\phi )}
  
. The search for the optimal autoencoder can be accomplished by any mathematical optimization technique, but usually by gradient descent. This search process is referred to as "training the autoencoder".
In most situations, the reference distribution is just the empirical distribution given by a dataset 
  
    
      
        {
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            N
          
        
        }
        ⊂
        
          
            X
          
        
      
    
    {\displaystyle \{x_{1},...,x_{N}\}\subset {\mathcal {X}}}
  
, so that
  
    
      
        
          μ
          
            r
            e
            f
          
        
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          δ
          
            
              x
              
                i
              
            
          
        
      
    
    {\displaystyle \mu _{ref}={\frac {1}{N}}\sum _{i=1}^{N}\delta _{x_{i}}}
  

where 
  
    
      
        
          δ
          
            
              x
              
                i
              
            
          
        
      
    
    {\displaystyle \delta _{x_{i}}}
  
 is the Dirac measure, the quality function is just L2 loss: 
  
    
      
        d
        (
        x
        ,
        
          x
          ′
        
        )
        =
        ‖
        x
        −
        
          x
          ′
        
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle d(x,x')=\|x-x'\|_{2}^{2}}
  
, and 
  
    
      
        ‖
        ⋅
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \|\cdot \|_{2}}
  
 is the Euclidean norm. Then the problem of searching for the optimal autoencoder is just a least-squares optimization:
  
    
      
        
          min
          
            θ
            ,
            ϕ
          
        
        L
        (
        θ
        ,
        ϕ
        )
        ,
        
        
          where 
        
        L
        (
        θ
        ,
        ϕ
        )
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        ‖
        
          x
          
            i
          
        
        −
        
          D
          
            θ
          
        
        (
        
          E
          
            ϕ
          
        
        (
        
          x
          
            i
          
        
        )
        )
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \min _{\theta ,\phi }L(\theta ,\phi ),\qquad {\text{where }}L(\theta ,\phi )={\frac {1}{N}}\sum _{i=1}^{N}\|x_{i}-D_{\theta }(E_{\phi }(x_{i}))\|_{2}^{2}}

Interpretation
An autoencoder has two main parts: an encoder that maps the message to a code, and a decoder that reconstructs the message from the code. An optimal autoencoder would perform as close to perfect reconstruction as possible, with "close to perfect" defined by the reconstruction quality function 
  
    
      
        d
      
    
    {\displaystyle d}
  
.
The simplest way to perform the copying task perfectly would be to duplicate the signal. To suppress this behavior, the code space 
  
    
      
        
          
            Z
          
        
      
    
    {\displaystyle {\mathcal {Z}}}
  
 usually has fewer dimensions than the message space 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
.
Such an autoencoder is called undercomplete. It can be interpreted as compressing the message, or reducing its dimensionality.
At the limit of an ideal undercomplete autoencoder, every possible code 
  
    
      
        z
      
    
    {\displaystyle z}
  
 in the code space is used to encode a message 
  
    
      
        x
      
    
    {\displaystyle x}
  
 that really appears in the distribution 
  
    
      
        
          μ
          
            r
            e
            f
          
        
      
    
    {\displaystyle \mu _{ref}}
  
, and the decoder is also perfect: 
  
    
      
        
          D
          
            θ
          
        
        (
        
          E
          
            ϕ
          
        
        (
        x
        )
        )
        =
        x
      
    
    {\displaystyle D_{\theta }(E_{\phi }(x))=x}
  
. This ideal autoencoder can then be used to generate messages indistinguishable from real messages, by feeding its decoder arbitrary code 
  
    
      
        z
      
    
    {\displaystyle z}
  
 and obtaining 
  
    
      
        
          D
          
            θ
          
        
        (
        z
        )
      
    
    {\displaystyle D_{\theta }(z)}
  
, which is a message that really appears in the distribution 
  
    
      
        
          μ
          
            r
            e
            f
          
        
      
    
    {\displaystyle \mu _{ref}}
  
.
If the code space 
  
    
      
        
          
            Z
          
        
      
    
    {\displaystyle {\mathcal {Z}}}
  
 has dimension larger than (overcomplete), or equal to, the message space 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  
, or the hidden units are given enough capacity, an autoencoder can learn the identity function and become useless. However, experimental results found that overcomplete autoencoders might still learn useful features.
In the ideal setting, the code dimension and the model capacity could be set on the basis of the complexity of the data distribution to be modeled. A standard way to do so is to add modifications to the basic autoencoder, to be detailed below.

History
The autoencoder was first proposed as a nonlinear generalization of principal components analysis (PCA) by Kramer. The autoencoder has also been called the autoassociator, or Diabolo network. Its first applications date to early 1990s. Their most traditional application was dimensionality reduction or feature learning, but the concept became widely used for learning generative models of data. Some of the most powerful AIs in the 2010s involved autoencoders stacked inside deep neural networks.

Variations
Regularized autoencoders
Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.

Sparse autoencoder
Inspired by the sparse coding hypothesis in neuroscience, sparse autoencoders (SAE) are variants of autoencoders, such that the codes 
  
    
      
        
          E
          
            ϕ
          
        
        (
        x
        )
      
    
    {\displaystyle E_{\phi }(x)}
  
 for messages tend to be sparse codes, that is, 
  
    
      
        
          E
          
            ϕ
          
        
        (
        x
        )
      
    
    {\displaystyle E_{\phi }(x)}
  
 is close to zero in most entries. Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time. Encouraging sparsity improves performance on classification tasks. 
There are two main ways to enforce sparsity. One way is to simply clamp all but the highest-k activations of the latent code to zero. This is the k-sparse autoencoder.
The k-sparse autoencoder inserts the following "k-sparse function" in the latent layer of a standard autoencoder:
  
    
      
        
          f
          
            k
          
        
        (
        
          x
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
        )
        =
        (
        
          x
          
            1
          
        
        
          b
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
        
          b
          
            n
          
        
        )
      
    
    {\displaystyle f_{k}(x_{1},...,x_{n})=(x_{1}b_{1},...,x_{n}b_{n})}
  
where 
  
    
      
        
          b
          
            i
          
        
        =
        1
      
    
    {\displaystyle b_{i}=1}
  
 if 
  
    
      
        
          |
        
        
          x
          
            i
          
        
        
          |
        
      
    
    {\displaystyle |x_{i}|}
  
 ranks in the top k, and 0 otherwise.
Backpropagating through 
  
    
      
        
          f
          
            k
          
        
      
    
    {\displaystyle f_{k}}
  
 is simple: set gradient to 0 for 
  
    
      
        
          b
          
            i
          
        
        =
        0
      
    
    {\displaystyle b_{i}=0}
  
 entries, and keep gradient for 
  
    
      
        
          b
          
            i
          
        
        =
        1
      
    
    {\displaystyle b_{i}=1}
  
 entries. This is essentially a generalized ReLU function.
The other way is a relaxed version of the k-sparse autoencoder. Instead of forcing sparsity, we add a sparsity regularization loss, then optimize for
  
    
      
        
          min
          
            θ
            ,
            ϕ
          
        
        L
        (
        θ
        ,
        ϕ
        )
        +
        λ
        
          L
          
            s
            p
            a
            r
            s
            i
            t
            y
          
        
        (
        θ
        ,
        ϕ
        )
      
    
    {\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )+\lambda L_{sparsity}(\theta ,\phi )}
  
where 
  
    
      
        λ
        >
        0
      
    
    {\displaystyle \lambda >0}
  
 measures how much sparsity we want to enforce.
Let the autoencoder architecture have 
  
    
      
        K
      
    
    {\displaystyle K}
  
 layers. To define a sparsity regularization loss, we need a "desired" sparsity 
  
    
      
        
          
            
              
                ρ
                ^
              
            
          
          
            k
          
        
      
    
    {\displaystyle {\hat {\rho }}_{k}}
  
 for each layer, a weight 
  
    
      
        
          w
          
            k
          
        
      
    
    {\displaystyle w_{k}}
  
 for how much to enforce each sparsity, and a function 
  
    
      
        s
        :
        [
        0
        ,
        1
        ]
        ×
        [
        0
        ,
        1
        ]
        →
        [
        0
        ,
        ∞
        ]
      
    
    {\displaystyle s:[0,1]\times [0,1]\to [0,\infty ]}
  
 to measure how much two sparsities differ.
For each input 
  
    
      
        x
      
    
    {\displaystyle x}
  
, let the actual sparsity of activation in each layer 
  
    
      
        k
      
    
    {\displaystyle k}
  
 be
  
    
      
        
          ρ
          
            k
          
        
        (
        x
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          a
          
            k
            ,
            i
          
        
        (
        x
        )
      
    
    {\displaystyle \rho _{k}(x)={\frac {1}{n}}\sum _{i=1}^{n}a_{k,i}(x)}
  
where 
  
    
      
        
          a
          
            k
            ,
            i
          
        
        (
        x
        )
      
    
    {\displaystyle a_{k,i}(x)}
  
 is the activation in the 
  
    
      
        i
      
    
    {\displaystyle i}
  
 -th neuron of the 
  
    
      
        k
      
    
    {\displaystyle k}
  
 -th layer upon input 
  
    
      
        x
      
    
    {\displaystyle x}
  
.
The sparsity loss upon input 
  
    
      
        x
      
    
    {\displaystyle x}
  
 for one layer is 
  
    
      
        s
        (
        
          
            
              
                ρ
                ^
              
            
          
          
            k
          
        
        ,
        
          ρ
          
            k
          
        
        (
        x
        )
        )
      
    
    {\displaystyle s({\hat {\rho }}_{k},\rho _{k}(x))}
  
, and the sparsity regularization loss for the entire autoencoder is the expected weighted sum of sparsity losses:
  
    
      
        
          L
          
            s
            p
            a
            r
            s
            i
            t
            y
          
        
        (
        θ
        ,
        ϕ
        )
        =
        
          
            
              E
            
          
          
            x
            ∼
            
              μ
              
                X
              
            
          
        
        
          [
          
            
              ∑
              
                k
                ∈
                1
                :
                K
              
            
            
              w
              
                k
              
            
            s
            (
            
              
                
                  
                    ρ
                    ^
                  
                
              
              
                k
              
            
            ,
            
              ρ
              
                k
              
            
            (
            x
            )
            )
          
          ]
        
      
    
    {\displaystyle L_{sparsity}(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X}}\left[\sum _{k\in 1:K}w_{k}s({\hat {\rho }}_{k},\rho _{k}(x))\right]}
  
Typically, the function 
  
    
      
        s
      
    
    {\displaystyle s}
  
 is either the Kullback-Leibler (KL) divergence, as

  
    
      
        s
        (
        ρ
        ,
        
          
            
              ρ
              ^
            
          
        
        )
        =
        K
        L
        (
        ρ
        
          |
        
        
          |
        
        
          
            
              ρ
              ^
            
          
        
        )
        =
        ρ
        log
        ⁡
        
          
            ρ
            
              
                ρ
                ^
              
            
          
        
        +
        (
        1
        −
        ρ
        )
        log
        ⁡
        
          
            
              1
              −
              ρ
            
            
              1
              −
              
                
                  
                    ρ
                    ^
                  
                
              
            
          
        
      
    
    {\displaystyle s(\rho ,{\hat {\rho }})=KL(\rho ||{\hat {\rho }})=\rho \log {\frac {\rho }{\hat {\rho }}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho }}}}}
  

or the L1 loss, as 
  
    
      
        s
        (
        ρ
        ,
        
          
            
              ρ
              ^
            
          
        
        )
        =
        
          |
        
        ρ
        −
        
          
            
              ρ
              ^
            
          
        
        
          |
        
      
    
    {\displaystyle s(\rho ,{\hat {\rho }})=|\rho -{\hat {\rho }}|}
  
, or the L2 loss, as 
  
    
      
        s
        (
        ρ
        ,
        
          
            
              ρ
              ^
            
          
        
        )
        =
        
          |
        
        ρ
        −
        
          
            
              ρ
              ^
            
          
        
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle s(\rho ,{\hat {\rho }})=|\rho -{\hat {\rho }}|^{2}}
  
.
Alternatively, the sparsity regularization loss may be defined without reference to any "desired sparsity", but simply force as much sparsity as possible. In this case, one can define the sparsity regularization loss as 
  
    
      
        
          L
          
            s
            p
            a
            r
            s
            i
            t
            y
          
        
        (
        θ
        ,
        ϕ
        )
        =
        
          
            
              E
            
          
          
            x
            ∼
            
              μ
              
                X
              
            
          
        
        
          [
          
            
              ∑
              
                k
                ∈
                1
                :
                K
              
            
            
              w
              
                k
              
            
            ‖
            
              h
              
                k
              
            
            ‖
          
          ]
        
      
    
    {\displaystyle L_{sparsity}(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X}}\left[\sum _{k\in 1:K}w_{k}\|h_{k}\|\right]}
  
where 
  
    
      
        
          h
          
            k
          
        
      
    
    {\displaystyle h_{k}}
  
 is the activation vector in the 
  
    
      
        k
      
    
    {\displaystyle k}
  
-th layer of the autoencoder. The norm 
  
    
      
        ‖
        ⋅
        ‖
      
    
    {\displaystyle \|\cdot \|}
  
 is usually the L1 norm (giving the L1 sparse autoencoder) or the L2 norm (giving the L2 sparse autoencoder).

Denoising autoencoder
Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion. 
A DAE, originally called a "robust autoassociative network", is trained by intentionally corrupting the inputs of a standard autoencoder during training. A noise process is defined by a probability distribution 
  
    
      
        
          μ
          
            T
          
        
      
    
    {\displaystyle \mu _{T}}
  
 over functions 
  
    
      
        T
        :
        
          
            X
          
        
        →
        
          
            X
          
        
      
    
    {\displaystyle T:{\mathcal {X}}\to {\mathcal {X}}}
  
. That is, the function 
  
    
      
        T
      
    
    {\displaystyle T}
  
 takes a message 
  
    
      
        x
        ∈
        
          
            X
          
        
      
    
    {\displaystyle x\in {\mathcal {X}}}
  
, and corrupts it to a noisy version 
  
    
      
        T
        (
        x
        )
      
    
    {\displaystyle T(x)}
  
. The function 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is selected randomly, with a probability distribution 
  
    
      
        
          μ
          
            T
          
        
      
    
    {\displaystyle \mu _{T}}
  
.
Given a task 
  
    
      
        (
        
          μ
          
            r
            e
            f
          
        
        ,
        d
        )
      
    
    {\displaystyle (\mu _{ref},d)}
  
, the problem of training a DAE is the optimization problem:
  
    
      
        
          min
          
            θ
            ,
            ϕ
          
        
        L
        (
        θ
        ,
        ϕ
        )
        =
        
          
            
              E
            
          
          
            x
            ∼
            
              μ
              
                X
              
            
            ,
            T
            ∼
            
              μ
              
                T
              
            
          
        
        [
        d
        (
        x
        ,
        (
        
          D
          
            θ
          
        
        ∘
        
          E
          
            ϕ
          
        
        ∘
        T
        )
        (
        x
        )
        )
        ]
      
    
    {\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X},T\sim \mu _{T}}[d(x,(D_{\theta }\circ E_{\phi }\circ T)(x))]}
  
That is, the optimal DAE should take any noisy message and attempt to recover the original message without noise, thus the name "denoising".
Usually, the noise process 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is applied only during training and testing, not during downstream use.
The use of DAE depends on two assumptions:

There exist representations to the messages that are relatively stable and robust to the type of noise we are likely to encounter;
The said representations capture structures in the input distribution that are useful for our purposes.
Example noise processes include:

additive isotropic Gaussian noise,
masking noise (a fraction of the input is randomly chosen and set to 0)
salt-and-pepper noise (a fraction of the input is randomly chosen and randomly set to its minimum or maximum value).

Contractive autoencoder (CAE)
A contractive autoencoder adds the contractive regularization loss to the standard autoencoder loss:
  
    
      
        
          min
          
            θ
            ,
            ϕ
          
        
        L
        (
        θ
        ,
        ϕ
        )
        +
        λ
        
          L
          
            c
            o
            n
            t
            r
            a
            c
            t
            i
            v
            e
          
        
        (
        θ
        ,
        ϕ
        )
      
    
    {\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )+\lambda L_{contractive}(\theta ,\phi )}
  
where 
  
    
      
        λ
        >
        0
      
    
    {\displaystyle \lambda >0}
  
 measures how much contractive-ness we want to enforce. The contractive regularization loss itself is defined as the expected Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input:
  
    
      
        
          L
          
            c
            o
            n
            t
            r
            a
            c
            t
            i
            v
            e
          
        
        (
        θ
        ,
        ϕ
        )
        =
        
          
            E
          
          
            x
            ∼
            
              μ
              
                r
                e
                f
              
            
          
        
        ‖
        
          ∇
          
            x
          
        
        
          E
          
            ϕ
          
        
        (
        x
        )
        
          ‖
          
            F
          
          
            2
          
        
      
    
    {\displaystyle L_{contractive}(\theta ,\phi )=\mathbb {E} _{x\sim \mu _{ref}}\|\nabla _{x}E_{\phi }(x)\|_{F}^{2}}
  
To understand what 
  
    
      
        
          L
          
            c
            o
            n
            t
            r
            a
            c
            t
            i
            v
            e
          
        
      
    
    {\displaystyle L_{contractive}}
  
 measures, note the fact
  
    
      
        ‖
        
          E
          
            ϕ
          
        
        (
        x
        +
        δ
        x
        )
        −
        
          E
          
            ϕ
          
        
        (
        x
        )
        
          ‖
          
            2
          
        
        ≤
        ‖
        
          ∇
          
            x
          
        
        
          E
          
            ϕ
          
        
        (
        x
        )
        
          ‖
          
            F
          
        
        ‖
        δ
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \|E_{\phi }(x+\delta x)-E_{\phi }(x)\|_{2}\leq \|\nabla _{x}E_{\phi }(x)\|_{F}\|\delta x\|_{2}}
  
for any message 
  
    
      
        x
        ∈
        
          
            X
          
        
      
    
    {\displaystyle x\in {\mathcal {X}}}
  
, and small variation 
  
    
      
        δ
        x
      
    
    {\displaystyle \delta x}
  
 in it. Thus, if 
  
    
      
        ‖
        
          ∇
          
            x
          
        
        
          E
          
            ϕ
          
        
        (
        x
        )
        
          ‖
          
            F
          
          
            2
          
        
      
    
    {\displaystyle \|\nabla _{x}E_{\phi }(x)\|_{F}^{2}}
  
 is small, it means that a small neighborhood of the message maps to a small neighborhood of its code. This is a desired property, as it means small variation in the message leads to small, perhaps even zero, variation in its code, like how two pictures may look the same even if they are not exactly the same.
The DAE can be understood as an infinitesimal limit of CAE: in the limit of small Gaussian input noise, DAEs make the reconstruction function resist small but finite-sized input perturbations, while CAEs make the extracted features resist infinitesimal input perturbations.

Minimal description length autoencoder
Concrete autoencoder
The concrete autoencoder is designed for discrete feature selection. A concrete autoencoder forces the latent space to consist only of a user-specified number of features. The concrete autoencoder uses a continuous relaxation of the categorical distribution to allow gradients to pass through the feature selector layer, which makes it possible to use standard backpropagation to learn an optimal subset of input features that minimize reconstruction loss.

Variational autoencoder (VAE)
Variational autoencoders (VAEs) belong to the families of variational Bayesian methods. Despite the architectural similarities with basic autoencoders, VAEs are architected with different goals and have a different mathematical formulation. The latent space is, in this case, composed of a mixture of distributions instead of fixed vectors.
Given an input dataset 
  
    
      
        x
      
    
    {\displaystyle x}
  
 characterized by an unknown probability function 
  
    
      
        P
        (
        x
        )
      
    
    {\displaystyle P(x)}
  
 and a multivariate latent encoding vector 
  
    
      
        z
      
    
    {\displaystyle z}
  
, the objective is to model the data as a distribution 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle p_{\theta }(x)}
  
, with 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
 defined as the set of the network parameters so that 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
        =
        
          ∫
          
            z
          
        
        
          p
          
            θ
          
        
        (
        x
        ,
        z
        )
        d
        z
      
    
    {\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }(x,z)dz}
  
.

Advantages of depth
Autoencoders are often trained with a single-layer encoder and a single-layer decoder, but using many-layered (deep) encoders and decoders offers many advantages.

Depth can exponentially reduce the computational cost of representing some functions.
Depth can exponentially decrease the amount of training data needed to learn some functions.
Experimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.

Training
Geoffrey Hinton developed the deep belief network technique for training many-layered deep autoencoders. His method involves treating each neighboring set of two layers as a restricted Boltzmann machine so that pretraining approximates a good solution, then using backpropagation to fine-tune the results.
Researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders. A 2015 study showed that joint training learns better data models along with more representative features for classification as compared to the layerwise method. However, their experiments showed that the success of joint training depends heavily on the regularization strategies adopted.

Applications
The two main applications of autoencoders are dimensionality reduction and information retrieval, but modern variations have been applied to other tasks.

Dimensionality reduction
Dimensionality reduction was one of the first deep learning applications.
For Hinton's 2006 study, he pretrained a multi-layer autoencoder with a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until hitting a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 components of a principal component analysis (PCA), and learned a representation that was qualitatively easier to interpret, clearly separating data clusters.
Representing dimensions can improve performance on tasks such as classification. Indeed, the hallmark of dimensionality reduction is to place semantically related examples near each other.

Principal component analysis
If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size 
  
    
      
        p
      
    
    {\displaystyle p}
  
 (where 
  
    
      
        p
      
    
    {\displaystyle p}
  
 is less than the size of the input) span the same vector subspace as the one spanned by the first 
  
    
      
        p
      
    
    {\displaystyle p}
  
 principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition.
However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.

Information retrieval and Search engine optimization
Information retrieval benefits particularly from dimensionality reduction in that search can become more efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by Salakhutdinov and Hinton in 2007. By training the algorithm to produce a low-dimensional binary code, all database entries could be stored in a hash table mapping binary code vectors to entries. This table would then support information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the query encoding.
The encoder-decoder architecture, often used in natural language processing and neural networks, can be scientifically applied in the field of SEO (Search Engine Optimization) in various ways:

Text Processing: By using an autoencoder, it's possible to compress the text of web pages into a more compact vector representation. This can help reduce page loading times and improve indexing by search engines.

Noise Reduction: Autoencoders can be used to remove noise from the textual data of web pages. This can lead to a better understanding of the content by search engines, thereby enhancing ranking in search engine result pages.

Meta Tag and Snippet Generation: Autoencoders can be trained to automatically generate meta tags, snippets, and descriptions for web pages using the page content. This can optimize the presentation in search results, increasing the Click-Through Rate (CTR).

Content Clustering: Using an autoencoder, web pages with similar content can be automatically grouped together. This can help organize the website logically and improve navigation, potentially positively affecting user experience and search engine rankings.

Generation of Related Content: An autoencoder can be employed to generate content related to what is already present on the site. This can enhance the website's attractiveness to search engines and provide users with additional relevant information.

Keyword Detection: Autoencoders can be trained to identify keywords and important concepts within the content of web pages. This can assist in optimizing keyword usage for better indexing.

Semantic Search: By using autoencoder techniques, semantic representation models of content can be created. These models can be used to enhance search engines' understanding of the themes covered in web pages.
In essence, the encoder-decoder architecture or autoencoders can be leveraged in SEO to optimize web page content, improve their indexing, and enhance their appeal to both search engines and users.

Anomaly detection
Another application for autoencoders is anomaly detection. By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. After training, the autoencoder will accurately reconstruct "normal" data, while failing to do so with unfamiliar anomalous data. Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies.
Recent literature has however shown that certain autoencoding models can, counterintuitively, be very good at reconstructing anomalous examples and consequently not able to reliably perform anomaly detection.

Image processing
The characteristics of autoencoders are useful in image processing.
One example can be found in lossy image compression, where autoencoders outperformed other approaches and proved competitive against JPEG 2000.
Another useful application of autoencoders in image preprocessing is image denoising.
Autoencoders found use in more demanding contexts such as medical imaging where they have been used for image denoising as well as super-resolution. In image-assisted diagnosis, experiments have applied autoencoders for breast cancer detection and for modelling the relation between the cognitive decline of Alzheimer's disease and the latent features of an autoencoder trained with MRI.

Drug discovery
In 2019 molecules generated with variational autoencoders were validated experimentally in mice.

Popularity prediction
Recently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts, which is helpful for online advertising strategies.

Machine translation
Autoencoders have been applied to machine translation, which is usually referred to as neural machine translation (NMT). Unlike traditional autoencoders, the output does not match the input - it is in another language. In NMT, texts are treated as sequences to be encoded into the learning procedure, while on the decoder side sequences in the target language(s) are generated. Language-specific autoencoders incorporate further linguistic features into the learning procedure, such as Chinese decomposition features. Machine translation is rarely still done with autoencoders, due to the availability of more effective transformer networks.

See also
Representation learning
Sparse dictionary learning
Deep learning


== References ==
Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science with close connections to mathematical logic. The word automata comes from the Greek word αὐτόματος, which means "self-acting, self-willed, self-moving". An automaton (automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a finite automaton (FA) or finite-state machine (FSM). The figure on the right illustrates a finite-state machine, which is a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the previous state and current input symbol as its arguments.
Automata theory is closely related to formal language theory. In this context, automata are used as finite representations of formal languages that may be infinite. Automata are often classified by the class of formal languages they can recognize, as in the Chomsky hierarchy, which describes a nesting relationship between major classes of automata. Automata play a major role in the theory of computation, compiler construction, artificial intelligence, parsing and formal verification.

History
The theory of abstract automata was developed in the mid-20th century in connection with finite automata. Automata theory was initially considered a branch of mathematical systems theory, studying the behavior of discrete-parameter systems. Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems. The theory of the finite-state transducer was developed under different names by different research communities. The earlier concept of Turing machine was also included in the discipline along with new forms of infinite-state automata, such as pushdown automata.
1956 saw the publication of Automata Studies, which collected work by scientists including Claude Shannon, W. Ross Ashby, John von Neumann, Marvin Minsky, Edward F. Moore, and Stephen Cole Kleene. With the publication of this volume, "automata theory emerged as a relatively autonomous discipline". The book included Kleene's description of the set of regular events, or regular languages, and a relatively stable measure of complexity in Turing machine programs by Shannon. 
In the same year, Noam Chomsky described the Chomsky hierarchy, a correspondence between automata and formal grammars, and Ross Ashby published An Introduction to Cybernetics, an accessible textbook explaining automata and information using basic set theory.
The study of linear bounded automata led to the Myhill–Nerode theorem, which gives a necessary and sufficient condition for a formal language to be regular, and an exact count of the number of states in a minimal machine for the language. The pumping lemma for regular languages, also useful in regularity proofs, was proven in this period by Michael O. Rabin and Dana Scott, along with the computational equivalence of deterministic and nondeterministic finite automata. 
In the 1960s, a body of algebraic results known as "structure theory" or "algebraic decomposition theory" emerged, which dealt with the realization of sequential machines from smaller machines by interconnection. While any finite automaton can be simulated using a  universal gate set, this requires that the simulating circuit contain loops of arbitrary complexity. Structure theory deals with the "loop-free" realizability of machines.
The theory of computational complexity also took shape in the 1960s. By the end of the decade, automata theory came to be seen as "the pure mathematics of computer science".

Automata
What follows is a general definition of an automaton, which restricts a broader definition of a system to one viewed as acting in discrete time-steps, with its state behavior and outputs defined at each step by unchanging functions of only its state and input.

Informal description
An automaton runs when it is given some sequence of inputs in discrete (individual) time steps (or just steps). An automaton processes one input picked from a set of symbols or letters, which is called an input alphabet. The symbols received by the automaton as input at any step are a sequence of symbols called words. An automaton has a set of states. At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input, it moves to another state (or transitions) based on a transition function that takes the previous state and current input symbol as parameters. At the same time, another function called the output function produces symbols from the output alphabet, also according to the previous state and current input symbol. The automaton reads the symbols of the input word and transitions between states until the word is read completely, if it is finite in length, at which point the automaton halts. A state at which the automaton halts is called the final state.
To investigate the possible state/input/output sequences in an automaton using formal language theory, a machine can be assigned a starting state and a set of accepting states. Then, depending on whether a run starting from the starting state ends in an accepting state, the automaton can be said to accept or reject an input sequence. The set of all the words accepted by an automaton is called the language recognized by the automaton. A familiar example of a machine recognizing a language is an electronic lock, which accepts or rejects attempts to enter the correct code.

Formal definition
Automaton
An automaton can be represented formally by a quintuple 
  
    
      
        M
        =
        ⟨
        Σ
        ,
        Γ
        ,
        Q
        ,
        δ
        ,
        λ
        ⟩
      
    
    {\displaystyle M=\langle \Sigma ,\Gamma ,Q,\delta ,\lambda \rangle }
  
, where:

  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  
 is a finite set of symbols, called the input alphabet of the automaton,

  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  
 is another finite set of symbols, called the output alphabet of the automaton,

  
    
      
        Q
      
    
    {\displaystyle Q}
  
 is a set of states,

  
    
      
        δ
      
    
    {\displaystyle \delta }
  
 is the next-state function or transition function 
  
    
      
        δ
        :
        Q
        ×
        Σ
        →
        Q
      
    
    {\displaystyle \delta :Q\times \Sigma \to Q}
  
 mapping state-input pairs to successor states,

  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 is the next-output function 
  
    
      
        λ
        :
        Q
        ×
        Σ
        →
        Γ
      
    
    {\displaystyle \lambda :Q\times \Sigma \to \Gamma }
  
 mapping state-input pairs to outputs.
If 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 is finite, then 
  
    
      
        M
      
    
    {\displaystyle M}
  
 is a finite automaton.
Input word
An automaton reads a finite string of symbols 
  
    
      
        
          a
          
            1
          
        
        
          a
          
            2
          
        
        .
        .
        .
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{1}a_{2}...a_{n}}
  
, where 
  
    
      
        
          a
          
            i
          
        
        ∈
        Σ
      
    
    {\displaystyle a_{i}\in \Sigma }
  
, which is called an input word. The set of all words is denoted by 
  
    
      
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle \Sigma ^{*}}
  
.
Run
A sequence of states 
  
    
      
        
          q
          
            0
          
        
        ,
        
          q
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          q
          
            n
          
        
      
    
    {\displaystyle q_{0},q_{1},...,q_{n}}
  
, where 
  
    
      
        
          q
          
            i
          
        
        ∈
        Q
      
    
    {\displaystyle q_{i}\in Q}
  
 such that 
  
    
      
        
          q
          
            i
          
        
        =
        δ
        (
        
          q
          
            i
            −
            1
          
        
        ,
        
          a
          
            i
          
        
        )
      
    
    {\displaystyle q_{i}=\delta (q_{i-1},a_{i})}
  
 for 
  
    
      
        0
        <
        i
        ≤
        n
      
    
    {\displaystyle 0<i\leq n}
  
, is a run of the automaton on an input 
  
    
      
        
          a
          
            1
          
        
        
          a
          
            2
          
        
        .
        .
        .
        
          a
          
            n
          
        
        ∈
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle a_{1}a_{2}...a_{n}\in \Sigma ^{*}}
  
 starting from state 
  
    
      
        
          q
          
            0
          
        
      
    
    {\displaystyle q_{0}}
  
. In other words, at first the automaton is at the start state 
  
    
      
        
          q
          
            0
          
        
      
    
    {\displaystyle q_{0}}
  
, and receives input 
  
    
      
        
          a
          
            1
          
        
      
    
    {\displaystyle a_{1}}
  
. For 
  
    
      
        
          a
          
            1
          
        
      
    
    {\displaystyle a_{1}}
  
 and every following 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
  
 in the input string, the automaton picks the next state 
  
    
      
        
          q
          
            i
          
        
      
    
    {\displaystyle q_{i}}
  
 according to the transition function 
  
    
      
        δ
        (
        
          q
          
            i
            −
            1
          
        
        ,
        
          a
          
            i
          
        
        )
      
    
    {\displaystyle \delta (q_{i-1},a_{i})}
  
, until the last symbol 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
  
 has been read, leaving the machine in the final state of the run, 
  
    
      
        
          q
          
            n
          
        
      
    
    {\displaystyle q_{n}}
  
. Similarly, at each step, the automaton emits an output symbol according to the output function 
  
    
      
        λ
        (
        
          q
          
            i
            −
            1
          
        
        ,
        
          a
          
            i
          
        
        )
      
    
    {\displaystyle \lambda (q_{i-1},a_{i})}
  
.
The transition function 
  
    
      
        δ
      
    
    {\displaystyle \delta }
  
 is extended inductively into 
  
    
      
        
          
            δ
            ¯
          
        
        :
        Q
        ×
        
          Σ
          
            ∗
          
        
        →
        Q
      
    
    {\displaystyle {\overline {\delta }}:Q\times \Sigma ^{*}\to Q}
  
 to describe the machine's behavior when fed whole input words. For the empty string 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, 
  
    
      
        
          
            δ
            ¯
          
        
        (
        q
        ,
        ε
        )
        =
        q
      
    
    {\displaystyle {\overline {\delta }}(q,\varepsilon )=q}
  
 for all states 
  
    
      
        q
      
    
    {\displaystyle q}
  
, and for strings 
  
    
      
        w
        a
      
    
    {\displaystyle wa}
  
 where 
  
    
      
        a
      
    
    {\displaystyle a}
  
 is the last symbol and 
  
    
      
        w
      
    
    {\displaystyle w}
  
 is the (possibly empty) rest of the string, 
  
    
      
        
          
            δ
            ¯
          
        
        (
        q
        ,
        w
        a
        )
        =
        δ
        (
        
          
            δ
            ¯
          
        
        (
        q
        ,
        w
        )
        ,
        a
        )
      
    
    {\displaystyle {\overline {\delta }}(q,wa)=\delta ({\overline {\delta }}(q,w),a)}
  
. The output function 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 may be extended similarly into 
  
    
      
        
          
            λ
            ¯
          
        
        (
        q
        ,
        w
        )
      
    
    {\displaystyle {\overline {\lambda }}(q,w)}
  
, which gives the complete output of the machine when run on word 
  
    
      
        w
      
    
    {\displaystyle w}
  
 from state 
  
    
      
        q
      
    
    {\displaystyle q}
  
.
Acceptor
In order to study an automaton with the theory of formal languages, an automaton may be considered as an acceptor, replacing the output alphabet and function 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  
 and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 with

  
    
      
        
          q
          
            0
          
        
        ∈
        Q
      
    
    {\displaystyle q_{0}\in Q}
  
, a designated start state, and

  
    
      
        F
      
    
    {\displaystyle F}
  
, a set of states of 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 (i.e. 
  
    
      
        F
        ⊆
        Q
      
    
    {\displaystyle F\subseteq Q}
  
) called accept states.
This allows the following to be defined:
Accepting word
A word 
  
    
      
        w
        =
        
          a
          
            1
          
        
        
          a
          
            2
          
        
        .
        .
        .
        
          a
          
            n
          
        
        ∈
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle w=a_{1}a_{2}...a_{n}\in \Sigma ^{*}}
  
 is an accepting word for the automaton if 
  
    
      
        
          
            δ
            ¯
          
        
        (
        
          q
          
            0
          
        
        ,
        w
        )
        ∈
        F
      
    
    {\displaystyle {\overline {\delta }}(q_{0},w)\in F}
  
, that is, if after consuming the whole string 
  
    
      
        w
      
    
    {\displaystyle w}
  
 the machine is in an accept state.
Recognized language
The language 
  
    
      
        L
        ⊆
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle L\subseteq \Sigma ^{*}}
  
 recognized by an automaton is the set of all the words that are accepted by the automaton, 
  
    
      
        L
        =
        {
        w
        ∈
        
          Σ
          
            ∗
          
        
         
        
          |
        
         
        
          
            δ
            ¯
          
        
        (
        
          q
          
            0
          
        
        ,
        w
        )
        ∈
        F
        }
      
    
    {\displaystyle L=\{w\in \Sigma ^{*}\ |\ {\overline {\delta }}(q_{0},w)\in F\}}
  
.
Recognizable languages
The recognizable languages are the set of languages that are recognized by some automaton. For finite automata the recognizable languages are regular languages. For different types of automata, the recognizable languages are different.

Variant definitions of automata
Automata are defined to study useful machines under mathematical formalism. So the definition of an automaton is open to variations according to the "real world machine" that we want to model using the automaton. People have studied many variations of automata. The following are some popular variations in the definition of different components of automata.

Input
Finite input: An automaton that accepts only finite sequences of symbols. The above introductory definition only encompasses finite words.
Infinite input: An automaton that accepts infinite words (ω-words). Such automata are called ω-automata.
Tree input: The input may be a tree of symbols instead of sequence of symbols. In this case after reading each symbol, the automaton reads all the successor symbols in the input tree. It is said that the automaton makes one copy of itself for each successor and each such copy starts running on one of the successor symbols from the state according to the transition relation of the automaton. Such an automaton is called a tree automaton.
Infinite tree input : The two extensions above can be combined, so the automaton reads a tree structure with (in)finite branches. Such an automaton is called an infinite tree automaton.
States
Single state: An automaton with one state, also called a combinational circuit, performs a transformation which may implement combinational logic.
Finite states: An automaton that contains only a finite number of states.
Infinite states: An automaton that may not have a finite number of states, or even a countable number of states. Different kinds of abstract memory may be used to give such machines finite descriptions.
Stack memory: An automaton may also contain some extra memory in the form of a stack in which symbols can be pushed and popped. This kind of automaton is called a pushdown automaton.
Queue memory: An automaton may have memory in the form of a queue. Such a machine is called queue machine and is Turing-complete.
Tape memory: The inputs and outputs of automata are often described as input and output tapes. Some machines have additional working tapes, including the Turing machine, linear bounded automaton, and log-space transducer.
Transition function
Deterministic: For a given current state and an input symbol, if an automaton can only jump to one and only one state then it is a deterministic automaton.
Nondeterministic: An automaton that, after reading an input symbol, may jump into any of a number of states, as licensed by its transition relation. The term transition function is replaced by transition relation: The automaton non-deterministically decides to jump into one of the allowed choices. Such automata are called nondeterministic automata.
Alternation: This idea is quite similar to tree automata but orthogonal. The automaton may run its multiple copies on the same next read symbol. Such automata are called alternating automata. The acceptance condition must be satisfied on all runs of such copies to accept the input.
Two-wayness: Automata may read their input from left to right, or they may be allowed to move back-and-forth on the input, in a way similar to a Turing machine. Automata which can move back-and-forth on the input are called two-way finite automata.
Acceptance condition
Acceptance of finite words: Same as described in the informal definition above.
Acceptance of infinite words: an ω-automaton cannot have final states, as infinite words never terminate. Rather, acceptance of the word is decided by looking at the infinite sequence of visited states during the run.
Probabilistic acceptance: An automaton need not strictly accept or reject an input. It may accept the input with some probability between zero and one. For example, quantum finite automata, geometric automata and metric automata have probabilistic acceptance.
Different combinations of the above variations produce many classes of automata.
Automata theory is a subject matter that studies properties of various types of automata. For example, the following questions are studied about a given type of automata.

Which class of formal languages is recognizable by some type of automata? (Recognizable languages)
Are certain automata closed under union, intersection, or complementation of formal languages? (Closure properties)
How expressive is a type of automata in terms of recognizing a class of formal languages? And, their relative expressive power? (Language hierarchy)
Automata theory also studies the existence or nonexistence of any effective algorithms to solve problems similar to the following list:

Does an automaton accept at least one input word? (Emptiness checking)
Is it possible to transform a given non-deterministic automaton into a deterministic automaton without changing the language recognized? (Determinization)
For a given formal language, what is the smallest automaton that recognizes it? (Minimization)

Types of automata
The following is an incomplete list of types of automata.

Discrete, continuous, and hybrid automata
Normally automata theory describes the states of abstract machines but there are discrete automata, analog automata or continuous automata, or hybrid discrete-continuous automata, which use digital data, analog data or continuous time, or digital and analog data, respectively.

Hierarchy in terms of powers
The following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.

Applications
Each model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of human languages. Cellular automata are used in the field of artificial life, the most famous example being John Conway's Game of Life. Some other examples which could be explained using automata theory in biology include mollusk and pine cone growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse, and was popularized in America by Edward Fredkin. Automata also appear in the theory of finite fields: the set of irreducible polynomials that can be written as composition of degree two polynomials is in fact a regular language.
Another problem for which automata can be used is the induction of regular languages.

Automata simulators
Automata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language  or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing's World, JFLAP, VAS, TAGS and SimStudio.

Category-theoretic models
One can define several distinct categories of automata following the automata classification into different types described in the previous section. The mathematical category of deterministic automata, sequential machines or sequential automata, and Turing machines with automata homomorphisms defining the arrows between automata is a Cartesian closed category, it has both categorical limits and colimits. An automata homomorphism maps a quintuple of an automaton Ai onto the quintuple of another automaton 
 Aj. Automata homomorphisms can also be considered as automata transformations or as semigroup homomorphisms, when the state space, S, of the automaton is defined as a semigroup Sg. Monoids are also considered as a suitable setting for automata in monoidal categories.

Categories of variable automata
One could also define a variable automaton, in the sense of Norbert Wiener in his book on The Human Use of Human Beings via the endomorphisms 
  
    
      
        
          A
          
            i
          
        
        →
        
          A
          
            i
          
        
      
    
    {\displaystyle A_{i}\to A_{i}}
  
. Then one can show that such variable automata homomorphisms form a mathematical group. In the case of non-deterministic, or other complex kinds of automata, the latter set of endomorphisms may become, however, a variable automaton groupoid. Therefore, in the most general case, categories of variable automata of any kind are categories of groupoids or groupoid categories. Moreover, the category of reversible automata is then a 
2-category, and also a subcategory of the 2-category of groupoids, or the groupoid category.

See also
Boolean differential calculus
Petri net

References
Further reading
John E. Hopcroft; Rajeev Motwani; Jeffrey D. Ullman (2000). Introduction to Automata Theory, Languages, and Computation (2nd ed.). Pearson Education. ISBN 978-0-201-44124-6.
Michael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing. ISBN 978-0-534-94728-6. Part One: Automata and Languages, chapters 1–2, pp. 29–122. Section 4.1: Decidable Languages, pp. 152–159. Section 5.1: Undecidable Problems from Language Theory, pp. 172–183.
Elaine Rich (2008). Automata, Computability and Complexity: Theory and Applications. Pearson. ISBN 978-0-13-228806-4.
Salomaa, Arto (1985). Computation and automata. Encyclopedia of Mathematics and Its Applications. Vol. 25. Cambridge University Press. ISBN 978-0-521-30245-6. Zbl 0565.68046.
Anderson, James A. (2006). Automata theory with modern applications. With contributions by Tom Head. Cambridge: Cambridge University Press. ISBN 978-0-521-61324-8. Zbl 1127.68049.
Conway, J.H. (1971). Regular algebra and finite machines. Chapman and Hall Mathematics Series. London: Chapman & Hall. Zbl 0231.94041.
John M. Howie (1991) Automata and Languages, Clarendon Press ISBN 0-19-853424-8 MR1254435
Sakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge University Press. ISBN 978-0-521-84425-3. Zbl 1188.68177.
James P. Schmeiser; David T. Barnard (1995). Producing a top-down parse order with bottom-up parsing. Elsevier North-Holland.
Igor Aleksander; F. Keith Hanna (1975). Automata Theory: An Engineering Approach. New York: Crane Russak. ISBN 978-0-8448-0657-0.
Marvin Minsky (1967). Computation: Finite and infinite machines. Princeton, N.J.: Prentice Hall.
John C. Martin (2011). Introduction to Languages and The Theory of Computation. New York: McGraw Hill. ISBN 978-0-07-319146-1.

External links
dk.brics.automaton
libfa
Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.

Overview
There are different definitions of ADM based on the level of automation involved. Some definitions suggests ADM involves decisions made through purely technological means without human input, such as the EU's General Data Protection Regulation (Article 22). However, ADM technologies and applications can take many forms ranging from decision-support systems that make recommendations for human decision-makers to act on, sometimes known as augmented intelligence or 'shared decision-making', to fully automated decision-making processes that make decisions on behalf of individuals or organizations without human involvement. Models used in automated decision-making systems can be as simple as checklists and decision trees through to artificial intelligence and deep neural networks (DNN).
Since the 1950s computers have gone from being able to do basic processing to having the capacity to undertake complex, ambiguous and highly skilled tasks such as image and speech recognition, gameplay, scientific and medical analysis and inferencing across multiple data sources. ADM is now being increasingly deployed across all sectors of society and many diverse domains from entertainment to transport.
An ADM system (ADMS) may involve multiple decision points, data sets, and technologies (ADMT) and may sit within a larger administrative or technical system such as a criminal justice system or business process.

Data
Automated decision-making involves using data as input to be analyzed within a process, model, or algorithm or for learning and generating new models. ADM systems may use and connect a wide range of data types and sources depending on the goals and contexts of the system, for example, sensor data for self-driving cars and robotics, identity data for security systems, demographic and financial data for public administration, medical records in health, criminal records in law. This can sometimes involve vast amounts of data and computing power.

Data quality
The quality of the available data and its ability to be used in ADM systems is fundamental to the outcomes. It is often highly problematic for many reasons. Datasets are often highly variable; corporations or governments may control large-scale data, restricted for privacy or security reasons, incomplete, biased, limited in terms of time or coverage, measuring and describing terms in different ways, and many other issues.
For machines to learn from data, large corpora are often required, which can be challenging to obtain or compute; however, where available, they have provided significant breakthroughs, for example, in diagnosing chest X-rays.

ADM Technologies
Automated decision-making technologies (ADMT) are software-coded digital tools that automate the translation of input data to output data, contributing to the function of automated decision-making systems. There are a wide range of technologies in use across ADM applications and systems.
ADMTs involving basic computational operations

Search (includes 1-2-1, 1-2-many, data matching/merge)
Matching (two different things)
Mathematical Calculation (formula)
ADMTs for assessment and grouping:

User profiling
Recommender systems
Clustering
Classification
Feature learning
Predictive analytics (includes forecasting)
ADMTs relating to space and flows:

Social network analysis (includes link prediction)
Mapping
Routing
ADMTs for processing of complex data formats

Image processing
Audio processing
Natural Language Processing (NLP)
Other ADMT

Business rules management systems
Time series analysis
Anomaly detection
Modelling/Simulation

Machine learning
Machine learning (ML) involves training computer programs through exposure to large data sets and examples to learn from experience and solve problems. Machine learning can be used to generate and analyse data as well as make algorithmic calculations and has been applied to image and speech recognition, translations, text, data and simulations. While machine learning has been around for some time, it is becoming increasingly powerful due to recent breakthroughs in training deep neural networks (DNNs), and dramatic increases in data storage capacity and computational power with GPU coprocessors and cloud computing.
Machine learning systems based on foundation models run on deep neural networks and use pattern matching to train a single huge system on large amounts of general data such as text and images. Early models tended to start from scratch for each new problem however since the early 2020s many are able to be adapted to new problems. Examples of these technologies include Open AI's DALL-E (an image creation program) and their various GPT language models, and Google's PaLM language model program.

Applications
ADM is being used to replace or augment human decision-making by both public and private-sector organisations for a range of reasons including to help increase consistency, improve efficiency, reduce costs and enable new solutions to complex problems.

Debate
Research and development are underway into uses of technology to assess argument quality, assess argumentative essays and judge debates. Potential applications of these argument technologies span education and society. Scenarios to consider, in these regards, include those involving the assessment and evaluation of conversational, mathematical, scientific, interpretive, legal, and political argumentation and debate.

Law
In legal systems around the world, algorithmic tools such as risk assessment instruments (RAI), are being used to supplement or replace the human judgment of judges, civil servants and police officers in many contexts. In the United States RAI are being used to generate scores to predict the risk of recidivism in pre-trial detention and sentencing decisions, evaluate parole for prisoners and to predict "hot spots" for future crime. These scores may result in automatic effects or may be used to inform decisions made by officials within the justice system. In Canada ADM has been used since 2014 to automate certain activities conducted by immigration officials and to support the evaluation of some immigrant and visitor applications.

Economics
Automated decision-making systems are used in certain computer programs to create buy and sell orders related to specific financial transactions and automatically submit the orders in the international markets. Computer programs can automatically generate orders based on predefined set of rules using trading strategies which are based on technical analyses, advanced statistical and mathematical computations, or inputs from other electronic sources.

Business
Continuous auditing
Continuous auditing uses advanced analytical tools to automate auditing processes. It can be utilized in the private sector by business enterprises and in the public sector by governmental organizations and municipalities. As artificial intelligence and machine learning continue to advance, accountants and auditors may make use of increasingly sophisticated algorithms which make decisions such as those involving determining what is anomalous, whether to notify personnel, and how to prioritize those tasks assigned to personnel.

Media and Entertainment
Digital media, entertainment platforms, and information services increasingly provide content to audiences via automated recommender systems based on demographic information, previous selections, collaborative filtering or content-based filtering. This includes music and video platforms, publishing, health information, product databases and search engines. Many recommender systems also provide some agency to users in accepting recommendations and incorporate data-driven algorithmic feedback loops based on the actions of the system user.
Large-scale machine learning language models and image creation programs being developed by companies such as OpenAI and Google in the 2020s have restricted access however they are likely to have widespread application in fields such as advertising, copywriting, stock imagery and graphic design as well as other fields such as journalism and law.

Advertising
Online advertising is closely integrated with many digital media platforms, websites and search engines and often involves automated delivery of display advertisements in diverse formats. 'Programmatic' online advertising involves automating the sale and delivery of digital advertising on websites and platforms via software rather than direct human decision-making. This is sometimes known as the waterfall model which involves a sequence of steps across various systems and players: publishers and data management platforms, user data, ad servers and their delivery data, inventory management systems, ad traders and ad exchanges. There are various issues with this system including lack of transparency for advertisers, unverifiable metrics, lack of control over ad venues, audience tracking and privacy concerns. Internet users who dislike ads have adopted counter measures such as ad blocking technologies which allow users to automatically filter unwanted advertising from websites and some internet applications. In 2017, 24% of Australian internet users had ad blockers.

Health
Deep learning AI image models are being used for reviewing x-rays and detecting the eye condition macular degeneration.

Social Services
Governments have been implementing digital technologies to provide more efficient administration and social services since the early 2000s, often referred to as e-government. Many governments around the world are now using automated, algorithmic systems for profiling and targeting policies and services including algorithmic policing based on risks, surveillance sorting of people such as airport screening, providing services based on risk profiles in child protection, providing employment services and governing the unemployed. A significant application of ADM in social services relates to the use of predictive analytics – eg predictions of risks to children from abuse/neglect in child protection, predictions of recidivism or crime in policing and criminal justice, predictions of welfare/tax fraud in compliance systems, predictions of long term unemployment in employment services. Historically these systems were based on standard statistical analyses, however from the early 2000s machine learning has increasingly been developed and deployed. Key issues with the use of ADM in social services include bias, fairness, accountability and explainability which refers to transparency around the reasons for a decision and the ability to explain the basis on which a machine made a decision. For example Australia's federal social security delivery agency, Centrelink, developed and implemented an automated processes for detecting and collecting debt which led to many cases of wrongful debt collection in what became known as the RoboDebt scheme.

Transport and Mobility
Connected and automated mobility (CAM) involves autonomous vehicles such as self-driving cars and other forms of transport which use automated decision-making systems to replace various aspects of human control of the vehicle. This can range from level 0 (complete human driving) to level 5 (completely autonomous). At level 5 the machine is able to make decisions to control the vehicle based on data models and geospatial mapping and real-time sensors and processing of the environment. Cars with levels 1 to 3 are already available on the market in 2021. In 2016 The German government established an 'Ethics Commission on Automated and Connected Driving' which recommended connected and automated vehicles (CAVs) be developed if the systems cause fewer accidents than human drivers (positive balance of risk). It also provided 20 ethical rules for the adaptation of automated and connected driving. In 2020 the European Commission strategy on CAMs recommended that they be adopted in Europe to reduce road fatalities and lower emissions however self-driving cars also raise many policy, security and legal issues in terms of liability and ethical decision-making in the case of accidents, as well as privacy issues. Issues of trust in autonomous vehicles and community concerns about their safety are key factors to be addressed if AVs are to be widely adopted.

Surveillance
Automated digital data collections via sensors, cameras, online transactions and social media have significantly expanded the scope, scale, and goals of surveillance practices and institutions in government and commercial sectors. As a result there has been a major shift from targeted monitoring of suspects to the ability to monitor entire populations. The level of surveillance now possible as a result of automated data collection has been described as surveillance capitalism or surveillance economy to indicate the way digital media involves large-scale tracking and accumulation of data on every interaction.

Ethical and legal issues
There are many social, ethical and legal implications of automated decision-making systems. Concerns raised include lack of transparency and contestability of decisions, incursions on privacy and surveillance, exacerbating systemic bias and inequality due to data and algorithmic bias, intellectual property rights, the spread of misinformation via media platforms, administrative discrimination, risk and responsibility, unemployment and many others. As ADM becomes more ubiquitous there is greater need to address the ethical challenges to ensure good governance in information societies.
ADM systems are often based on machine learning and algorithms which are not easily able to be viewed or analysed, leading to concerns that they are 'black box' systems which are not transparent or accountable.
A report from Citizen lab in Canada argues for a critical human rights analysis of the application of ADM in various areas to ensure the use of automated decision-making does not result in infringements on rights, including the rights to equality and non-discrimination; freedom of movement, expression, religion, and association; privacy rights and the rights to life, liberty, and security of the person.
Legislative responses to ADM include:

The European General Data Protection Regulation (GDPR), introduced in 2016, is a regulation in EU law on data protection and privacy in the European Union (EU). Article 22(1) enshrines the right of data subjects not to be subject to decisions, which have legal or other significant effects, being based solely on automatic individual decision making. GDPR also includes some rules on the right to explanation however the exact scope and nature of these is currently subject to pending review by the Court of Justice of the European Union. These provisions were not first introduced in the GDPR, but have been present in a similar form across Europe since the Data Protection Directive in 1995, and the 1978 French law, the loi informatique et libertés. Similarly scoped and worded provisions with varying attached rights and obligations are present in the data protection laws of many other jurisdictions across the world, including Uganda, Morocco and the US state of Virginia.
Rights for the explanation of public sector automated decisions forming 'algorithmic treatment' under the French loi pour une République numérique.

Bias
ADM may incorporate algorithmic bias arising from:

Data sources, where data inputs are biased in their collection or selection
Technical design of the algorithm, for example where assumptions have been made about how a person will behave
Emergent bias, where the application of ADM in unanticipated circumstances creates a biased outcome

Explainability
Questions of biased or incorrect data or algorithms and concerns that some ADMs are black box technologies, closed to human scrutiny or interrogation, has led to what is referred to as the issue of explainability, or the right to an explanation of automated decisions and AI. This is also known as Explainable AI (XAI), or Interpretable AI, in which the results of the solution can be analysed and understood by humans. XAI algorithms are considered to follow three principles - transparency, interpretability and explainability.

Information asymmetry
Automated decision-making may increase the information asymmetry between individuals whose data feeds into the system and the platforms and decision-making systems capable of inferring information from that data. On the other hand it has been observed that in financial trading the information asymmetry between two artificial intelligent agents may be much less than between two human agents or between human and machine agents.

Research fields
Many academic disciplines and fields are increasingly turning their attention to the development, application and implications of ADM including business, computer sciences, human computer interaction (HCI), law, public administration, and media and communications. The automation of media content and algorithmically driven news, video and other content via search systems and platforms is a major focus of academic research in media studies.
The ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include ADM and AI.
Key research centres investigating ADM include: 

Algorithm Watch, Germany
ARC Centre of Excellence for Automated Decision-Making and Society, Australia
Citizen Lab, Canada
Informatics Europe

See also
Automated decision support
Algorithmic bias
Decision-making software
Decision Management
Ethics of artificial intelligence
Government by algorithm
Machine learning
Recommender systems


== References ==
Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. 
AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. 
Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.

Comparison to the standard approach
In a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. 
Each of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.
AutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.

Targets of automation
Automated machine learning can target various stages of the machine learning process.  Steps to automate are:

Data preparation and ingestion (from raw data and miscellaneous formats)
Column type detection; e.g., Boolean, discrete numerical, continuous numerical, or text
Column intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature
Task detection; e.g., binary classification, regression, clustering, or ranking
Feature engineering
Feature selection
Feature extraction
Meta-learning and transfer learning
Detection and handling of skewed data and/or missing values
Model selection - choosing which machine learning algorithm to use, often including multiple competing software implementations
Ensembling - a form of consensus where using multiple models often gives better results than any single model
Hyperparameter optimization of the learning algorithm and featurization
Neural architecture search
Pipeline selection under time, memory, and complexity constraints
Selection of evaluation metrics and validation procedures
Problem checking
Leakage detection
Misconfiguration detection
Analysis of obtained results
Creating user interfaces and visualizations

Challenges and Limitations
There are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as "development as a cottage industry". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.
Additionally, some other challenges include meta-learning challenges and computational resource allocation.

See also
Neural architecture search
Neuroevolution
Self-tuning
Neural Network Intelligence
AutoAI
ModelOps
Hyperparameter optimization

References
Further reading
"Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI". Bizety. 2020-06-16.
Ferreira, Luís, et al. "A comparison of AutoML tools for machine learning, deep learning and XGBoost." 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https://repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and robust automated machine learning. Advances in neural information processing systems, 28. https://proceedings.neurips.cc/paper_files/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf
Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.
CAD also has potential future applications in digital pathology with the advent of whole-slide imaging and machine learning algorithms. So far its application has been limited to quantifying immunostaining but is also being investigated for the standard H&E stain.
CAD is an interdisciplinary technology combining elements of artificial intelligence and computer vision with radiological and pathology image processing. A typical application is the detection of a tumor. For instance, some hospitals use CAD to support preventive medical check-ups in mammography (diagnosis of breast cancer), the detection of polyps in colonoscopy, and lung cancer.
Computer-aided detection (CADe) systems are usually confined to marking conspicuous structures and sections. Computer-aided diagnosis (CADx) systems evaluate the conspicuous structures. For example, in mammography CAD highlights microcalcification clusters and hyperdense structures in the soft tissue. This allows the radiologist to draw conclusions about the condition of the pathology. Another application is CADq, which quantifies, e.g., the size of a tumor or the tumor's behavior in contrast medium uptake. Computer-aided simple triage (CAST) is another type of CAD, which performs a fully automatic initial interpretation and triage of studies into some meaningful categories (e.g. negative and positive). CAST is particularly applicable in emergency diagnostic imaging, where a prompt diagnosis of critical, life-threatening condition is required.
Although CAD has been used in clinical environments for over 40 years, CAD usually does not substitute the doctor or other professional, but rather plays a supporting role. The professional (generally a radiologist) is generally responsible for the final interpretation of a medical image. However, the goal of some CAD systems is to detect earliest signs of abnormality in patients that human professionals cannot, as in diabetic retinopathy, architectural distortion in mammograms, ground-glass nodules in thoracic CT, and non-polypoid (“flat”) lesions in CT colonography.

History
In the late 1950s, with the dawn of modern computers researchers in various fields started exploring the possibility of building computer-aided medical diagnostic (CAD) systems. These first CAD systems used flow-charts, statistical pattern-matching, probability theory, or knowledge bases to drive their decision-making process.
In the early 1970s, some of the very early CAD systems in medicine, which were often referred as “expert systems” in medicine, were developed and used mainly for educational purposes. Examples include the MYCIN expert system, the Internist-I expert system and the CADUCEUS (expert system).
During the beginning of the early developments, the researchers were aiming at building entirely automated CAD / expert systems. The expectated capability of computers was unrealistically optimistic among these scientists. However, after the breakthrough paper, “Reducibility among Combinatorial Problems” by Richard M. Karp, it became clear that there were limitations but also potential opportunities when one develops algorithms to solve groups of important computational problems.
As result of the new understanding of the various algorithmic limitations that Karp discovered in the early 1970s, researchers started realizing the serious limitations that CAD and expert systems in medicine have. The recognition of these limitations brought the investigators to develop new kinds of CAD systems by using advanced approaches. Thus, by the late 1980s and early 1990s the focus sifted in the use of data mining approaches for the purpose of using more advanced and flexible CAD systems.
In 1998, the first commercial CAD system for mammography, the ImageChecker system, was approved by the US Food and Drug Administration (FDA). In the following years several commercial CAD systems for analyzing mammography, breast MRI, medical imagining of lung, colon, and heart also received FDA approvals. Currently, CAD systems are used as a diagnostic aid to provide physicians for better medical decision-making.

Methodology
CAD is fundamentally based on highly complex pattern recognition. X-ray or other types of images are scanned for suspicious structures. Normally a few thousand images are required to optimize the algorithm. Digital image data are copied to a CAD server in a DICOM-format and are prepared and analyzed in several steps.
1. Preprocessing for

Reduction of artifacts (bugs in images)
Image noise reduction
Leveling (harmonization) of image quality (increased contrast) for clearing the image's different basic conditions e.g. different exposure parameter.
Filtering
2. Segmentation for

Differentiation of different structures in the image, e.g. heart, lung, ribcage, blood vessels, possible round lesions
Matching with anatomic databank
Sample gray-values in volume of interest
3. Structure/ROI (Region of Interest) Analyze
Every detected region is analyzed individually for special characteristics:

Compactness
Form, size and location
Reference to close by structures / ROIs
Average grey level value analyze within a ROI
Proportion of grey levels to border of the structure inside the ROI
4. Evaluation / classification
After the structure is analyzed, every ROI is evaluated individually (scoring) for the probability of a TP. The following procedures are examples of classification algorithms.

Nearest-Neighbor Rule (e.g. k-nearest neighbors)
Minimum distance classifier
Cascade classifier
Naive Bayes classifier
Artificial neural network
Radial basis function network (RBF)
Support vector machine (SVM)
Principal component analysis (PCA)
If the detected structures have reached a certain threshold level, they are highlighted in the image for the radiologist. Depending on the CAD system these markings can be permanently or temporary saved. The latter's advantage is that only the markings which are approved by the radiologist are saved. False hits should not be saved, because an examination at a later date becomes more difficult then.

Relation to provider metrics
Sensitivity and specificity
CAD systems seek to highlight suspicious structures. Today's CAD systems cannot detect 100% of pathological changes. The hit rate (sensitivity) can be up to 90% depending on system and application. A correct hit is termed a True Positive (TP), while the incorrect marking of healthy sections constitutes a False Positive (FP). The less FPs indicated, the higher the specificity is. A low specificity reduces the acceptance of the CAD system because the user has to identify all of these wrong hits. The FP-rate in lung overview examinations (CAD Chest) could be reduced to 2 per examination. In other segments (e.g. CT lung examinations) the FP-rate could be 25 or more. In CAST systems the FP rate must be extremely low (less than 1 per examination) to allow a meaningful study triage.

Absolute detection rate
The absolute detection rate of a radiologist is an alternative metric to sensitivity and specificity. Overall, results of clinical trials about sensitivity, specificity, and the absolute detection rate can vary markedly. Each study result depends on its basic conditions and has to be evaluated on those terms. The following facts have a strong influence:

Retrospective or prospective design
Quality of the used images
Condition of the x-ray examination
Radiologist's experience and education
Type of lesion
Size of the considered lesion

Challenges
Despite the many developments that CAD has achieved since the dawn of computers, there are still certain challenges that CAD systems face today.
Some challenges are related to various algorithmic limitations in the procedures of a CAD system including input data collection, preprocessing, processing and system assessments. Algorithms are generally designed to select a single likely diagnosis, thus providing suboptimal results for patients with multiple, concurrent disorders. Today input data for CAD mostly come from electronic health records (EHR). Effective designing, implementing and analyzing for EHR is a major necessity on any CAD systems.
Due to the massive availability of data and the need to analyze such data, big data is also one of the biggest challenges that CAD systems face today. The increasingly vast amount of patient data is a serious problem. Often the patient data are complex and can be semi-structured or unstructured data. It requires highly developed approaches to store, retrieve and analyze them in reasonable time.
During the preprocessing stage, input data must be normalized. The normalization of input data includes noise reduction and filtering. 
Processing may contain a few sub-steps depending on applications. Basic three sub-steps on medical imaging are segmentation, feature extraction / selection, and classification. These sub-steps require advanced techniques to analyze input data with less computational time. Although much effort has been devoted to creating innovative techniques for these procedures of CAD systems, no single best algorithm has emerged for any individual step. Ongoing studies in building innovative algorithms for all the aspects of CAD systems is essential.
There is also a lack of standardized assessment measures for CAD systems. This fact may cause the difficulty for obtaining approval for commercial use from governing bodies such as the FDA. Moreover, while many positive developments of CAD systems have been proven, studies for validating their algorithms for clinical practice have not been confirmed.
Other challenges are related to the problem for healthcare providers to adopt new CAD systems in clinical practice. Some negative studies may discourage the use of CAD. In addition, the lack of training of health professionals on the use of CAD sometimes brings the incorrect interpretation of the system outcomes.

Applications
CAD is used in the diagnosis of breast cancer, lung cancer, colon cancer, prostate cancer, bone metastases, coronary artery disease, congenital heart defect, pathological brain detection, fracture detection, Alzheimer's disease, and diabetic retinopathy.

Breast cancer
CAD is used in screening mammography (X-ray examination of the female breast). Screening mammography is used for the early detection of breast cancer. CAD systems are often utilized to help classify a tumor as malignant (cancerous) or benign (non-cancerous). CAD is especially established in the US and the Netherlands and is used in addition to human evaluation, usually by a radiologist.
The first CAD system for mammography was developed in a research project at the University of Chicago. Today it is commercially offered by iCAD and Hologic. However, while achieving high sensitivities, CAD systems tend to have very low specificity and the benefits of using CAD remain uncertain. A 2008 systematic review on computer-aided detection in screening mammography concluded that CAD does not have a significant effect on cancer detection rate, but does undesirably increase recall rate (i.e. the rate of false positives). However, it noted considerable heterogeneity in the impact on recall rate across studies.
Recent advances in machine learning, deep-learning and artificial intelligence technology have enabled the development of CAD systems that are clinically proven to assist radiologists in addressing the challenges of reading mammographic images by improving cancer detection rates and reducing false positives and unnecessary patient recalls, while significantly decreasing reading times.
Procedures to evaluate mammography based on magnetic resonance imaging (MRI) exist too.

Lung cancer (bronchial carcinoma)
In the diagnosis of lung cancer, computed tomography with special three-dimensional CAD systems are established and considered as appropriate second opinions. At this a volumetric dataset with up to 3,000 single images is prepared and analyzed. Round lesions (lung cancer, metastases and benign changes) from 1 mm are detectable. Today all well-known vendors of medical systems offer corresponding solutions.
Early detection of lung cancer is valuable. However, the random detection of lung cancer in the early stage (stage 1) in the X-ray image is difficult. Round lesions that vary from 5–10 mm are easily overlooked. The routine application of CAD Chest Systems may help to detect small changes without initial suspicion. A number of researchers developed CAD systems for detection of lung nodules (round lesions less than 30 mm) in chest radiography and CT, and CAD systems for diagnosis (e.g., distinction between malignant and benign) of lung nodules in CT. Virtual dual-energy imaging improved the performance of CAD systems in chest radiography.

Colon cancer
CAD is available for detection of colorectal polyps in the colon in CT colonography. Polyps are small growths that arise from the inner lining of the colon. CAD detects the polyps by identifying their characteristic "bump-like" shape. To avoid excessive false positives, CAD ignores the normal colon wall, including the haustral folds.

Cardiovascular disease
State-of-the-art methods in cardiovascular computing, cardiovascular informatics, and mathematical and computational modeling can provide valuable tools in clinical decision-making. CAD systems with novel image-analysis-based markers as input can aid vascular physicians to decide with higher confidence on best suitable treatment for cardiovascular disease patients.
Reliable early-detection and risk-stratification of carotid atherosclerosis is of outmost importance for predicting strokes in asymptomatic patients. To this end, various noninvasive and low-cost markers have been proposed, using ultrasound-image-based features. These combine echogenicity, texture, and motion characteristics to assist clinical decision towards improved prediction, assessment and management of cardiovascular risk.
CAD is available for the automatic detection of significant (causing more than 50% stenosis) coronary artery disease in coronary CT angiography (CCTA) studies.

Congenital heart defect
Early detection of pathology can be the difference between life and death. CADe can be done by auscultation with a digital stethoscope and specialized software, also known as computer-aided auscultation. Murmurs, irregular heart sounds, caused by blood flowing through a defective heart, can be detected with high sensitivity and specificity. Computer-aided auscultation is sensitive to external noise and bodily sounds and requires an almost silent environment to function accurately.

Pathological brain detection (PBD)
Chaplot et al. was the first to use Discrete Wavelet Transform (DWT) coefficients to detect pathological brains. Maitra and Chatterjee employed the Slantlet transform, which is an improved version of DWT. Their feature vector of each image is created by considering the magnitudes of Slantlet transform outputs corresponding to six spatial positions chosen according to a specific logic.
In 2010, Wang and Wu presented a forward neural network (FNN) based method to classify a given MR brain image as normal or abnormal. The parameters of FNN were optimized via adaptive chaotic particle swarm optimization (ACPSO). Results over 160 images showed that the classification accuracy was 98.75%.
In 2011, Wu and Wang proposed using DWT for feature extraction, PCA for feature reduction, and FNN with scaled chaotic artificial bee colony (SCABC) as classifier.
In 2013, Saritha et al. were the first to apply wavelet entropy (WE) to detect pathological brains. Saritha also suggested to use spider-web plots. Later, Zhang et al. proved removing spider-web plots did not influence the performance. Genetic pattern search method was applied to identify abnormal brain from normal controls. Its classification accuracy was reported as 95.188%. Das et al. proposed to use Ripplet transform. Zhang et al. proposed to use particle swarm optimization (PSO). Kalbkhani et al. suggested to use GARCH model.
In 2014, El-Dahshan et al. suggested the use of pulse coupled neural network.
In 2015, Zhou et al. suggested application of naive Bayes classifier to detect pathological brains.

Alzheimer's disease
CADs can be used to identify subjects with Alzheimer's and mild cognitive impairment from normal elder controls.
In 2014, Padma et al. used combined wavelet statistical texture features to segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel support vector machine decision tree had 80% classification accuracy, with an average computation time of 0.022s for each image classification.
In 2019, Signaevsky et al. have first reported a trained Fully Convolutional Network (FCN) for detection and quantification of neurofibrillary tangles (NFT) in Alzheimer's disease and an array of other tauopathies. The trained FCN achieved high precision and recall in naive digital whole slide image (WSI) semantic segmentation, correctly identifying NFT objects using a SegNet model trained for 200 epochs. The FCN reached near-practical efficiency with average processing time of 45 min per WSI per graphics processing unit (GPU), enabling reliable and reproducible large-scale detection of NFTs. The measured performance on test data of eight naive WSI across various tauopathies resulted in the recall, precision, and an F1 score of 0.92, 0.72, and 0.81, respectively.
Eigenbrain is a novel brain feature that can help to detect AD, based on principal component analysis (PCA) or independent component analysis decomposition. Polynomial kernel SVM has been shown to achieve good accuracy. The polynomial KSVM performs better than linear SVM and RBF kernel SVM. Other approaches with decent results involve the use of texture analysis, morphological features, or high-order statistical features

Nuclear medicine
CADx is available for nuclear medicine images. Commercial CADx systems for the diagnosis of bone metastases in whole-body bone scans and coronary artery disease in myocardial perfusion images exist.
With a high sensitivity and an acceptable false lesions detection rate, computer-aided automatic lesion detection system is demonstrated as useful and will probably in the future be able to help nuclear medicine physicians to identify possible bone lesions.

Diabetic retinopathy
Diabetic retinopathy is a disease of the retina that is diagnosed predominantly by fundoscopic images. Diabetic patients in industrialised countries generally undergo regular screening for the condition. Imaging is used to recognize early signs of abnormal retinal blood vessels. Manual analysis of these images can be time-consuming and unreliable. CAD has been employed to enhance the accuracy, sensitivity, and specificity of automated detection method. The use of some CAD systems to replace human graders can be safe and cost effective.
Image pre-processing, and feature extraction and classification are two main stages of these CAD algorithms.

Pre-processing methods
Image normalization is minimizing the variation across the entire image. Intensity variations in areas between periphery and central macular region of the eye have been reported to cause inaccuracy of vessel segmentation. Based on the 2014 review, this technique was the most frequently used and appeared in 11 out of 40 recently (since 2011) published primary research.

Histogram equalization is useful in enhancing contrast within an image. This technique is used to increase local contrast. At the end of the processing, areas that were dark in the input image would be brightened, greatly enhancing the contrast among the features present in the area. On the other hand, brighter areas in the input image would remain bright or be reduced in brightness to equalize with the other areas in the image. Besides vessel segmentation, other features related to diabetic retinopathy can be further separated by using this pre-processing technique. Microaneurysm and hemorrhages are red lesions, whereas exudates are yellow spots. Increasing contrast between these two groups allow better visualization of lesions on images. With this technique, 2014 review found that 10 out of the 14 recently (since 2011) published primary research.
Green channel filtering is another technique that is useful in differentiating lesions rather than vessels. This method is important because it provides the maximal contrast between diabetic retinopathy-related lesions. Microaneurysms and hemorrhages are red lesions that appear dark after application of green channel filtering. In contrast, exudates, which appear yellow in normal image, are transformed into bright white spots after green filtering. This technique is mostly used according to the 2014 review, with appearance in 27 out of 40 published articles in the past three years. In addition, green channel filtering can be used to detect center of optic disc in conjunction with double-windowing system.
Non-uniform illumination correction is a technique that adjusts for non-uniform illumination in fundoscopic image. Non-uniform illumination can be a potential error in automated detection of diabetic retinopathy because of changes in statistical characteristics of image. These changes can affect latter processing such as feature extraction and are not observable by humans. Correction of non-uniform illumination (f') can be achieved by modifying the pixel intensity using known original pixel intensity (f), and average intensities of local (λ) and desired pixels (μ) (see formula below). Walter-Klein transformation is then applied to achieve the uniform illumination. This technique is the least used pre-processing method in the review from 2014.

  
    
      
        
          f
          ′
        
        =
        f
        +
        μ
        −
        λ
      
    
    {\displaystyle f'=f+\mu -\lambda }
  

Morphological operations is the second least used pre-processing method in 2014 review. The main objective of this method is to provide contrast enhancement, especially darker regions compared to background.

Feature extractions and classifications
After pre-processing of funduscopic image, the image will be further analyzed using different computational methods. However, the current literature agreed that some methods are used more often than others during vessel segmentation analyses. These methods are SVM, multi-scale, vessel-tracking, region growing approach, and model-based approaches.

Support vector machine is by far the most frequently used classifier in vessel segmentation, up to 90% of cases. SVM is a supervised learning model that belongs to the broader category of pattern recognition technique. The algorithm works by creating a largest gap between distinct samples in the data. The goal is to create the largest gap between these components that minimize the potential error in classification. In order to successfully segregate blood vessel information from the rest of the eye image, SVM algorithm creates support vectors that separate the blood vessel pixel from the rest of the image through a supervised environment. Detecting blood vessel from new images can be done through similar manner using support vectors. Combination with other pre-processing technique, such as green channel filtering, greatly improves the accuracy of detection of blood vessel abnormalities. Some beneficial properties of SVM include 

Flexibility – Highly flexible in terms of function
Simplicity – Simple, especially with large datasets (only support vectors are needed to create separation between data)
Multi-scale approach is a multiple resolution approach in vessel segmentation. At low resolution, large-diameter vessels can first be extracted. By increasing resolution, smaller branches from the large vessels can be easily recognized. Therefore, one advantage of using this technique is the increased analytical speed. Additionally, this approach can be used with 3D images. The surface representation is a surface normal to the curvature of the vessels, allowing the detection of abnormalities on vessel surface.
Vessel tracking is the ability of the algorithm to detect "centerline" of vessels. These centerlines are maximal peak of vessel curvature. Centers of vessels can be found using directional information that is provided by Gaussian filter. Similar approaches that utilize the concept of centerline are the skeleton-based and differential geometry-based.
Region growing approach is a method of detecting neighboring pixels with similarities. A seed point is required for such method to start. Two elements are needed for this technique to work: similarity and spatial proximity. A neighboring pixel to the seed pixel with similar intensity is likely to be the same type and will be added to the growing region. One disadvantage of this technique is that it requires manual selection of seed point, which introduces bias and inconsistency in the algorithm. This technique is also being used in optic disc identification.
Model-based approaches employ representation to extract vessels from images. Three broad categories of model-based are known: deformable, parametric, and template matching. Deformable methods uses objects that will be deformed to fit the contours of the objects on the image. Parametric uses geometric parameters such as tubular, cylinder, or ellipsoid representation of blood vessels. Classical snake contour in combination with blood vessel topological information can also be used as a model-based approach. Lastly, template matching is the usage of a template, fitted by stochastic deformation process using Hidden Markov Mode 1.

Effects on employment
Automation of medical diagnosis labor (for example, quantifying red blood cells) has some historical precedent. The deep learning revolution of the 2010s has already produced AI that are more accurate in many areas of visual diagnosis than radiologists and dermatologists, and this gap is expected to grow. 
Some experts, including many doctors, are dismissive of the effects that AI will have on medical specialties. 
In contrast, many economists and artificial intelligence experts believe that fields such as radiology will be massively disrupted, with unemployment or downward pressure on the wages of radiologists; hospitals will need fewer radiologists overall, and many of the radiologists who still exist will require substantial retraining. Geoffrey Hinton, the "Godfather of deep learning", argues that in light of the likely advances expected in the next five or ten years, hospitals should immediately stop training radiologists, as their time-consuming and expensive training on visual diagnosis will soon be mostly obsolete, leading to a glut of traditional radiologists. 
An op-ed in JAMA argues that pathologists and radiologists should merge into a single "information specialist" role, and state that "To avoid being replaced by computers, radiologists must allow themselves to be displaced by computers." Information specialists would be trained in "Bayesian logic, statistics, data science", and some genomics and biometrics; manual visual pattern recognition would be greatly de-emphasized compared with current onerous radiology training.

See also
Computerized Systems Used In Clinical Trials
Diagnostic robot

Footnotes
References
External links
Digital Retinal Images for Vessel Extraction (DRIVE)
STructured Analysis of the REtina (STARE)
High-Resolution Fundus (HRF) Image Database
Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.
In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.

Overview
Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).
The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.

Are the actions deterministic or non-deterministic? For nondeterministic actions, are the associated probabilities available?
Are the state variables discrete or continuous? If they are discrete, do they have only a finite number of possible values?
Can the current state be observed unambiguously? There can be full observability and partial observability.
How many initial states are there, finite or arbitrarily many?
Do actions have a duration?
Can several actions be taken concurrently, or is only one action possible at a time?
Is the objective of a plan to reach a designated goal state, or to maximize a reward function?
Is there only one agent or are there several agents? Are the agents cooperative or selfish? Do all of the agents construct their own plans separately, or are the plans constructed centrally for all agents?
The simplest possible planning problem, known as the Classical Planning Problem, is determined by:

a unique known initial state,
durationless actions,
deterministic actions,
which can be taken only one at a time,
and a single agent.
Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.
Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.
With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.
Discrete-time Markov decision processes (MDP) are planning problems with:

durationless actions,
nondeterministic actions with probabilities,
full observability,
maximization of a reward function,
and a single agent.
When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP).
If there are more than one agent, we have multi-agent planning, which is closely related to game theory.

Domain independent planning
In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called "domain independent" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.

Planning domain modelling languages
The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.
An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.

Algorithms for planning
Classical planning
forward chaining state space search, possibly enhanced with heuristics
backward chaining search, possibly enhanced by the use of state constraints (see STRIPS, graphplan)
partial-order planning

Reduction to other problems
reduction to the propositional satisfiability problem (satplan).
reduction to model checking - both are essentially problems of traversing state spaces, and the classical planning problem corresponds to a subclass of model checking problems.

Temporal planning
Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied.

Probabilistic planning
Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.
With partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.

Preference-based planning
In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.

Conditional planning
Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree. The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.
An early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s. What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed. A major advantage of conditional planning is the ability to handle partial plans. An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems.

Contingency planning
We speak of "contingent planning" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning. The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.
Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete. A particular case of contiguous planning is represented by FOND problems - for "fully-observable and non-deterministic". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete and 2EXPTIME-complete if the goal is specified with LDLf.

Conformant planning
Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning, but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete, and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.

Deployment of planning systems
The Hubble Space Telescope uses a short-term system called SPSS and a long-term planning system called Spike .

See also
Action description language
Actor model
Applications of artificial intelligence
International Conference on Automated Planning and Scheduling
Constraint satisfaction problem
Reactive planning
Scheduling (computing)
Strategy (game theory)
Lists
List of SMT solvers
List of constraint programming languages
List of emerging technologies
Outline of artificial intelligence

References
Further reading
Vlahavas, I. "Planning and Scheduling". EETN. Archived from the original on 2013-12-22.

External links
International Conference on Automated Planning and Scheduling
Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major motivating factor for the development of computer science.

Logical foundations
While the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's Begriffsschrift (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic. His Foundations of Arithmetic, published in 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential Principia Mathematica, first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.
In 1929, Mojżesz Presburger showed that the first-order theory of the natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.
However, shortly after this positive result, Kurt Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems (1931), showing that in any sufficiently strong axiomatic system there are true statements that cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples of undecidable questions.

First implementations
Shortly after World War II, the first general-purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum-tube computer at the Institute for Advanced Study in Princeton, New Jersey. According to Davis, "Its great triumph was to prove that the sum of two even numbers is even". More ambitious was the Logic Theorist in 1956, a deduction system for the propositional logic of the Principia Mathematica, developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theorist constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the Principia.
The "heuristic" approach of the Logic Theorist tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle. In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.

Decidability of the problem
Depending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the common case of propositional logic, the problem is decidable but co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first-order predicate calculus, Gödel's completeness theorem states that the theorems (provable statements) are exactly the semantically valid well-formed formulas, so the valid formulas are computably enumerable: given unbounded resources, any valid formula can eventually be proven. However, invalid formulas (those that are not entailed by a given theory), cannot always be recognized.
The above applies to first-order theories, such as Peano arithmetic. However, for a specific model that may be described by a first-order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel's incompleteness theorem, we know that any consistent theory whose axioms are true for the natural numbers cannot prove all first-order statements true for the natural numbers, even if the list of axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first-order theory (such as the integers).

Related problems
A simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.
Since the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial, and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.
Proof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.
Another distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference. Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).
There are hybrid theorem proving systems that use model checking as an inference rule. There are also programs that were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof that was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs). Another example of a program-assisted proof is the one that shows that the game of Connect Four can always be won by the first player.

Applications
Commercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification. Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.
Other uses of theorem provers include program synthesis, constructing programs that satisfy a formal specification. Automated theorem provers have been integrated with proof assistants, including Isabelle/HOL.
Applications of theorem provers are also found in natural language processing and formal semantics, where they are used to analyze discourse representations.

First-order theorem proving
In the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using John Alan Robinson's resolution principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.
First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling fully automated systems. More expressive logics, such as higher-order logics, allow the convenient expression of a wider range of problems than first-order logic, but theorem proving for these logics is less well developed.

Relationship with SMT
There is substantial overlap between first-order automated theorem provers and SMT solvers. Generally, automated theorem provers focus on supporting full first-order logic with quantifiers, whereas SMT solvers focus more on supporting various theories (interpreted predicate symbols). ATPs excel at problems with lots of quantifiers, whereas SMT solvers do well on large problems without quantifiers. The line is blurry enough that some ATPs participate in SMT-COMP, while some SMT solvers participate in CASC.

Benchmarks, competitions, and sources
The quality of implemented systems has benefited from the existence of a large library of standard benchmark examples—the Thousands of Problems for Theorem Provers (TPTP) Problem Library—as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.
Some important systems (all have won at least one CASC competition division) are listed below.

E is a high-performance prover for full first-order logic, but built on a purely equational calculus, originally developed in the automated reasoning group of Technical University of Munich under the direction of Wolfgang Bibel, and now at Baden-Württemberg Cooperative State University in Stuttgart.
Otter, developed at the Argonne National Laboratory, is based on first-order resolution and paramodulation. Otter has since been replaced by Prover9, which is paired with Mace4.
SETHEO is a high-performance system based on the goal-directed model elimination calculus, originally developed by a team under direction of Wolfgang Bibel. E and SETHEO have been combined (with other systems) in the composite theorem prover E-SETHEO.
Vampire was originally developed and implemented at Manchester University by Andrei Voronkov and Kryštof Hoder. It is now developed by a growing international team. It has won the FOF division (among other divisions) at the CADE ATP System Competition regularly since 2001.
Waldmeister is a specialized system for unit-equational first-order logic developed by Arnim Buch and Thomas Hillenbrand. It won the CASC UEQ division for fourteen consecutive years (1997–2010).
SPASS is a first-order logic theorem prover with equality. This is developed by the research group Automation of Logic, Max Planck Institute for Computer Science.
The Theorem Prover Museum is an initiative to conserve the sources of theorem prover systems for future analysis, since they are important cultural/scientific artefacts. It has the sources of many of the systems mentioned above.

Popular techniques
First-order resolution with unification
Model elimination
Method of analytic tableaux
Superposition and term rewriting
Model checking
Mathematical induction
Binary decision diagrams
DPLL
Higher-order unification
Quantifier elimination

Software systems
Free software
Alt-Ergo
Automath
CVC
E
IsaPlanner
LCF
Mizar
NuPRL
Paradox
Prover9
PVS
SPARK (programming language)
Twelf
Z3 Theorem Prover

Proprietary software
CARINE
Wolfram Mathematica
ResearchCyc

See also
Notes
References
External links
A list of theorem proving tools
In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program.
Automatic differentiation exploits the fact that every computer calculation, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor of more arithmetic operations than the original program.

Difference from other differentiation methods
Automatic differentiation is distinct from symbolic differentiation and numerical differentiation. 
Symbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase. Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems.

Applications
Automatic differentiation is particularly important in the field of machine learning. For example, it allows one to implement backpropagation in a neural network without a manually-computed derivative.

Forward and reverse accumulation
Chain rule of partial derivatives of composite functions
Fundamental to automatic differentiation is the decomposition of differentials provided by the chain rule of partial derivatives of composite functions. For the simple composition

  
    
      
        
          
            
              
                y
              
              
                
                =
                f
                (
                g
                (
                h
                (
                x
                )
                )
                )
                =
                f
                (
                g
                (
                h
                (
                
                  w
                  
                    0
                  
                
                )
                )
                )
                =
                f
                (
                g
                (
                
                  w
                  
                    1
                  
                
                )
                )
                =
                f
                (
                
                  w
                  
                    2
                  
                
                )
                =
                
                  w
                  
                    3
                  
                
              
            
            
              
                
                  w
                  
                    0
                  
                
              
              
                
                =
                x
              
            
            
              
                
                  w
                  
                    1
                  
                
              
              
                
                =
                h
                (
                
                  w
                  
                    0
                  
                
                )
              
            
            
              
                
                  w
                  
                    2
                  
                
              
              
                
                =
                g
                (
                
                  w
                  
                    1
                  
                
                )
              
            
            
              
                
                  w
                  
                    3
                  
                
              
              
                
                =
                f
                (
                
                  w
                  
                    2
                  
                
                )
                =
                y
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}y&=f(g(h(x)))=f(g(h(w_{0})))=f(g(w_{1}))=f(w_{2})=w_{3}\\w_{0}&=x\\w_{1}&=h(w_{0})\\w_{2}&=g(w_{1})\\w_{3}&=f(w_{2})=y\end{aligned}}}
  

the chain rule gives

  
    
      
        
          
            
              ∂
              y
            
            
              ∂
              x
            
          
        
        =
        
          
            
              ∂
              y
            
            
              ∂
              
                w
                
                  2
                
              
            
          
        
        
          
            
              ∂
              
                w
                
                  2
                
              
            
            
              ∂
              
                w
                
                  1
                
              
            
          
        
        
          
            
              ∂
              
                w
                
                  1
                
              
            
            
              ∂
              x
            
          
        
        =
        
          
            
              ∂
              f
              (
              
                w
                
                  2
                
              
              )
            
            
              ∂
              
                w
                
                  2
                
              
            
          
        
        
          
            
              ∂
              g
              (
              
                w
                
                  1
                
              
              )
            
            
              ∂
              
                w
                
                  1
                
              
            
          
        
        
          
            
              ∂
              h
              (
              
                w
                
                  0
                
              
              )
            
            
              ∂
              x
            
          
        
      
    
    {\displaystyle {\frac {\partial y}{\partial x}}={\frac {\partial y}{\partial w_{2}}}{\frac {\partial w_{2}}{\partial w_{1}}}{\frac {\partial w_{1}}{\partial x}}={\frac {\partial f(w_{2})}{\partial w_{2}}}{\frac {\partial g(w_{1})}{\partial w_{1}}}{\frac {\partial h(w_{0})}{\partial x}}}

Two types of automatic differentiation
Usually, two distinct modes of automatic differentiation are presented.

forward accumulation (also called bottom-up, forward mode, or tangent mode)
reverse accumulation (also called top-down, reverse mode, or adjoint mode)
Forward accumulation specifies that one traverses the chain rule from inside to outside (that is, first compute 
  
    
      
        ∂
        
          w
          
            1
          
        
        
          /
        
        ∂
        x
      
    
    {\displaystyle \partial w_{1}/\partial x}
  
 and then 
  
    
      
        ∂
        
          w
          
            2
          
        
        
          /
        
        ∂
        
          w
          
            1
          
        
      
    
    {\displaystyle \partial w_{2}/\partial w_{1}}
  
 and at last 
  
    
      
        ∂
        y
        
          /
        
        ∂
        
          w
          
            2
          
        
      
    
    {\displaystyle \partial y/\partial w_{2}}
  
), while reverse accumulation has the traversal from outside to inside (first compute 
  
    
      
        ∂
        y
        
          /
        
        ∂
        
          w
          
            2
          
        
      
    
    {\displaystyle \partial y/\partial w_{2}}
  
 and then 
  
    
      
        ∂
        
          w
          
            2
          
        
        
          /
        
        ∂
        
          w
          
            1
          
        
      
    
    {\displaystyle \partial w_{2}/\partial w_{1}}
  
 and at last 
  
    
      
        ∂
        
          w
          
            1
          
        
        
          /
        
        ∂
        x
      
    
    {\displaystyle \partial w_{1}/\partial x}
  
). More succinctly,

Forward accumulation computes the recursive relation: 
  
    
      
        
          
            
              ∂
              
                w
                
                  i
                
              
            
            
              ∂
              x
            
          
        
        =
        
          
            
              ∂
              
                w
                
                  i
                
              
            
            
              ∂
              
                w
                
                  i
                  −
                  1
                
              
            
          
        
        
          
            
              ∂
              
                w
                
                  i
                  −
                  1
                
              
            
            
              ∂
              x
            
          
        
      
    
    {\displaystyle {\frac {\partial w_{i}}{\partial x}}={\frac {\partial w_{i}}{\partial w_{i-1}}}{\frac {\partial w_{i-1}}{\partial x}}}
  
 with 
  
    
      
        
          w
          
            3
          
        
        =
        y
      
    
    {\displaystyle w_{3}=y}
  
, and,
Reverse accumulation computes the recursive relation: 
  
    
      
        
          
            
              ∂
              y
            
            
              ∂
              
                w
                
                  i
                
              
            
          
        
        =
        
          
            
              ∂
              y
            
            
              ∂
              
                w
                
                  i
                  +
                  1
                
              
            
          
        
        
          
            
              ∂
              
                w
                
                  i
                  +
                  1
                
              
            
            
              ∂
              
                w
                
                  i
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial y}{\partial w_{i}}}={\frac {\partial y}{\partial w_{i+1}}}{\frac {\partial w_{i+1}}{\partial w_{i}}}}
  
 with 
  
    
      
        
          w
          
            0
          
        
        =
        x
      
    
    {\displaystyle w_{0}=x}
  
.
The value of the partial derivative, called seed, is propagated forward or backward and is initially 
  
    
      
        
          
            
              ∂
              x
            
            
              ∂
              x
            
          
        
        =
        1
      
    
    {\displaystyle {\frac {\partial x}{\partial x}}=1}
  
 or 
  
    
      
        
          
            
              ∂
              y
            
            
              ∂
              y
            
          
        
        =
        1
      
    
    {\displaystyle {\frac {\partial y}{\partial y}}=1}
  
. Forward accumulation evaluates the function and calculates the derivative with respect to one independent variable in one pass. For each independent variable 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},x_{2},\dots ,x_{n}}
  
 a separate pass is therefore necessary in which the derivative with respect to that independent variable is set to one (
  
    
      
        
          
            
              ∂
              
                x
                
                  1
                
              
            
            
              ∂
              
                x
                
                  1
                
              
            
          
        
        =
        1
      
    
    {\displaystyle {\frac {\partial x_{1}}{\partial x_{1}}}=1}
  
) and of all others to zero (
  
    
      
        
          
            
              ∂
              
                x
                
                  2
                
              
            
            
              ∂
              
                x
                
                  1
                
              
            
          
        
        =
        ⋯
        =
        
          
            
              ∂
              
                x
                
                  n
                
              
            
            
              ∂
              
                x
                
                  1
                
              
            
          
        
        =
        0
      
    
    {\displaystyle {\frac {\partial x_{2}}{\partial x_{1}}}=\dots ={\frac {\partial x_{n}}{\partial x_{1}}}=0}
  
). In contrast, reverse accumulation requires the evaluated partial functions for the partial derivatives. Reverse accumulation therefore evaluates the function first and calculates the derivatives with respect to all independent variables in an additional pass.
Which of these two types should be used depends on the sweep count. The computational complexity of one sweep is proportional to the complexity of the original code.

Forward accumulation is more efficient than reverse accumulation for functions f : Rn → Rm with n ≪ m as only n sweeps are necessary, compared to m sweeps for reverse accumulation.
Reverse accumulation is more efficient than forward accumulation for functions f : Rn → Rm with n ≫ m as only m sweeps are necessary, compared to n sweeps for forward accumulation.
Backpropagation of errors in multilayer perceptrons, a technique used in machine learning, is a special case of reverse accumulation.
Forward accumulation was introduced by R.E. Wengert in 1964. According to Andreas Griewank, reverse accumulation has been suggested since the late 1960s, but the inventor is unknown. Seppo Linnainmaa published reverse accumulation in 1976.

Forward accumulation
In forward accumulation AD, one first fixes the independent variable with respect to which differentiation is performed and computes the derivative of each sub-expression recursively. In a pen-and-paper calculation, this involves repeatedly substituting the derivative of the inner functions in the chain rule:

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      x
                    
                  
                
              
              
                
                =
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      
                        w
                        
                          n
                          −
                          1
                        
                      
                    
                  
                
                
                  
                    
                      ∂
                      
                        w
                        
                          n
                          −
                          1
                        
                      
                    
                    
                      ∂
                      x
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      
                        w
                        
                          n
                          −
                          1
                        
                      
                    
                  
                
                
                  (
                  
                    
                      
                        
                          ∂
                          
                            w
                            
                              n
                              −
                              1
                            
                          
                        
                        
                          ∂
                          
                            w
                            
                              n
                              −
                              2
                            
                          
                        
                      
                    
                    
                      
                        
                          ∂
                          
                            w
                            
                              n
                              −
                              2
                            
                          
                        
                        
                          ∂
                          x
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      
                        w
                        
                          n
                          −
                          1
                        
                      
                    
                  
                
                
                  (
                  
                    
                      
                        
                          ∂
                          
                            w
                            
                              n
                              −
                              1
                            
                          
                        
                        
                          ∂
                          
                            w
                            
                              n
                              −
                              2
                            
                          
                        
                      
                    
                    
                      (
                      
                        
                          
                            
                              ∂
                              
                                w
                                
                                  n
                                  −
                                  2
                                
                              
                            
                            
                              ∂
                              
                                w
                                
                                  n
                                  −
                                  3
                                
                              
                            
                          
                        
                        
                          
                            
                              ∂
                              
                                w
                                
                                  n
                                  −
                                  3
                                
                              
                            
                            
                              ∂
                              x
                            
                          
                        
                      
                      )
                    
                  
                  )
                
              
            
            
              
              
                
                =
                ⋯
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial y}{\partial x}}&={\frac {\partial y}{\partial w_{n-1}}}{\frac {\partial w_{n-1}}{\partial x}}\\[6pt]&={\frac {\partial y}{\partial w_{n-1}}}\left({\frac {\partial w_{n-1}}{\partial w_{n-2}}}{\frac {\partial w_{n-2}}{\partial x}}\right)\\[6pt]&={\frac {\partial y}{\partial w_{n-1}}}\left({\frac {\partial w_{n-1}}{\partial w_{n-2}}}\left({\frac {\partial w_{n-2}}{\partial w_{n-3}}}{\frac {\partial w_{n-3}}{\partial x}}\right)\right)\\[6pt]&=\cdots \end{aligned}}}
  

This can be generalized to multiple variables as a matrix product of Jacobians.
Compared to reverse accumulation, forward accumulation is natural and easy to implement as the flow of derivative information coincides with the order of evaluation. Each variable 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
 is augmented with its derivative 
  
    
      
        
          
            
              
                w
                ˙
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\dot {w}}_{i}}
  
 (stored as a numerical value, not a symbolic expression),

  
    
      
        
          
            
              
                w
                ˙
              
            
          
          
            i
          
        
        =
        
          
            
              ∂
              
                w
                
                  i
                
              
            
            
              ∂
              x
            
          
        
      
    
    {\displaystyle {\dot {w}}_{i}={\frac {\partial w_{i}}{\partial x}}}
  

as denoted by the dot. The derivatives are then computed in sync with the evaluation steps and combined with other derivatives via the chain rule.
Using the chain rule, if 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
 has predecessors in the computational graph:

  
    
      
        
          
            
              
                w
                ˙
              
            
          
          
            i
          
        
        =
        
          ∑
          
            j
            ∈
            {
            
              predecessors of i
            
            }
          
        
        
          
            
              ∂
              
                w
                
                  i
                
              
            
            
              ∂
              
                w
                
                  j
                
              
            
          
        
        
          
            
              
                w
                ˙
              
            
          
          
            j
          
        
      
    
    {\displaystyle {\dot {w}}_{i}=\sum _{j\in \{{\text{predecessors of i}}\}}{\frac {\partial w_{i}}{\partial w_{j}}}{\dot {w}}_{j}}
  

As an example, consider the function:

  
    
      
        
          
            
              
                y
              
              
                
                =
                f
                (
                
                  x
                  
                    1
                  
                
                ,
                
                  x
                  
                    2
                  
                
                )
              
            
            
              
              
                
                =
                
                  x
                  
                    1
                  
                
                
                  x
                  
                    2
                  
                
                +
                sin
                ⁡
                
                  x
                  
                    1
                  
                
              
            
            
              
              
                
                =
                
                  w
                  
                    1
                  
                
                
                  w
                  
                    2
                  
                
                +
                sin
                ⁡
                
                  w
                  
                    1
                  
                
              
            
            
              
              
                
                =
                
                  w
                  
                    3
                  
                
                +
                
                  w
                  
                    4
                  
                
              
            
            
              
              
                
                =
                
                  w
                  
                    5
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}y&=f(x_{1},x_{2})\\&=x_{1}x_{2}+\sin x_{1}\\&=w_{1}w_{2}+\sin w_{1}\\&=w_{3}+w_{4}\\&=w_{5}\end{aligned}}}
  

For clarity, the individual sub-expressions have been labeled with the variables 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
.
The choice of the independent variable to which differentiation is performed affects the seed values ẇ1 and ẇ2. Given interest in the derivative of this function with respect to x1, the seed values should be set to:

  
    
      
        
          
            
              
                
                  
                    
                      
                        w
                        ˙
                      
                    
                  
                  
                    1
                  
                
                =
                
                  
                    
                      ∂
                      
                        w
                        
                          1
                        
                      
                    
                    
                      ∂
                      
                        x
                        
                          1
                        
                      
                    
                  
                
                =
                
                  
                    
                      ∂
                      
                        x
                        
                          1
                        
                      
                    
                    
                      ∂
                      
                        x
                        
                          1
                        
                      
                    
                  
                
                =
                1
              
            
            
              
                
                  
                    
                      
                        w
                        ˙
                      
                    
                  
                  
                    2
                  
                
                =
                
                  
                    
                      ∂
                      
                        w
                        
                          2
                        
                      
                    
                    
                      ∂
                      
                        x
                        
                          1
                        
                      
                    
                  
                
                =
                
                  
                    
                      ∂
                      
                        x
                        
                          2
                        
                      
                    
                    
                      ∂
                      
                        x
                        
                          1
                        
                      
                    
                  
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\dot {w}}_{1}={\frac {\partial w_{1}}{\partial x_{1}}}={\frac {\partial x_{1}}{\partial x_{1}}}=1\\{\dot {w}}_{2}={\frac {\partial w_{2}}{\partial x_{1}}}={\frac {\partial x_{2}}{\partial x_{1}}}=0\end{aligned}}}
  

With the seed values set, the values propagate using the chain rule as shown. Figure 2 shows a pictorial depiction of this process as a computational graph.

To compute the gradient of this example function, which requires not only 
  
    
      
        
          
            
              
                ∂
                y
              
              
                ∂
                
                  x
                  
                    1
                  
                
              
            
          
        
      
    
    {\displaystyle {\tfrac {\partial y}{\partial x_{1}}}}
  
 but also 
  
    
      
        
          
            
              
                ∂
                y
              
              
                ∂
                
                  x
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\tfrac {\partial y}{\partial x_{2}}}}
  
, an additional sweep is performed over the computational graph using the seed values 
  
    
      
        
          
            
              
                w
                ˙
              
            
          
          
            1
          
        
        =
        0
        ;
        
          
            
              
                w
                ˙
              
            
          
          
            2
          
        
        =
        1
      
    
    {\displaystyle {\dot {w}}_{1}=0;{\dot {w}}_{2}=1}
  
.

Implementation
Pseudocode
Forward accumulation calculates the function and the derivative (but only for one independent variable each) in one pass. The associated method call expects the expression Z to be derived with regard to a variable V. The method returns a pair of the evaluated function and its derivative. The method traverses the expression tree recursively until a variable is reached. If the derivative with respect to this variable is requested, its derivative is 1, 0 otherwise. Then the partial function as well as the partial derivative are evaluated.

C++
Reverse accumulation
In reverse accumulation AD, the dependent variable to be differentiated is fixed and the derivative is computed with respect to each sub-expression recursively. In a pen-and-paper calculation, the derivative of the outer functions is repeatedly substituted in the chain rule:

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      x
                    
                  
                
              
              
                
                =
                
                  
                    
                      ∂
                      y
                    
                    
                      ∂
                      
                        w
                        
                          1
                        
                      
                    
                  
                
                
                  
                    
                      ∂
                      
                        w
                        
                          1
                        
                      
                    
                    
                      ∂
                      x
                    
                  
                
              
            
            
              
              
                
                =
                
                  (
                  
                    
                      
                        
                          ∂
                          y
                        
                        
                          ∂
                          
                            w
                            
                              2
                            
                          
                        
                      
                    
                    
                      
                        
                          ∂
                          
                            w
                            
                              2
                            
                          
                        
                        
                          ∂
                          
                            w
                            
                              1
                            
                          
                        
                      
                    
                  
                  )
                
                
                  
                    
                      ∂
                      
                        w
                        
                          1
                        
                      
                    
                    
                      ∂
                      x
                    
                  
                
              
            
            
              
              
                
                =
                
                  (
                  
                    
                      (
                      
                        
                          
                            
                              ∂
                              y
                            
                            
                              ∂
                              
                                w
                                
                                  3
                                
                              
                            
                          
                        
                        
                          
                            
                              ∂
                              
                                w
                                
                                  3
                                
                              
                            
                            
                              ∂
                              
                                w
                                
                                  2
                                
                              
                            
                          
                        
                      
                      )
                    
                    
                      
                        
                          ∂
                          
                            w
                            
                              2
                            
                          
                        
                        
                          ∂
                          
                            w
                            
                              1
                            
                          
                        
                      
                    
                  
                  )
                
                
                  
                    
                      ∂
                      
                        w
                        
                          1
                        
                      
                    
                    
                      ∂
                      x
                    
                  
                
              
            
            
              
              
                
                =
                ⋯
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial y}{\partial x}}&={\frac {\partial y}{\partial w_{1}}}{\frac {\partial w_{1}}{\partial x}}\\&=\left({\frac {\partial y}{\partial w_{2}}}{\frac {\partial w_{2}}{\partial w_{1}}}\right){\frac {\partial w_{1}}{\partial x}}\\&=\left(\left({\frac {\partial y}{\partial w_{3}}}{\frac {\partial w_{3}}{\partial w_{2}}}\right){\frac {\partial w_{2}}{\partial w_{1}}}\right){\frac {\partial w_{1}}{\partial x}}\\&=\cdots \end{aligned}}}
  

In reverse accumulation, the quantity of interest is the adjoint, denoted with a bar 
  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\bar {w}}_{i}}
  
; it is a derivative of a chosen dependent variable with respect to a subexpression 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
:

  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            i
          
        
        =
        
          
            
              ∂
              y
            
            
              ∂
              
                w
                
                  i
                
              
            
          
        
      
    
    {\displaystyle {\bar {w}}_{i}={\frac {\partial y}{\partial w_{i}}}}
  

Using the chain rule, if 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
 has successors in the computational graph:

  
    
      
        
          
            
              
                w
                ¯
              
            
          
          
            i
          
        
        =
        
          ∑
          
            j
            ∈
            {
            
              successors of i
            
            }
          
        
        
          
            
              
                w
                ¯
              
            
          
          
            j
          
        
        
          
            
              ∂
              
                w
                
                  j
                
              
            
            
              ∂
              
                w
                
                  i
                
              
            
          
        
      
    
    {\displaystyle {\bar {w}}_{i}=\sum _{j\in \{{\text{successors of i}}\}}{\bar {w}}_{j}{\frac {\partial w_{j}}{\partial w_{i}}}}
  

Reverse accumulation traverses the chain rule from outside to inside, or in the case of the computational graph in Figure 3, from top to bottom. The example function is scalar-valued, and thus there is only one seed for the derivative computation, and only one sweep of the computational graph is needed to calculate the (two-component) gradient. This is only half the work when compared to forward accumulation, but reverse accumulation requires the storage of the intermediate variables wi as well as the instructions that produced them in a data structure known as a "tape" or a Wengert list (however, Wengert published forward accumulation, not reverse accumulation), which may consume significant memory if the computational graph is large. This can be mitigated to some extent by storing only a subset of the intermediate variables and then reconstructing the necessary work variables by repeating the evaluations, a technique known as rematerialization. Checkpointing is also used to save intermediary states.

The operations to compute the derivative using reverse accumulation are shown in the table below (note the reversed order):

The data flow graph of a computation can be manipulated to calculate the gradient of its original calculation. This is done by adding an adjoint node for each primal node, connected by adjoint edges which parallel the primal edges but flow in the opposite direction. The nodes in the adjoint graph represent multiplication by the derivatives of the functions calculated by the nodes in the primal. For instance, addition in the primal causes fanout in the adjoint; fanout in the primal causes addition in the adjoint; a unary function y = f(x) in the primal causes x̄ = ȳ f′(x) in the adjoint; etc.

Implementation
Pseudo code
Reverse accumulation requires two passes: In the forward pass, the function is evaluated first and the partial results are cached. In the reverse pass, the partial derivatives are calculated and the previously derived value is backpropagated. The corresponding method call expects the expression Z to be derived and seed with the derived value of the parent expression. For the top expression, Z derived with regard to Z, this is 1. The method traverses the expression tree recursively until a variable is reached and adds the current seed value to the derivative expression.

C++
Beyond forward and reverse accumulation
Forward and reverse accumulation are just two (extreme) ways of traversing the chain rule. The problem of computing a full Jacobian of f : Rn → Rm with a minimum number of arithmetic operations is known as the optimal Jacobian accumulation (OJA) problem, which is NP-complete. Central to this proof is the idea that algebraic dependencies may exist between the local partials that label the edges of the graph. In particular, two or more edge labels may be recognized as equal. The complexity of the problem is still open if it is assumed that all edge labels are unique and algebraically independent.

Automatic differentiation using dual numbers
Forward mode automatic differentiation is accomplished by augmenting the algebra of real numbers and obtaining a new arithmetic. An additional component is added to every number to represent the derivative of a function at the number, and all arithmetic operators are extended for the augmented algebra. The augmented algebra is the algebra of dual numbers.
Replace every number 
  
    
      
        
        x
      
    
    {\displaystyle \,x}
  
 with the number 
  
    
      
        x
        +
        
          x
          ′
        
        ε
      
    
    {\displaystyle x+x'\varepsilon }
  
, where 
  
    
      
        
          x
          ′
        
      
    
    {\displaystyle x'}
  
 is a real number, but 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
 is an abstract number with the property 
  
    
      
        
          ε
          
            2
          
        
        =
        0
      
    
    {\displaystyle \varepsilon ^{2}=0}
  
 (an infinitesimal; see Smooth infinitesimal analysis). Using only this, regular arithmetic gives

  
    
      
        
          
            
              
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
                +
                (
                y
                +
                
                  y
                  ′
                
                ε
                )
              
              
                
                =
                x
                +
                y
                +
                (
                
                  x
                  ′
                
                +
                
                  y
                  ′
                
                )
                ε
              
            
            
              
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
                −
                (
                y
                +
                
                  y
                  ′
                
                ε
                )
              
              
                
                =
                x
                −
                y
                +
                (
                
                  x
                  ′
                
                −
                
                  y
                  ′
                
                )
                ε
              
            
            
              
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
                ⋅
                (
                y
                +
                
                  y
                  ′
                
                ε
                )
              
              
                
                =
                x
                y
                +
                x
                
                  y
                  ′
                
                ε
                +
                y
                
                  x
                  ′
                
                ε
                +
                
                  x
                  ′
                
                
                  y
                  ′
                
                
                  ε
                  
                    2
                  
                
                =
                x
                y
                +
                (
                x
                
                  y
                  ′
                
                +
                y
                
                  x
                  ′
                
                )
                ε
              
            
            
              
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
                
                  /
                
                (
                y
                +
                
                  y
                  ′
                
                ε
                )
              
              
                
                =
                (
                x
                
                  /
                
                y
                +
                
                  x
                  ′
                
                ε
                
                  /
                
                y
                )
                
                  /
                
                (
                1
                +
                
                  y
                  ′
                
                ε
                
                  /
                
                y
                )
                =
                (
                x
                
                  /
                
                y
                +
                
                  x
                  ′
                
                ε
                
                  /
                
                y
                )
                ⋅
                (
                1
                −
                
                  y
                  ′
                
                ε
                
                  /
                
                y
                )
                =
                x
                
                  /
                
                y
                +
                (
                
                  x
                  ′
                
                
                  /
                
                y
                −
                x
                
                  y
                  ′
                
                
                  /
                
                
                  y
                  
                    2
                  
                
                )
                ε
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}(x+x'\varepsilon )+(y+y'\varepsilon )&=x+y+(x'+y')\varepsilon \\(x+x'\varepsilon )-(y+y'\varepsilon )&=x-y+(x'-y')\varepsilon \\(x+x'\varepsilon )\cdot (y+y'\varepsilon )&=xy+xy'\varepsilon +yx'\varepsilon +x'y'\varepsilon ^{2}=xy+(xy'+yx')\varepsilon \\(x+x'\varepsilon )/(y+y'\varepsilon )&=(x/y+x'\varepsilon /y)/(1+y'\varepsilon /y)=(x/y+x'\varepsilon /y)\cdot (1-y'\varepsilon /y)=x/y+(x'/y-xy'/y^{2})\varepsilon \end{aligned}}}
  

using 
  
    
      
        (
        1
        +
        
          y
          ′
        
        ε
        
          /
        
        y
        )
        ⋅
        (
        1
        −
        
          y
          ′
        
        ε
        
          /
        
        y
        )
        =
        1
      
    
    {\displaystyle (1+y'\varepsilon /y)\cdot (1-y'\varepsilon /y)=1}
  
.
Now, polynomials can be calculated in this augmented arithmetic. If 
  
    
      
        P
        (
        x
        )
        =
        
          p
          
            0
          
        
        +
        
          p
          
            1
          
        
        x
        +
        
          p
          
            2
          
        
        
          x
          
            2
          
        
        +
        ⋯
        +
        
          p
          
            n
          
        
        
          x
          
            n
          
        
      
    
    {\displaystyle P(x)=p_{0}+p_{1}x+p_{2}x^{2}+\cdots +p_{n}x^{n}}
  
, then

  
    
      
        
          
            
              
                P
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
              
              
                
                =
                
                  p
                  
                    0
                  
                
                +
                
                  p
                  
                    1
                  
                
                (
                x
                +
                
                  x
                  ′
                
                ε
                )
                +
                ⋯
                +
                
                  p
                  
                    n
                  
                
                (
                x
                +
                
                  x
                  ′
                
                ε
                
                  )
                  
                    n
                  
                
              
            
            
              
              
                
                =
                
                  p
                  
                    0
                  
                
                +
                
                  p
                  
                    1
                  
                
                x
                +
                ⋯
                +
                
                  p
                  
                    n
                  
                
                
                  x
                  
                    n
                  
                
                +
                
                  p
                  
                    1
                  
                
                
                  x
                  ′
                
                ε
                +
                2
                
                  p
                  
                    2
                  
                
                x
                
                  x
                  ′
                
                ε
                +
                ⋯
                +
                n
                
                  p
                  
                    n
                  
                
                
                  x
                  
                    n
                    −
                    1
                  
                
                
                  x
                  ′
                
                ε
              
            
            
              
              
                
                =
                P
                (
                x
                )
                +
                
                  P
                  
                    (
                    1
                    )
                  
                
                (
                x
                )
                
                  x
                  ′
                
                ε
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}P(x+x'\varepsilon )&=p_{0}+p_{1}(x+x'\varepsilon )+\cdots +p_{n}(x+x'\varepsilon )^{n}\\&=p_{0}+p_{1}x+\cdots +p_{n}x^{n}+p_{1}x'\varepsilon +2p_{2}xx'\varepsilon +\cdots +np_{n}x^{n-1}x'\varepsilon \\&=P(x)+P^{(1)}(x)x'\varepsilon \end{aligned}}}
  

where 
  
    
      
        
          P
          
            (
            1
            )
          
        
      
    
    {\displaystyle P^{(1)}}
  
 denotes the derivative of 
  
    
      
        P
      
    
    {\displaystyle P}
  
 with respect to its first argument, and 
  
    
      
        
          x
          ′
        
      
    
    {\displaystyle x'}
  
, called a seed, can be chosen arbitrarily.
The new arithmetic consists of ordered pairs, elements written 
  
    
      
        ⟨
        x
        ,
        
          x
          ′
        
        ⟩
      
    
    {\displaystyle \langle x,x'\rangle }
  
, with ordinary arithmetics on the first component, and first order differentiation arithmetic on the second component, as described above. Extending the above results on polynomials to analytic functions gives a list of the basic arithmetic and some standard functions for the new arithmetic:

  
    
      
        
          
            
              
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
                +
                
                  ⟨
                  
                    v
                    ,
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    u
                    +
                    v
                    ,
                    
                      u
                      ′
                    
                    +
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
            
            
              
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
                −
                
                  ⟨
                  
                    v
                    ,
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    u
                    −
                    v
                    ,
                    
                      u
                      ′
                    
                    −
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
            
            
              
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
                ∗
                
                  ⟨
                  
                    v
                    ,
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    u
                    v
                    ,
                    
                      u
                      ′
                    
                    v
                    +
                    u
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
            
            
              
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
                
                  /
                
                
                  ⟨
                  
                    v
                    ,
                    
                      v
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    
                      
                        u
                        v
                      
                    
                    ,
                    
                      
                        
                          
                            u
                            ′
                          
                          v
                          −
                          u
                          
                            v
                            ′
                          
                        
                        
                          v
                          
                            2
                          
                        
                      
                    
                  
                  ⟩
                
                
                (
                v
                ≠
                0
                )
              
            
            
              
                sin
                ⁡
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    sin
                    ⁡
                    (
                    u
                    )
                    ,
                    
                      u
                      ′
                    
                    cos
                    ⁡
                    (
                    u
                    )
                  
                  ⟩
                
              
            
            
              
                cos
                ⁡
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    cos
                    ⁡
                    (
                    u
                    )
                    ,
                    −
                    
                      u
                      ′
                    
                    sin
                    ⁡
                    (
                    u
                    )
                  
                  ⟩
                
              
            
            
              
                exp
                ⁡
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    exp
                    ⁡
                    u
                    ,
                    
                      u
                      ′
                    
                    exp
                    ⁡
                    u
                  
                  ⟩
                
              
            
            
              
                log
                ⁡
                
                  ⟨
                  
                    u
                    ,
                    
                      u
                      ′
                    
                  
                  ⟩
                
              
              
                
                =
                
                  ⟨
                  
                    log
                    ⁡
                    (
                    u
                    )
                    ,
                    
                      u
                      ′
                    
                    
                      /
                    
                    u
                  
                  ⟩
                
                
                (
                u
                >
                0
                )
              
            
            
              
                
                  
                    ⟨
                    
                      u
                      ,
                      
                        u
                        ′
                      
                    
                    ⟩
                  
                  
                    k
                  
                
              
              
                
                =
                
                  ⟨
                  
                    
                      u
                      
                        k
                      
                    
                    ,
                    
                      u
                      ′
                    
                    k
                    
                      u
                      
                        k
                        −
                        1
                      
                    
                  
                  ⟩
                
                
                (
                u
                ≠
                0
                )
              
            
            
              
                
                  |
                  
                    ⟨
                    
                      u
                      ,
                      
                        u
                        ′
                      
                    
                    ⟩
                  
                  |
                
              
              
                
                =
                
                  ⟨
                  
                    
                      |
                      u
                      |
                    
                    ,
                    
                      u
                      ′
                    
                    sign
                    ⁡
                    u
                  
                  ⟩
                
                
                (
                u
                ≠
                0
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\left\langle u,u'\right\rangle +\left\langle v,v'\right\rangle &=\left\langle u+v,u'+v'\right\rangle \\\left\langle u,u'\right\rangle -\left\langle v,v'\right\rangle &=\left\langle u-v,u'-v'\right\rangle \\\left\langle u,u'\right\rangle *\left\langle v,v'\right\rangle &=\left\langle uv,u'v+uv'\right\rangle \\\left\langle u,u'\right\rangle /\left\langle v,v'\right\rangle &=\left\langle {\frac {u}{v}},{\frac {u'v-uv'}{v^{2}}}\right\rangle \quad (v\neq 0)\\\sin \left\langle u,u'\right\rangle &=\left\langle \sin(u),u'\cos(u)\right\rangle \\\cos \left\langle u,u'\right\rangle &=\left\langle \cos(u),-u'\sin(u)\right\rangle \\\exp \left\langle u,u'\right\rangle &=\left\langle \exp u,u'\exp u\right\rangle \\\log \left\langle u,u'\right\rangle &=\left\langle \log(u),u'/u\right\rangle \quad (u>0)\\\left\langle u,u'\right\rangle ^{k}&=\left\langle u^{k},u'ku^{k-1}\right\rangle \quad (u\neq 0)\\\left|\left\langle u,u'\right\rangle \right|&=\left\langle \left|u\right|,u'\operatorname {sign} u\right\rangle \quad (u\neq 0)\end{aligned}}}
  

and in general for the primitive function 
  
    
      
        g
      
    
    {\displaystyle g}
  
,

  
    
      
        g
        (
        ⟨
        u
        ,
        
          u
          ′
        
        ⟩
        ,
        ⟨
        v
        ,
        
          v
          ′
        
        ⟩
        )
        =
        ⟨
        g
        (
        u
        ,
        v
        )
        ,
        
          g
          
            u
          
        
        (
        u
        ,
        v
        )
        
          u
          ′
        
        +
        
          g
          
            v
          
        
        (
        u
        ,
        v
        )
        
          v
          ′
        
        ⟩
      
    
    {\displaystyle g(\langle u,u'\rangle ,\langle v,v'\rangle )=\langle g(u,v),g_{u}(u,v)u'+g_{v}(u,v)v'\rangle }
  

where 
  
    
      
        
          g
          
            u
          
        
      
    
    {\displaystyle g_{u}}
  
 and 
  
    
      
        
          g
          
            v
          
        
      
    
    {\displaystyle g_{v}}
  
 are the derivatives of 
  
    
      
        g
      
    
    {\displaystyle g}
  
 with respect to its first and second arguments, respectively.
When a binary basic arithmetic operation is applied to mixed arguments—the pair 
  
    
      
        ⟨
        u
        ,
        
          u
          ′
        
        ⟩
      
    
    {\displaystyle \langle u,u'\rangle }
  
 and the real number 
  
    
      
        c
      
    
    {\displaystyle c}
  
—the real number is first lifted to 
  
    
      
        ⟨
        c
        ,
        0
        ⟩
      
    
    {\displaystyle \langle c,0\rangle }
  
. The derivative of a function 
  
    
      
        f
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} }
  
 at the point 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
 is now found by calculating 
  
    
      
        f
        (
        ⟨
        
          x
          
            0
          
        
        ,
        1
        ⟩
        )
      
    
    {\displaystyle f(\langle x_{0},1\rangle )}
  
 using the above arithmetic, which gives 
  
    
      
        ⟨
        f
        (
        
          x
          
            0
          
        
        )
        ,
        
          f
          ′
        
        (
        
          x
          
            0
          
        
        )
        ⟩
      
    
    {\displaystyle \langle f(x_{0}),f'(x_{0})\rangle }
  
 as the result.

Implementation
An example implementation based on the dual number approach follows.

Pseudo code
Dual plus(Dual A, Dual B) {
  return {
    realPartOf(A) + realPartOf(B),
    infinitesimalPartOf(A) + infinitesimalPartOf(B)
  };
}
Dual minus(Dual A, Dual B) {
  return {
    realPartOf(A) - realPartOf(B),
    infinitesimalPartOf(A) - infinitesimalPartOf(B)
  };
}
Dual multiply(Dual A, Dual B) {
  return {
    realPartOf(A) * realPartOf(B),
    realPartOf(B) * infinitesimalPartOf(A) + realPartOf(A) * infinitesimalPartOf(B)
  };
}
X = {x, 0};
Y = {y, 0};
Epsilon = {0, 1};
xPartial = infinitesimalPartOf(f(X + Epsilon, Y));
yPartial = infinitesimalPartOf(f(X, Y + Epsilon));

C++
Vector arguments and functions
Multivariate functions can be handled with the same efficiency and mechanisms as univariate functions by adopting a directional derivative operator. That is, if it is sufficient to compute 
  
    
      
        
          y
          ′
        
        =
        ∇
        f
        (
        x
        )
        ⋅
        
          x
          ′
        
      
    
    {\displaystyle y'=\nabla f(x)\cdot x'}
  
, the directional derivative 
  
    
      
        
          y
          ′
        
        ∈
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle y'\in \mathbb {R} ^{m}}
  
 of 
  
    
      
        f
        :
        
          
            R
          
          
            n
          
        
        →
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} ^{m}}
  
 at 
  
    
      
        x
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{n}}
  
 in the direction 
  
    
      
        
          x
          ′
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x'\in \mathbb {R} ^{n}}
  
 may be calculated as 
  
    
      
        (
        ⟨
        
          y
          
            1
          
        
        ,
        
          y
          
            1
          
          ′
        
        ⟩
        ,
        …
        ,
        ⟨
        
          y
          
            m
          
        
        ,
        
          y
          
            m
          
          ′
        
        ⟩
        )
        =
        f
        (
        ⟨
        
          x
          
            1
          
        
        ,
        
          x
          
            1
          
          ′
        
        ⟩
        ,
        …
        ,
        ⟨
        
          x
          
            n
          
        
        ,
        
          x
          
            n
          
          ′
        
        ⟩
        )
      
    
    {\displaystyle (\langle y_{1},y'_{1}\rangle ,\ldots ,\langle y_{m},y'_{m}\rangle )=f(\langle x_{1},x'_{1}\rangle ,\ldots ,\langle x_{n},x'_{n}\rangle )}
  
 using the same arithmetic as above. If all the elements of 
  
    
      
        ∇
        f
      
    
    {\displaystyle \nabla f}
  
 are desired, then 
  
    
      
        n
      
    
    {\displaystyle n}
  
 function evaluations are required. Note that in many optimization applications, the directional derivative is indeed sufficient.

High order and many variables
The above arithmetic can be generalized to calculate second order and higher derivatives of multivariate functions. However, the arithmetic rules quickly grow complicated: complexity is quadratic in the highest derivative degree. Instead, truncated Taylor polynomial algebra can be used. The resulting arithmetic, defined on generalized dual numbers, allows efficient computation using functions as if they were a data type. Once the Taylor polynomial of a function is known, the derivatives are easily extracted.

Implementation
Forward-mode AD is implemented by a nonstandard interpretation of the program in which real numbers are replaced by dual numbers, constants are lifted to dual numbers with a zero epsilon coefficient, and the numeric primitives are lifted to operate on dual numbers. This nonstandard interpretation is generally implemented using one of two strategies: source code transformation or operator overloading.

Source code transformation (SCT)
The source code for a function is replaced by an automatically generated source code that includes statements for calculating the derivatives interleaved with the original instructions.
Source code transformation can be implemented for all programming languages, and it is also easier for the compiler to do compile time optimizations. However, the implementation of the AD tool itself is more difficult and the build system is more complex.

Operator overloading (OO)
Operator overloading is a possibility for source code written in a language supporting it. Objects for real numbers and elementary mathematical operations must be overloaded to cater for the augmented arithmetic depicted above. This requires no change in the form or sequence of operations in the original source code for the function to be differentiated, but often requires changes in basic data types for numbers and vectors to support overloading and often also involves the insertion of special flagging operations. Due to the inherent operator overloading overhead on each loop, this approach usually demonstrates weaker speed performance.

Operator overloading and source code transformation
Overloaded Operators can be used to extract the valuation graph, followed by automatic generation of the AD-version of the primal function at run-time. Unlike the classic OO AAD, such AD-function does not change from one iteration to the next one. Hence there is any OO or tape interpretation run-time overhead per Xi sample.
With the AD-function being generated at runtime, it can be optimised to take into account the current state of the program and precompute certain values. In addition, it can be generated in a way to consistently utilize native CPU vectorization to process 4(8)-double chunks of user data (AVX2\AVX512 speed up x4-x8). With multithreading added into account, such approach can lead to a final acceleration of order 8 × #Cores compared to the traditional AAD tools. A reference implementation is available on GitHub.

See also
Differentiable programming

Notes
References
Further reading
Rall, Louis B. (1981). Automatic Differentiation: Techniques and Applications. Lecture Notes in Computer Science. Vol. 120. Springer. ISBN 978-3-540-10861-0.
Griewank, Andreas; Walther, Andrea (2008). Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Other Titles in Applied Mathematics. Vol. 105 (2nd ed.). SIAM. doi:10.1137/1.9780898717761. ISBN 978-0-89871-659-7.
Neidinger, Richard (2010). "Introduction to Automatic Differentiation and MATLAB Object-Oriented Programming" (PDF). SIAM Review. 52 (3): 545–563. CiteSeerX 10.1.1.362.6580. doi:10.1137/080743627. S2CID 17134969. Retrieved 2013-03-15.
Naumann, Uwe (2012). The Art of Differentiating Computer Programs. Software-Environments-tools. SIAM. ISBN 978-1-611972-06-1.
Henrard, Marc (2017). Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Palgrave Macmillan. ISBN 978-3-319-53978-2.

External links
www.autodiff.org, An "entry site to everything you want to know about automatic differentiation"
Automatic Differentiation of Parallel OpenMP Programs
Automatic Differentiation, C++ Templates and Photogrammetry
Automatic Differentiation, Operator Overloading Approach
Compute analytic derivatives of any Fortran77, Fortran95, or C program through a web-based interface Automatic Differentiation of Fortran programs
Description and example code for forward Automatic Differentiation in Scala Archived 2016-08-03 at the Wayback Machine
finmath-lib stochastic automatic differentiation, Automatic differentiation for random variables (Java implementation of the stochastic automatic differentiation).
Adjoint Algorithmic Differentiation: Calibration and Implicit Function Theorem
C++ Template-based automatic differentiation article and implementation
Tangent Source-to-Source Debuggable Derivatives
Exact First- and Second-Order Greeks by Algorithmic Differentiation
Adjoint Algorithmic Differentiation of a GPU Accelerated Application
Adjoint Methods in Computational Finance Software Tool Support for Algorithmic Differentiationop
More than a Thousand Fold Speed Up for xVA Pricing Calculations with Intel Xeon Scalable Processors
A self-driving car, also known as an autonomous car (AC), driverless car, robotaxi, robotic car or robo-car, is a car that is capable of operating with reduced or no human input. Self-driving cars are responsible for all driving activities, such as perceiving the environment, monitoring important systems, and controlling the vehicle, which includes navigating from origin to destination.
As of early 2024, no system has achieved full autonomy (SAE Level 5). In December 2020, Waymo was the first to offer rides in self-driving taxis to the public in limited geographic areas (SAE Level 4), and as of April 2024 offers services in Arizona (Phoenix) and California (San Francisco and Los Angeles). In June 2024, after a Waymo self-driving taxi crashed into a utility pole in Phoenix, Arizona,  all 672 of its Jaguar I-Pace were recalled after they were found to have susceptibility to crashing into pole like items and had their software updated. In July 2021, DeepRoute.ai started offering self-driving taxi rides in Shenzhen, China. Starting in February 2022, Cruise offered self-driving taxi service in San Francisco, but suspended service in 2023. In 2021, Honda was the first manufacturer to sell an SAE Level 3 car, followed by Mercedes-Benz in 2023.

History
Experiments have been conducted on advanced driver assistance systems (ADAS) since at least the 1920s. The first ADAS system was cruise control, which was invented in 1948 by Ralph Teetor.
Trials began in the 1950s. The first semi-autonomous car was developed in 1977, by Japan's Tsukuba Mechanical Engineering Laboratory. It required specially marked streets that were interpreted by two cameras on the vehicle and an analog computer. The vehicle reached speeds of 30 km/h (19 mph) with the support of an elevated rail.
Carnegie Mellon University's Navlab and ALV semi-autonomous projects launched in the 1980s, funded by the United States' Defense Advanced Research Projects Agency (DARPA) starting in 1984 and Mercedes-Benz and Bundeswehr University Munich's EUREKA Prometheus Project in 1987. By 1985, ALV had reached 31 km/h (19 mph), on two-lane roads. Obstacle avoidance came in 1986, and day and night off-road driving by 1987. In 1995 Navlab 5 completed the first autonomous US coast-to-coast journey. Traveling from Pittsburgh, Pennsylvania and San Diego, California, 98.2% of the trip was autonomous. It completed the trip at an average speed of 63.8 mph (102.7 km/h). Until the second DARPA Grand Challenge in 2005, automated vehicle research in the United States was primarily funded by DARPA, the US Army, and the US Navy, yielding incremental advances in speeds, driving competence, controls, and sensor systems.
The US allocated US$650 million in 1991 for research on the National Automated Highway System, which demonstrated automated driving, combining highway-embedded automation with vehicle technology, and cooperative networking between the vehicles and highway infrastructure. The programme concluded with a successful demonstration in 1997. Partly funded by the National Automated Highway System and DARPA, Navlab drove 4,584 km (2,848 mi) across the US in 1995, 4,501 km (2,797 mi) or 98% autonomously. In 2015, Delphi piloted a Delphi technology-based Audi, over 5,472 km (3,400 mi) through 15 states, 99% autonomously. In 2015, Nevada, Florida, California, Virginia, Michigan, and Washington DC allowed autonomous car testing on public roads.
From 2016 to 2018, the European Commission funded development for connected and automated driving through Coordination Actions CARTRE and SCOUT programs. The Strategic Transport Research and Innovation Agenda (STRIA) Roadmap for Connected and Automated Transport was published in 2019.
In November 2017, Waymo announced testing of autonomous cars without a safety driver. However, an employee was in the car to handle emergencies.
In March 2018, Elaine Herzberg became the first reported pedestrian killed by a self-driving car, an Uber test vehicle with a human backup driver; prosecutors did not charge Uber, while the human driver was sentenced to probation.
In December 2018, Waymo was the first to commercialize a robotaxi service, in Phoenix, Arizona. In October 2020, Waymo launched a robotaxi service in a (geofenced) part of the area. The cars were monitored in real-time, and remote engineers intervened to handle exceptional conditions.
In March 2019, ahead of Roborace, Robocar set the Guinness World Record as the world's fastest autonomous car. Robocar reached 282.42 km/h (175.49 mph).
In March 2021, Honda began leasing in Japan a limited edition of 100 Legend Hybrid EX sedans equipped with Level 3 "Traffic Jam Pilot" driving technology, which legally allowed drivers to take their eyes off the road when the car was travelling under 30 kilometres per hour (19 mph).
In December 2020, Waymo became the first service provider to offer driverless taxi rides to the general public, in a part of Phoenix, Arizona. Nuro began autonomous commercial delivery operations in California in 2021. DeepRoute.ai launched robotaxi service in Shenzhen in July 2021. In December 2021, Mercedes-Benz received approval for a Level 3 car. In February 2022, Cruise became the second service provider to offer driverless taxi rides to the general public, in San Francisco. In December 2022, several manufacturers scaled back plans for self-driving technology, including Ford and Volkswagen. In 2023, Cruise suspended its robotaxi service. Nuro was approved for Level 4 in Palo Alto in August, 2023.
As of August 2023, vehicles operating at Level 3 and above were an insignificant market factor; as of early 2024, Honda leases a Level 3 car in Japan, and Mercedes sells two Level 3 cars in Germany, California and Nevada.

Definitions
Organizations such as SAE have proposed terminology standards. However, most terms have no standard definition and are employed variously by vendors and others. Proposals to adopt aviation automation terminology for cars have not prevailed.
Names such as AutonoDrive, PilotAssist, Full-Self Driving or DrivePilot are used even though the products offer an assortment of features that may not match the names. Despite offering a system ot called Full Self-Driving, Tesla stated that its system did not autonomously handle all driving tasks. In the United Kingdom, a fully self-driving car is defined as a car so registered, rather than one that supports a specific feature set. The Association of British Insurers claimed that the usage of the word autonomous in marketing was dangerous because car ads make motorists think "autonomous" and "autopilot" imply that the driver can rely on the car to control itself, even though they do not.

Automated driving system
An ADS is an SAE J3016 level 3 or higher system.

Advanced driver assistance system
An ADAS is a system that automates specific driving features, such as keeping the car within its lane, cruise control, and emergency braking. An ADAS requires a human driver to handle tasks that the ADAS does not support.

Autonomy versus automation
Autonomy implies that an automation system is under the control of the vehicle rather than a driver. Automation is function-specific, handling issues such as speed control, but leaves broader decision-making to the driver.
Euro NCAP defined autonomous as "the system acts independently of the driver to avoid or mitigate the accident".
In Europe, the words automated and autonomous can be used together. For instance, Regulation (EU) 2019/2144 supplied:

"automated vehicle" means a vehicle that can move without continuous driver supervision, but that driver intervention is still expected or required in the operational design domains (ODD);
"fully automated vehicle" means a vehicle that can move entirely without driver supervision;

Cooperative system
A remote driver is a driver that operates a vehicle at a distance, using a video and data connection.

According to SAE J3016, Some driving automation systems may indeed be autonomous if they perform all of their functions independently and self-sufficiently, but if they depend on communication and/or cooperation with outside entities, they should be considered cooperative rather than autonomous.

Operational design domain
Vendors have taken a variety of approaches to the self-driving problem. Tesla's approach is to allow their "full self-driving" (FSD) system to be used in all ODDs as a Level 2 (hands/on, eyes/on) ADAS. Waymo picked specific ODDs (city streets in Phoenix and San Francisco) for their Level 5 robotaxi service. Mercedes Benz offers Level 3 service in Las Vegas in highway traffic jams at speeds up to 40 miles per hour (64 km/h). Mobileye's SuperVision system offers hands-off/eyes-on driving on all road types at speeds up to 130 kilometres per hour (81 mph). GM's hands-free Super Cruise operates on specific roads in specific conditions, stopping or returning control to the driver when ODD changes. In 2024 the company announced plans to expand road coverage from 400,000 miles to 750,000 miles. Ford's BlueCruise hands-off system operates on 130,000 miles of US divided highways.

Self-driving
The Union of Concerned Scientists defined self-driving as "cars or trucks in which human drivers are never required to take control to safely operate the vehicle. Also known as autonomous or 'driverless' cars, they combine sensors and software to control, navigate, and drive the vehicle."
The British Automated and Electric Vehicles Act 2018 law defines a vehicle as "driving itself" if the vehicle is "not being controlled, and does not need to be monitored, by an individual".
Another British government definition stated, "Self-driving vehicles are vehicles that can safely and lawfully drive themselves".

British definitions
In British English, the word automated alone has several meanings, such as in the sentence: "Thatcham also found that the automated lane keeping systems could only meet two out of the twelve principles required to guarantee safety, going on to say they cannot, therefore, be classed as 'automated driving', preferring 'assisted driving'". The first occurrence of the "automated" word refers to an Unece automated system, while the second refers to the British legal definition of an automated vehicle. British law interprets the meaning of "automated vehicle" based on the interpretation section related to a vehicle "driving itself" and an insured vehicle.
In November 2023 the British Government introduced the Automated Vehicles Bill. It proposed definitions for related terms:

Self-driving: "A vehicle “satisfies the self-driving test” if it is designed or adapted with the intention that a feature of the vehicle will allow it to travel autonomously, and it is capable of doing so, by means of that feature, safely and legally."
Autonomy: A vehicle travels “autonomously” if it is controlled by the vehicle, and neither the vehicle nor its surroundings are monitored by a person who can intervene.
Control: control of vehicle motion.
Safe: a vehicle that conforms to an acceptably safe standard.
Legal: a vehicle that offers an acceptably low risk of committing a traffic infraction.

SAE classification
A six-level classification system – ranging from fully manual to fully automated – was published in 2014 by SAE International as J3016, Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems; the details are revised occasionally. This classification is based on the role of the driver, rather than the vehicle's capabilities, although these are related. After SAE updated its classification in 2016, (J3016_201609), the National Highway Traffic Safety Administration (NHTSA) adopted the SAE standard. The classification is a topic of debate, with various revisions proposed.

Classifications
A "driving mode", aka driving scenario, combines an ODD with matched driving requirements (e.g., expressway merging, traffic jam). Cars may switch levels in accord with the driving mode. 
Above Level 1, level differences are related to how responsibility for safe movement is divided/shared between ADAS and driver rather than specific driving features. 

SAE Automation Levels have been criticized for their technological focus. It has been argued that the structure of the levels suggests that automation increases linearly and that more automation is better, which may not be the case. SAE Levels also do not account for changes that may be required to infrastructure and road user behavior.

Mobileye System
Mobileye CEO Amnon Shashua and CTO Shai Shalev-Shwartz proposed an alternative taxonomy for autonomous driving systems, claiming that a more consumer-friendly approach was needed. Its categories reflect the amount of driver engagement that is required. Some vehicle makers have informally adopted some of the terminology involved, while not formally committing to it.

Eyes-on/hands-on
The first level, hands-on/eyes-on, implies that the driver is fully engaged in operating the vehicle, but is supervised by the system, which intervenes according to the features it supports (e.g., adaptive cruise control, automatic emergency braking). The driver is entirely responsible, with hands on the wheel, and eyes on the road.

Eyes-on/hands-off
Eyes-on/hands-off allows the driver to let go of the wheel. The system drives, the driver monitors and remains prepared to resume control as needed.

Eyes-off/hands-off
Eyes-off/hands-off means that the driver can stop monitoring the system, leaving the system in full control.  Eyes-off requires that no errors be reproducible (not triggered by exotic transitory conditions) or frequent, that speeds are contextually appropriate (e.g., 80 mph on limited-access roads), and that the system handle typical maneuvers (e.g., getting cut off by another vehicle). The automation level could vary according to the road (e.g., eyes-off on freeways, eyes-on on side streets).

No driver
The highest level does not require a human driver in the car: monitoring is done either remotely (telepresence) or not at all.

Safety
A critical requirement for the higher two levels is that the vehicle be able to conduct a Minimum Risk Maneuver and stop safely out of traffic without driver intervention.

Technology
Architecture
The perception system processes visual and audio data from outside and inside the car to create a local model of the vehicle, the road, traffic, traffic controls and other observable objects, and their relative motion. The control system then takes actions to move the vehicle, considering the local model, road map, and driving regulations.

Several classifications have been proposed to describe ADAS technology. One proposal is to adopt these categories: navigation, path planning, perception, and car control.

Navigation
Navigation involves the use of maps to define a path between origin and destination. Hybrid navigation is the use of multiple navigation systems. Some systems use basic maps, relying on perception to deal with anomalies. Such a map understands which roads lead to which others, whether a road is a freeway, a highway, are one-way, etc. Other systems require highly detailed maps, including lane maps, obstacles, traffic controls, etc.

Perception
ACs need to be able to perceive the world around them. Supporting technologies include combinations of cameras, LiDAR, radar, audio, and ultrasound, GPS, and inertial measurement. Deep neural networks are used to analyse inputs from these sensors to detect and identify objects and their trajectories. Some systems use Bayesian simultaneous localization and mapping (SLAM) algorithms. Another technique is detection and tracking of other moving objects (DATMO), used to handle potential obstacles. Other systems use roadside real-time locating system (RTLS) technologies to aid localization. Tesla's "vision only" system uses eight cameras, without LIDAR or radar, to create its bird's-eye view of the environment.

Path planning
Path planning finds a sequence of segments that a vehicle can use to move from origin to destination. Techniques used for path planning include graph-based search and variational-based optimization techniques. Graph-based techniques can make harder decisions such as how to pass another vehicle/obstacle. Variational-based optimization techniques require more stringent restrictions on the vehicle's path to prevent collisions. The large scale path of the vehicle can be determined by using a voronoi diagram, an occupancy grid mapping, or a driving corridor algorithm. The latter allows the vehicle to locate and drive within open space that is bounded by lanes or barriers.

Maps
Maps are necessary for navigation. Map sophistication varies from simple graphs that show which roads connect to each other, with details such as one-way vs two-way, to those that are highly detailed, with information about lanes, traffic controls, roadworks, and more. Researchers at the MITComputer Science and Artificial Intelligence Laboratory (CSAIL) developed a system called MapLite, which allows self-driving cars to drive with simple maps. The system combines the GPS position of the vehicle, a "sparse topological map" such as OpenStreetMap (which has only 2D road features), with sensors that observe road conditions. One issue with highly-detailed maps is updating them as the world changes. Vehicles that can operate with less-detailed maps do not require frequent updates or geo-fencing.

Sensors
Sensors are necessary for the vehicle to properly respond to the driving environment. Sensor types include cameras, LiDAR, ultrasound, and radar. Control systems typically combine data from multiple sensors.  Multiple sensors can provide a more complete view of the surroundings and can be used to cross-check each other to correct errors. For example, radar can image a scene in, e.g., a nighttime snowstorm, that defeats cameras and LiDAR, albeit at reduced precision. After experimenting with radar and ultrasound, Tesla adopted a vision-only approach, asserting that humans drive using only vision, and that cars should be able to do the same, while citing the lower cost of cameras versus other sensor types. By contrast, Waymo makes use of the higher resolution of LiDAR sensors and cites the declining cost of that technology.

Drive by wire
Drive by wire is the use of electrical or electro-mechanical systems for performing vehicle functions such as steering or speed control that are traditionally achieved by mechanical linkages.

Driver monitoring
Driver monitoring is used to assess the driver's attention and alertness. Techniques in use include eye monitoring, and requiring the driver to maintain torque on the steering wheel. It attempts to understand driver status and identify dangerous driving behaviors.

Vehicle communication
Vehicles can potentially benefit from communicating with others to share information about traffic, road obstacles, to receive map and software updates, etc.
ISO/TC 22 specifies in-vehicle transport information and control systems, while ISO/TC 204 specifies information, communication and control systems in surface transport. International standards have been developed for ADAS functions, connectivity, human interaction, in-vehicle systems, management/engineering, dynamic map and positioning, privacy and security.
Rather than communicating among vehicles, they can communicate with road-based systems to receive similar information.

Software update
Software controls the vehicle, and can provide entertainment and other services. Over-the-air updates can deliver bug fixes and additional features over the internet. Software updates are one way to accomplish recalls that in the past required a visit to a service center. In March 2021, the UNECE regulation on software update and software update management systems was published.

Safety model
A safety model is software that attempts to formalize rules that ensure that ACs operate safely. 
IEEE is attempting to forge a standard for safety models as "IEEE P2846: A Formal Model for Safety Considerations in Automated Vehicle Decision Making". In 2022, a research group at National Institute of Informatics (NII, Japan) enhanced Mobileye's Reliable Safety System as "Goal-Aware RSS" to enable RSS rules to deal with complex scenarios via program logic.

Notification
The US has standardized the use of turquoise lights to inform other drivers that a vehicle is driving autonomously. It will be used in the 2026 Mercedes-Benz EQS and S-Class sedans with Drive Pilot, an SAE Level 3 driving system.
As of 2023, the Turquoise light had not been standardized by the P.R.C or the UN-ECE.

Artificial Intelligence
Artificial intelligence (AI) plays a pivotal role in the development and operation of autonomous vehicles (AVs), enabling them to perceive their surroundings, make decisions, and navigate safely without human intervention. AI algorithms empower AVs to interpret sensory data from various onboard sensors, such as cameras, LiDAR, radar, and GPS, to understand their environment and improve its technological ability and overall safety over time.

Challenges
Obstacles
The primary obstacle to ACs is the advanced software and mapping required to make them work safely across the wide variety of conditions that drivers experience. In addition to handling day/night driving in good and bad weather on roads of arbitrary quality, ACs must cope with other vehicles, road obstacles, poor/missing traffic controls, flawed maps, and handle endless edge cases, such as following the instructions of a police officer managing traffic at a crash site.
Other obstacles include cost, liability, consumer reluctance, ethical dilemmas, security, privacy, and legal/regulatory framework. Further, AVs could automate the work of professional drivers, eliminating many jobs, which could slow acceptance.

Concerns
Deceptive marketing
Tesla calls its Level 2 ADAS "Full Self-Driving (FSD) Beta". US Senators Richard Blumenthal and Edward Markey called on the Federal Trade Commission (FTC) to investigate this marketing in 2021. In December 2021 in Japan, Mercedes-Benz was punished by the Consumer Affairs Agency for misleading product descriptions.
Mercedes-Benz was criticized for a misleading US commercial advertising E-Class models. At that time, Mercedes-Benz rejected the claims and stopped its "self-driving car" ad campaign that had been running. In August 2022, the California Department of Motor Vehicles (DMV) accused Tesla of deceptive marketing practices.
With the Automated Vehicles Bill (AVB) self-driving car-makers could face prison for misleading adverts in the United-Kingdom.

Security
In the 2020s, concerns over ACs' vulnerability to cyberattacks and data theft emerged.

Espionage
In 2018 and 2019 former Apple engineers were charged with stealing information related to Apple's self-driving car project. In 2021 the United States Department of Justice (DOJ) accused Chinese security officials of coordinating a hacking campaign to steal information from government entities, including research related to autonomous vehicles. China has prepared "the Provisions on Management of Automotive Data Security (Trial) to protect its own data".
Cellular Vehicle-to-Everything technologies are based on 5G wireless networks. As of November 2022, the US Congress was considering the possibility that imported Chinese AC technology could facilitate espionage.
Testing of Chinese automated cars in the US has raised concern over which US data are collected by Chinese vehicles to be stored in Chinese country and concern with any link with the Chinese communist party.

Driver communications
ACs complicate the need for drivers to communicate with each other, e.g., to decide which car enters an intersection first. In an AC without a driver, traditional means such as hand signals do not work (no driver, no hands).

Behavior prediction
ACs must be able to predict the behavior of possibly moving vehicles, pedestrians, etc in real time in order to proceed safely. The task becomes more challenging the further into the future the prediction extends, requiring rapid revisions to the estimate to cope with unpredicted behavior. One approach is to wholly recompute the position and trajectory of each object many times per second. Another is to cache the results of an earlier prediction for use in the next one to reduce computational complexity.

Handover
The ADAS has to be able to safely accept control from and return control to the driver.

Trust
Consumers will avoid ACs unless they trust them as safe. Robotaxis operating in San Francisco received pushback over perceived safety risks. Automatic elevators were invented in 1900, but did not become common until operator strikes and trust was built with advertising and features such as an emergency stop button.

Economics
Autonomous also present various political and economic implications. The transportation sector holds significant sway in many the political and economic landscapes. For instance, many US states generates much annual revenue from transportation fees and taxes. The advent of self-driving cars could profoundly affect the economy by potentially altering state tax revenue streams. Furthermore, the transition to autonomous vehicles might disrupt employment patterns and labor markets, particularly in industries heavily reliant on driving professions. Data from the U.S. Bureau of Labor Statistics indicates that in 2019, the sector employed over two million individuals as tractor-trailer truck drivers. Additionally, taxi and delivery drivers represented approximately 370,400 positions, and bus drivers constituted a workforce of over 680,000. Collectively, this amounts to a conceivable displacement of nearly 2.9 million jobs, surpassing the job losses experienced in the 2008 Great Recession.

Equity and Inclusion
The prominence of certain demographic groups within the tech industry inevitably shapes the trajectory of autonomous vehicle (AV) development, potentially perpetuating existing inequalities. There are others in society without a political agenda who believe that the advancement of technology has nothing to do with promoting inequalities in certain groups and see this as a ridiculous presumption.

Ethical issues
Pedestrian Detection
Research from Georgia Tech revealed that autonomous vehicle detection systems were generally five percent less effective at recognizing darker-skinned individuals. This accuracy gap persisted despite adjustments for environmental variables like lighting and visual obstructions.

Rationale for liability
Standards for liability have yet to be adopted to address crashes and other incidents. Liability could rest with the vehicle occupant, its owner, the vehicle manufacturer, or even the ADAS technology supplier, possibly depending on the circumstances of the crash. Additionally, the infusion of ArtificiaI Intelligence technology in autonomous vehicles adds layers of complexity to ownership and ethical dynamics. Given that AI systems are inherently self-learning, a question arises of whether accountability should rest with the vehicle owner, the manufacturer, or the AI developer?

Trolley problem
The trolley problem is a thought experiment in ethics. Adapted for ACs, it considers an AC carrying one passenger confronts a pedestrian who steps in its way. The ADAS notionally has to choose between killing the pedestrian or swerving into a wall, killing the passenger. Possible frameworks include deontology (formal rules) and utilitarianism (harm reduction).
One public opinion survey reported that harm reduction was preferred, except that passengers wanted the vehicle to prefer them, while pedestrians took the opposite view. Utilitarian regulations were unpopular. Additionally, cultural viewpoints exert substantial influence on shaping responses to these ethical quandaries. Another study found that cultural biases impact preferences in prioritizing the rescue of certain individuals over others in car accident scenarios.

Privacy
Some ACs require an internet connection to function, opening the possibility that a hacker might gain access to private information such as destinations, routes, camera recordings, media preferences, and/or behavioral patterns, although this is true of an internet-connected device.

Road infrastructure
ACs make use of road infrastructure (e.g., traffic signs, turn lanes) and may require modifications to that infrastructure to fully achieve their safety and other goals. In March 2023, the Japanese government unveiled a plan to set up a dedicated highway lane for ACs. In April 2023, JR East announced their challenge to raise their self-driving level of Kesennuma Line bus rapid transit (BRT) in rural area from the current Level 2 to Level 4 at 60 km/h.

Testing
Approaches
ACs can be tested via digital simulations, in a controlled test environment, and/or on public roads. Road testing typically requires some form of permit or a commitment to adhere to acceptable operating principles. For example, New York requires a test driver to be in the vehicle, prepared to override the ADAS as necessary.

2010s and disengagements
In California, self-driving car manufacturers are required to submit annual reports describing how often their vehicles autonomously disengaged from autonomous mode. This is one measure of system robustness (ideally, the system should never disengage).
In 2017, Waymo reported 63 disengagements over 352,545 mi (567,366 km) of testing, an average distance of 5,596 mi (9,006 km) between disengagements, the highest (best) among companies reporting such figures. Waymo also logged more autonomous miles than other companies. Their 2017 rate of 0.18 disengagements per 1,000 mi (1,600 km) was an improvement over the 0.2 disengagements per 1,000 mi (1,600 km) in 2016, and 0.8 in 2015. In March 2017, Uber reported an average of 0.67 mi (1.08 km) per disengagement. In the final three months of 2017, Cruise (owned by GM) averaged 5,224 mi (8,407 km) per disengagement over 62,689 mi (100,888 km).

2020s
Disengagement definitions
Reporting companies use varying definitions of what qualifies as a disengagement, and such definitions can change over time. Executives of self-driving car companies have criticized disengagements as a deceptive metric, because it does not consider varying road conditions.

Standards
In April 2021, WP.29 GRVA proposed a "Test Method for Automated Driving (NATM)".
In October 2021, Europe's pilot test, L3Pilot, demonstrated ADAS for cars in Hamburg, Germany, in conjunction with ITS World Congress 2021. SAE Level 3 and 4 functions were tested on ordinary roads.
In November 2022, an International Standard ISO 34502 on "Scenario based safety evaluation framework" was published.

Collision avoidance
In April 2022, collision avoidance testing was demonstrated by Nissan. Waymo published a document about collision avoidance testing in December 2022.

Simulation and validation
In September 2022, Biprogy released Driving Intelligence Validation Platform (DIVP) as part of Japanese national project "SIP-adus", which is interoperable with Open Simulation Interface (OSI) of ASAM.

Toyota
In November 2022, Toyota demonstrated one of its GR Yaris test cars, which had been trained using professional rally drivers. Toyota used its collaboration with Microsoft in FIA World Rally Championship since the 2017 season.

Pedestrian reactions
In 2023 David R. Large, senior research fellow with the Human Factors Research Group at the University of Nottingham, disguised himself as a car seat in a study to test people's reactions to driverless cars. He said, "We wanted to explore how pedestrians would interact with a driverless car and developed this unique methodology to explore their reactions." The study found that, in the absence of someone in the driving seat, pedestrians trust certain visual prompts more than others when deciding whether to cross the road.

Incidents
Tesla
As of 2023, Tesla's ADAS Autopilot/Full Self Driving (beta) was classified as Level 2 ADAS.
On 20 January 2016, the first of five known fatal crashes of a Tesla with Autopilot occurred, in China's Hubei province. Initially, Tesla stated that the vehicle was so badly damaged from the impact that their recorder was not able to determine whether the car had been on Autopilot at the time. However, the car failed to take evasive action. 
Another fatal Autopilot crash occurred in May in Florida in a Tesla Model S that crashed into a tractor-trailer. In a civil suit between the father of the driver killed and Tesla, Tesla documented that the car had been on Autopilot. According to Tesla, "neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied." Tesla claimed that this was Tesla's first known Autopilot death in over 130 million miles (210 million kilometers) with Autopilot engaged. Tesla claimed that on average one fatality occurs every 94 million miles (151 million kilometers) across all vehicle types in the US. However, this number also includes motorcycle/pedestrian fatalities. The ultimate National Transportation Safety Board (NTSB) report concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.

Google Waymo
In June 2015, Google confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, in two of which the vehicle was side-swiped by another driver, one in which another driver rolled a stop sign, and one where a driver was controlling the car manually. In July 2015, three employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake. This was the first collision that resulted in injuries. 
According to Google Waymo's accident reports as of early 2016, their test cars had been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash. On 14 February 2016 a Google vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Google stated, "In this case, we clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision." Google characterized the crash as a misunderstanding and a learning experience. No injuries were reported.

Uber's Advanced Technologies Group (ATG)
In March 2018, Elaine Herzberg died after she was hit by an AC tested by Uber's Advanced Technologies Group (ATG) in Arizona. A safety driver was in the car. Herzberg was crossing the road about 400 feet from an intersection. Some experts said a human driver could have avoided the crash. Arizona governor Doug Ducey suspended the company's ability to test its ACs citing an "unquestionable failure" of Uber to protect public safety. Uber also stopped testing in California until receiving a new permit in 2020.
NTSB's final report determined that the immediate cause of the accident was that safety driver Rafaela Vasquez failed to monitor the road, because she was distracted by her phone, but that Uber's "inadequate safety culture" contributed. The report noted that the victim had "a very high level" of methamphetamine in her body. The board called on federal regulators to carry out a review before allowing automated test vehicles to operate on public roads.
In September 2020, Vasquez pled guilty to endangerment and was sentenced to three years' probation.

NIO Navigate on Pilot
On 12 August 2021, a 31-year-old Chinese man was killed after his NIO ES8 collided with a construction vehicle. NIO's self-driving feature was in beta and could not deal with static obstacles. The vehicle's manual clearly stated that the driver must take over near construction sites. Lawyers of the deceased's family questioned NIO's private access to the vehicle, which they argued did not guarantee the integrity of the data.

Pony.ai
In November 2021, the California Department of Motor Vehicles (DMV) notified Pony.ai that it was suspending its testing permit following a reported collision in Fremont on 28 October. In May 2022, DMV revoked Pony.ai's permit for failing to monitor the driving records of its safety drivers.

Cruise
In April 2022, Cruise's testing vehicle was reported to have blocked a fire engine on emergency call, and sparked questions about its ability to handle unexpected circumstances.

Ford
In February 2024, a driver using the Ford BlueCruise hands-free driving feature struck and killed the driver of a stationary car with no lights on in the middle lane of a freeway in Texas. 
In March 2024, a drunk driver who was speeding, holding her cell phone, and using BlueCruise on a Pennsylvania freeway struck and killed two people who had been driving two cars. The first car had become disabled and was on the left shoulder with part of the car in the left driving lane. The second driver had parked his car behind the first car presumably to help the first driver.
The NTSB is investigating both incidents.

Total Incidents
The NHTSA began mandating incident reports from autonomous vehicle companies in June 2021. Some reports cite incidents from as early as August 2019, with current data available through June 17, 2024.
There have been a total of 3,979 autonomous vehicle incidents (both ADS and ADAS) reported during this timeframe. 2,146 of those incidents (53.9%) involved Tesla vehicles.

Public opinion surveys
2010s
In a 2011 online survey of 2,006 US and UK consumers, 49% said they would be comfortable using a "driverless car".
A 2012 survey of 17,400 vehicle owners found 37% who initially said they would be interested in purchasing a "fully autonomous car". However, that figure dropped to 20% if told the technology would cost US$3,000 more.
In a 2012 survey of about 1,000 German drivers, 22% had a positive attitude, 10% were undecided, 44% were skeptical and 24% were hostile.
A 2013 survey of 1,500 consumers across 10 countries found 57% "stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver", with Brazil, India and China the most willing to trust automated technology.
In a 2014 US telephone survey, over three-quarters of licensed drivers said they would consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an automated car was available.
In 2015, a survey of 5,000 people from 109 countries reported that average respondents found manual driving the most enjoyable. 22% did not want to pay more money for autonomy. Respondents were found to be most concerned about hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries were less comfortable with their vehicle sharing data. The survey reported consumer interest in purchasing an AC, stating that 37% of surveyed current owners were either "definitely" or "probably" interested.
In 2016, a survey of 1,603 people in Germany that controlled for age, gender, and education reported that men felt less anxiety and more enthusiasm, whereas women showed the opposite. The difference was pronounced between young men and women and decreased with age.
In a 2016 US  survey of 1,584 people, "66 percent of respondents said they think autonomous cars are probably smarter than the average human driver". People were worried about safety and hacking risk. Nevertheless, only 13% of the interviewees saw no advantages in this new kind of cars.
In a 2017 survey of 4,135 US adults found that many Americans anticipated significant impacts from various automation technologies including the widespread adoption of automated vehicles.
In 2019, results from two opinion surveys of 54 and 187 US adults respectively were published. The questionnaire was termed the autonomous vehicle acceptance model (AVAM), including additional description to help respondents better understand the implications of various automation levels. Users were less accepting of high autonomy levels and displayed significantly lower intention to use autonomous vehicles. Additionally, partial autonomy (regardless of level) was perceived as requiring uniformly higher driver engagement (usage of hands, feet and eyes) than full autonomy.

In the 2020s
In 2022, a survey reported that only a quarter (27%) of the world's population would feel safe in self-driving cars.
In 2024, a study by Saravanos et al. at New York University reported that 87% of their respondents (from a sample of 358) believed that conditionally automated cars (at Level 3) would be easy to use.
Opinion surveys may have little salience given that few respondents had any personal experience with ACs.

Regulation
The regulation of autonomous cars concerns liability, approvals, and international conventions.
In the 2010s, researchers openly worried that delayed regulations could delay deployment. In 2020, UNECE WP.29 GRVA was issued to address regulation of Level 3 automated driving.

Commercialization
Vehicles operating below Level 5 still offer many advantages.
As of 2023 most commercially available ADAS vehicles are SAE Level 2. A couple of companies reached higher levels, but only in restricted (geofenced) locations.

Level 2 – Partial Automation
SAE Level 2 features are available as part of the ADAS systems in many vehicles. In the US, 50% of new cars provide driver assistance for both steering and speed.
Ford started offering BlueCruise service on certain vehicles in 2022; the system is named ActiveGlide in Lincoln vehicles. The system provided features such as lane centering, street sign recognition, and hands-free highway driving on more than 130,000 miles of divided highways. The 2022 1.2 version added features including hands-free lane changing, in-lane repositioning, and predictive speed assist. In April 2023 BlueCruise was approved in the UK for use on certain motorways, starting with 2023 models of Ford's electric Mustang Mach-E SUV.
Tesla's Autopilot and its Full Self-Driving (FSD) ADAS suites are available on all Tesla cars since 2016. FSD offers highway and street driving (without geofencing), navigation/turn management, steering, and dynamic cruise control, collision avoidance, lane-keeping/switching, emergency braking, obstacle avoidance, but still requires the driver to remain ready to control the vehicle at any moment. Its driver management system combines eye tracking with monitoring pressure on the steering wheel to ensure that drives are both eyes on and hands on.
Tesla's FSD rewrite V12 (released in March 2024) uses a single deep learning transformer model for all aspects of perception, monitoring, and control. It relies on its eight cameras for its vision-only perception system, without use of LiDAR, radar, or ultrasound. As of April 2024, FSD has been deployed on two million Tesla cars. As of January 2024, Tesla has not initiated requests for Level 3 status for its systems and has not disclosed its reason for not doing so.

Development
General Motors is developing the "Ultra Cruise" ADAS system, that will be a dramatic improvement over their current "Super Cruise" system. Ultra Cruise will cover "95 percent" of driving scenarios on 2 million miles of roads in the US, according to the company. The system hardware in and around the car includes multiple cameras, short- and long-range radar, and a LiDAR sensor, and will be powered by the Qualcomm Snapdragon Ride Platform. The luxury Cadillac Celestiq electric vehicle will be one of the first vehicles to feature Ultra Cruise.
Europe is developing a new "Driver Control Assistance Systems" (DCAS) level 2 regulation to no longer limit the use of lane changing systems to roads with 2 lanes and a physical separation from traffic in the opposite direction.

Level 3 – Conditional Automation
As of April 2024, two car manufacturers have sold or leased Level 3 cars: Honda in Japan, and Mercedes in Germany, Nevada and California.
Mercedes Drive Pilot has been available on the EQS and S-class sedan in Germany since 2022, and in California and Nevada since 2023. A subscription costs between €5,000 and €7,000 for three years in Germany and $2,500 for one year in the United States. Drive Pilot can only be used when the vehicle is traveling under 40 miles per hour (64 km/h), there is a vehicle in front, readable line markings, during the day, clear weather, and on freeways mapped by Mercedes down to the centimeter (100,000 miles in California). As of April 2024, one Mercedes vehicle with this capability has been sold in California.

Development
Honda continued to enhance its Level 3 technology. As of 2023, 80 vehicles with Level 3 support had been sold.
Mercedes-Benz received authorization in early 2023 to pilot its Level 3 software in Las Vegas. California also authorized Drive Pilot in 2023. 
BMW commercialized its AC in 2021.  In 2023 BMW stated that its Level-3 technology was nearing release. It would be the second manufacturer to deliver Level-3 technology, but the only one with a Level 3 technology which works in the dark.
In 2023, in China, IM Motors, Mercedes, and BMW obtained authorization to test vehicles with Level 3 systems on motorways.
In September 2021, Stellantis presented its findings from its Level 3 pilot testing on Italian highways. Stellantis's Highway Chauffeur claimed Level 3 capabilities, as tested on the Maserati Ghibli and Fiat 500X prototypes.
Polestar, a Volvo Cars' brand, announced in January 2022 its plan to offer Level 3 autonomous driving system in the Polestar 3 SUV, a Volvo XC90 successor, with technologies from Luminar Technologies, Nvidia, and Zenseact. 
In January 2022, Bosch and the Volkswagen Group subsidiary CARIAD released a collaboration for autonomous driving up to Level 3. This joint development targets Level 4 capabilities.
Hyundai Motor Company is enhancing cybersecurity of connected cars to offer a Level 3 self-driving Genesis G90. Kia and Hyundai Korean car makers delayed their Level 3 plans, and will not deliver Level 3 vehicles in 2023.

Level 4 – High Automation
Waymo offers robotaxi services in parts of Arizona (Phoenix) and California (San Francisco and Los Angeles), as fully autonomous vehicles without safety drivers.
In April 2023 in Japan, a Level 4 protocol became part of the amended Road Traffic Act. ZEN drive Pilot Level 4 made by AIST operates there.

Development
In July 2020, Toyota started public demonstration rides on Lexus LS (fifth generation) based TRI-P4 with Level 4 capability. In August 2021, Toyota operated a potentially Level 4 service using e-Palette around the Tokyo 2020 Olympic Village.
In September 2020, Mercedes-Benz introduced world's first commercial Level 4 Automated Valet Parking (AVP) system named Intelligent Park Pilot for its new S-Class. In November 2022, Germany’s Federal Motor Transport Authority (KBA) approved the system for use at Stuttgart Airport.
In September 2021, Cruise, General Motors, and Honda started a joint testing programme, using Cruise AV. In 2023, the Origin was put on indefinite hold following Cruise's loss of its operating permit.
In January 2023, Holon announced an autonomous shuttle during the 2023 Consumer Electronics Show (CES). The company claimed the vehicle is the world's first Level 4 shuttle built to automotive standard.

See also
Autopilot
Driving

References
Further reading
 Media related to Self-driving cars at Wikimedia Commons
In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it can be used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.
Contrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root.
Large language models are called autoregressive, but they are not a classical autoregressive model in this sense because they are not linear.

Definition
The notation 
  
    
      
        A
        R
        (
        p
        )
      
    
    {\displaystyle AR(p)}
  
 indicates an autoregressive model of order p. The AR(p) model is defined as

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            p
          
        
        
          φ
          
            i
          
        
        
          X
          
            t
            −
            i
          
        
        +
        
          ε
          
            t
          
        
      
    
    {\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}}
  

where 
  
    
      
        
          φ
          
            1
          
        
        ,
        …
        ,
        
          φ
          
            p
          
        
      
    
    {\displaystyle \varphi _{1},\ldots ,\varphi _{p}}
  
 are the parameters of the model, and 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 is white noise. This can be equivalently written using the backshift operator B as

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            p
          
        
        
          φ
          
            i
          
        
        
          B
          
            i
          
        
        
          X
          
            t
          
        
        +
        
          ε
          
            t
          
        
      
    
    {\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}B^{i}X_{t}+\varepsilon _{t}}
  

so that, moving the summation term to the left side and using polynomial notation, we have

  
    
      
        ϕ
        [
        B
        ]
        
          X
          
            t
          
        
        =
        
          ε
          
            t
          
        
      
    
    {\displaystyle \phi [B]X_{t}=\varepsilon _{t}}
  

An autoregressive model can thus be viewed as the output of an all-pole infinite impulse response filter whose input is white noise.
Some parameter constraints are necessary for the model to remain weak-sense stationary.  For example, processes in the AR(1) model with 
  
    
      
        
          |
        
        
          φ
          
            1
          
        
        
          |
        
        ≥
        1
      
    
    {\displaystyle |\varphi _{1}|\geq 1}
  
 are not stationary. More generally, for an AR(p) model to be weak-sense stationary, the roots of the polynomial 
  
    
      
        Φ
        (
        z
        )
        :=
        
          1
          −
          
            ∑
            
              i
              =
              1
            
            
              p
            
          
          
            φ
            
              i
            
          
          
            z
            
              i
            
          
        
      
    
    {\displaystyle \Phi (z):=\textstyle 1-\sum _{i=1}^{p}\varphi _{i}z^{i}}
  
 must lie outside the unit circle, i.e., each (complex) root 
  
    
      
        
          z
          
            i
          
        
      
    
    {\displaystyle z_{i}}
  
 must satisfy 
  
    
      
        
          |
        
        
          z
          
            i
          
        
        
          |
        
        >
        1
      
    
    {\displaystyle |z_{i}|>1}
  
 (see pages 89,92 ).

Intertemporal effect of shocks
In an AR process, a one-time shock affects values of the evolving variable infinitely far into the future. For example, consider the AR(1) model 
  
    
      
        
          X
          
            t
          
        
        =
        
          φ
          
            1
          
        
        
          X
          
            t
            −
            1
          
        
        +
        
          ε
          
            t
          
        
      
    
    {\displaystyle X_{t}=\varphi _{1}X_{t-1}+\varepsilon _{t}}
  
. A non-zero value for 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 at say time t=1 affects 
  
    
      
        
          X
          
            1
          
        
      
    
    {\displaystyle X_{1}}
  
 by the amount  
  
    
      
        
          ε
          
            1
          
        
      
    
    {\displaystyle \varepsilon _{1}}
  
. Then by the AR equation for 
  
    
      
        
          X
          
            2
          
        
      
    
    {\displaystyle X_{2}}
  
 in terms of 
  
    
      
        
          X
          
            1
          
        
      
    
    {\displaystyle X_{1}}
  
, this affects 
  
    
      
        
          X
          
            2
          
        
      
    
    {\displaystyle X_{2}}
  
 by the amount 
  
    
      
        
          φ
          
            1
          
        
        
          ε
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}\varepsilon _{1}}
  
. Then by the AR equation for 
  
    
      
        
          X
          
            3
          
        
      
    
    {\displaystyle X_{3}}
  
 in terms of 
  
    
      
        
          X
          
            2
          
        
      
    
    {\displaystyle X_{2}}
  
, this affects 
  
    
      
        
          X
          
            3
          
        
      
    
    {\displaystyle X_{3}}
  
 by the amount 
  
    
      
        
          φ
          
            1
          
          
            2
          
        
        
          ε
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}^{2}\varepsilon _{1}}
  
. Continuing this process shows that the effect of 
  
    
      
        
          ε
          
            1
          
        
      
    
    {\displaystyle \varepsilon _{1}}
  
 never ends, although if the process is stationary then the effect diminishes toward zero in the limit.
Because each shock affects X values infinitely far into the future from when they occur, any given value Xt is affected by shocks occurring infinitely far into the past. This can also be seen by rewriting the autoregression

  
    
      
        ϕ
        (
        B
        )
        
          X
          
            t
          
        
        =
        
          ε
          
            t
          
        
        
      
    
    {\displaystyle \phi (B)X_{t}=\varepsilon _{t}\,}
  

(where the constant term has been suppressed by assuming that the variable has been measured as deviations from its mean) as

  
    
      
        
          X
          
            t
          
        
        =
        
          
            1
            
              ϕ
              (
              B
              )
            
          
        
        
          ε
          
            t
          
        
        
        .
      
    
    {\displaystyle X_{t}={\frac {1}{\phi (B)}}\varepsilon _{t}\,.}
  

When the polynomial division on the right side is carried out, the polynomial in the backshift operator applied to 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 has an infinite order—that is, an infinite number of lagged values of 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 appear on the right side of the equation.

Characteristic polynomial
The autocorrelation function of an AR(p) process can be expressed as 

  
    
      
        ρ
        (
        τ
        )
        =
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          a
          
            k
          
        
        
          y
          
            k
          
          
            −
            
              |
            
            τ
            
              |
            
          
        
        ,
      
    
    {\displaystyle \rho (\tau )=\sum _{k=1}^{p}a_{k}y_{k}^{-|\tau |},}
  

where 
  
    
      
        
          y
          
            k
          
        
      
    
    {\displaystyle y_{k}}
  
 are the roots of the polynomial

  
    
      
        ϕ
        (
        B
        )
        =
        1
        −
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          φ
          
            k
          
        
        
          B
          
            k
          
        
      
    
    {\displaystyle \phi (B)=1-\sum _{k=1}^{p}\varphi _{k}B^{k}}
  

where B is the backshift operator, where 
  
    
      
        ϕ
        (
        ⋅
        )
      
    
    {\displaystyle \phi (\cdot )}
  
 is the function defining the autoregression, and where 
  
    
      
        
          φ
          
            k
          
        
      
    
    {\displaystyle \varphi _{k}}
  
 are the coefficients in the autoregression. The formula is valid only if all the roots have multiplicity 1.
The autocorrelation function of an AR(p) process is a sum of decaying exponentials.

Each real root contributes a component to the autocorrelation function that decays exponentially.
Similarly, each pair of complex conjugate roots contributes an exponentially damped oscillation.

Graphs of AR(p) processes
The simplest AR process is AR(0), which has no dependence between the terms.  Only the error/innovation/noise term contributes to the output of the process, so in the figure, AR(0) corresponds to white noise.
For an AR(1) process with a positive 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
, only the previous term in the process and the noise term contribute to the output.  If 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 is close to 0, then the process still looks like white noise, but as 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 approaches 1, the output gets a larger contribution from the previous term relative to the noise. This results in a "smoothing" or integration of the output, similar to a low pass filter.
For an AR(2) process, the previous two terms and the noise term contribute to the output. If both 
  
    
      
        
          φ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
 and 
  
    
      
        
          φ
          
            2
          
        
      
    
    {\displaystyle \varphi _{2}}
  
 are positive, the output will resemble a low pass filter, with the high frequency part of the noise decreased. If 
  
    
      
        
          φ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
 is positive while 
  
    
      
        
          φ
          
            2
          
        
      
    
    {\displaystyle \varphi _{2}}
  
 is negative, then the process favors changes in sign between terms of the process.  The output oscillates. This can be likened to edge detection or detection of change in direction.

Example: An AR(1) process
An AR(1) process is given by:
  
    
      
        
          X
          
            t
          
        
        =
        φ
        
          X
          
            t
            −
            1
          
        
        +
        
          ε
          
            t
          
        
        
      
    
    {\displaystyle X_{t}=\varphi X_{t-1}+\varepsilon _{t}\,}
  
where 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 is a white noise process with zero mean and constant variance 
  
    
      
        
          σ
          
            ε
          
          
            2
          
        
      
    
    {\displaystyle \sigma _{\varepsilon }^{2}}
  
.
(Note: The subscript on 
  
    
      
        
          φ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
 has been dropped.) The process is weak-sense stationary if 
  
    
      
        
          |
        
        φ
        
          |
        
        <
        1
      
    
    {\displaystyle |\varphi |<1}
  
 since it is obtained as the output of a stable filter whose input is white noise.  (If 
  
    
      
        φ
        =
        1
      
    
    {\displaystyle \varphi =1}
  
 then the variance of 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 depends on time lag t, so that the variance of the series diverges to infinity as t goes to infinity, and is therefore not weak sense stationary.) Assuming 
  
    
      
        
          |
        
        φ
        
          |
        
        <
        1
      
    
    {\displaystyle |\varphi |<1}
  
, the mean 
  
    
      
        E
        ⁡
        (
        
          X
          
            t
          
        
        )
      
    
    {\displaystyle \operatorname {E} (X_{t})}
  
 is identical for all values of t by the very definition of weak sense stationarity. If the mean is denoted by 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
, it follows from
  
    
      
        E
        ⁡
        (
        
          X
          
            t
          
        
        )
        =
        φ
        E
        ⁡
        (
        
          X
          
            t
            −
            1
          
        
        )
        +
        E
        ⁡
        (
        
          ε
          
            t
          
        
        )
        ,
      
    
    {\displaystyle \operatorname {E} (X_{t})=\varphi \operatorname {E} (X_{t-1})+\operatorname {E} (\varepsilon _{t}),}
  
that
  
    
      
        μ
        =
        φ
        μ
        +
        0
        ,
      
    
    {\displaystyle \mu =\varphi \mu +0,}
  
and hence

  
    
      
        μ
        =
        0.
      
    
    {\displaystyle \mu =0.}
  

The variance is

  
    
      
        
          
            var
          
        
        (
        
          X
          
            t
          
        
        )
        =
        E
        ⁡
        (
        
          X
          
            t
          
          
            2
          
        
        )
        −
        
          μ
          
            2
          
        
        =
        
          
            
              σ
              
                ε
              
              
                2
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\textrm {var}}(X_{t})=\operatorname {E} (X_{t}^{2})-\mu ^{2}={\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}},}
  

where 
  
    
      
        
          σ
          
            ε
          
        
      
    
    {\displaystyle \sigma _{\varepsilon }}
  
 is the standard deviation of 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
. This can be shown by noting that

  
    
      
        
          
            var
          
        
        (
        
          X
          
            t
          
        
        )
        =
        
          φ
          
            2
          
        
        
          
            var
          
        
        (
        
          X
          
            t
            −
            1
          
        
        )
        +
        
          σ
          
            ε
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\textrm {var}}(X_{t})=\varphi ^{2}{\textrm {var}}(X_{t-1})+\sigma _{\varepsilon }^{2},}
  

and then by noticing that the quantity above is a stable fixed point of this relation.
The autocovariance is given by

  
    
      
        
          B
          
            n
          
        
        =
        E
        ⁡
        (
        
          X
          
            t
            +
            n
          
        
        
          X
          
            t
          
        
        )
        −
        
          μ
          
            2
          
        
        =
        
          
            
              σ
              
                ε
              
              
                2
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
        
        
        
          φ
          
            
              |
            
            n
            
              |
            
          
        
        .
      
    
    {\displaystyle B_{n}=\operatorname {E} (X_{t+n}X_{t})-\mu ^{2}={\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,\,\varphi ^{|n|}.}
  

It can be seen that the autocovariance function decays with a decay time (also called time constant) of 
  
    
      
        τ
        =
        1
        −
        φ
      
    
    {\displaystyle \tau =1-\varphi }
  
.
The spectral density function is the Fourier transform of the autocovariance function. In discrete terms this will be the discrete-time Fourier transform:

  
    
      
        Φ
        (
        ω
        )
        =
        
          
            1
            
              2
              π
            
          
        
        
        
          ∑
          
            n
            =
            −
            ∞
          
          
            ∞
          
        
        
          B
          
            n
          
        
        
          e
          
            −
            i
            ω
            n
          
        
        =
        
          
            1
            
              2
              π
            
          
        
        
        
          (
          
            
              
                σ
                
                  ε
                
                
                  2
                
              
              
                1
                +
                
                  φ
                  
                    2
                  
                
                −
                2
                φ
                cos
                ⁡
                (
                ω
                )
              
            
          
          )
        
        .
      
    
    {\displaystyle \Phi (\omega )={\frac {1}{\sqrt {2\pi }}}\,\sum _{n=-\infty }^{\infty }B_{n}e^{-i\omega n}={\frac {1}{\sqrt {2\pi }}}\,\left({\frac {\sigma _{\varepsilon }^{2}}{1+\varphi ^{2}-2\varphi \cos(\omega )}}\right).}
  

This expression is periodic due to the discrete nature of the 
  
    
      
        
          X
          
            j
          
        
      
    
    {\displaystyle X_{j}}
  
, which is manifested as the cosine term in the denominator.  If we assume that the sampling time (
  
    
      
        Δ
        t
        =
        1
      
    
    {\displaystyle \Delta t=1}
  
) is much smaller than the decay time (
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
), then we can use a continuum approximation to 
  
    
      
        
          B
          
            n
          
        
      
    
    {\displaystyle B_{n}}
  
:

  
    
      
        B
        (
        t
        )
        ≈
        
          
            
              σ
              
                ε
              
              
                2
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
        
        
        
          φ
          
            
              |
            
            t
            
              |
            
          
        
      
    
    {\displaystyle B(t)\approx {\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,\,\varphi ^{|t|}}
  

which yields a Lorentzian profile for the spectral density:

  
    
      
        Φ
        (
        ω
        )
        =
        
          
            1
            
              2
              π
            
          
        
        
        
          
            
              σ
              
                ε
              
              
                2
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
        
        
          
            γ
            
              π
              (
              
                γ
                
                  2
                
              
              +
              
                ω
                
                  2
                
              
              )
            
          
        
      
    
    {\displaystyle \Phi (\omega )={\frac {1}{\sqrt {2\pi }}}\,{\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,{\frac {\gamma }{\pi (\gamma ^{2}+\omega ^{2})}}}
  

where 
  
    
      
        γ
        =
        1
        
          /
        
        τ
      
    
    {\displaystyle \gamma =1/\tau }
  
 is the angular frequency associated with the decay time 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
.
An alternative expression for 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 can be derived by first substituting 
  
    
      
        φ
        
          X
          
            t
            −
            2
          
        
        +
        
          ε
          
            t
            −
            1
          
        
      
    
    {\displaystyle \varphi X_{t-2}+\varepsilon _{t-1}}
  
 for 
  
    
      
        
          X
          
            t
            −
            1
          
        
      
    
    {\displaystyle X_{t-1}}
  
 in the defining equation. Continuing this process N times yields

  
    
      
        
          X
          
            t
          
        
        =
        
          φ
          
            N
          
        
        
          X
          
            t
            −
            N
          
        
        +
        
          ∑
          
            k
            =
            0
          
          
            N
            −
            1
          
        
        
          φ
          
            k
          
        
        
          ε
          
            t
            −
            k
          
        
        .
      
    
    {\displaystyle X_{t}=\varphi ^{N}X_{t-N}+\sum _{k=0}^{N-1}\varphi ^{k}\varepsilon _{t-k}.}
  

For N approaching infinity, 
  
    
      
        
          φ
          
            N
          
        
      
    
    {\displaystyle \varphi ^{N}}
  
 will approach zero and:

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            k
            =
            0
          
          
            ∞
          
        
        
          φ
          
            k
          
        
        
          ε
          
            t
            −
            k
          
        
        .
      
    
    {\displaystyle X_{t}=\sum _{k=0}^{\infty }\varphi ^{k}\varepsilon _{t-k}.}
  

It is seen that 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 is white noise convolved with the 
  
    
      
        
          φ
          
            k
          
        
      
    
    {\displaystyle \varphi ^{k}}
  
 kernel plus the constant mean. If the white noise 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 is a Gaussian process then 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 is also a Gaussian process. In other cases, the central limit theorem indicates that 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 will be approximately normally distributed when 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 is close to one.
For 
  
    
      
        
          ε
          
            t
          
        
        =
        0
      
    
    {\displaystyle \varepsilon _{t}=0}
  
, the process 
  
    
      
        
          X
          
            t
          
        
        =
        φ
        
          X
          
            t
            −
            1
          
        
      
    
    {\displaystyle X_{t}=\varphi X_{t-1}}
  
 will be a geometric progression (exponential growth or decay). In this case, the solution can be found analytically: 
  
    
      
        
          X
          
            t
          
        
        =
        a
        
          φ
          
            t
          
        
      
    
    {\displaystyle X_{t}=a\varphi ^{t}}
  
 whereby 
  
    
      
        a
      
    
    {\displaystyle a}
  
 is an unknown constant (initial condition).

Explicit mean/difference form of AR(1) process
The AR(1) model is the discrete-time analogy of the continuous Ornstein-Uhlenbeck process.  It is therefore sometimes useful to understand the properties of the AR(1) model cast in an equivalent form. In this form, the AR(1) model, with process parameter 
  
    
      
        θ
        ∈
        
          R
        
      
    
    {\displaystyle \theta \in \mathbb {R} }
  
, is given by

  
    
      
        
          X
          
            t
            +
            1
          
        
        =
        
          X
          
            t
          
        
        +
        (
        1
        −
        θ
        )
        (
        μ
        −
        
          X
          
            t
          
        
        )
        +
        
          ε
          
            t
            +
            1
          
        
      
    
    {\displaystyle X_{t+1}=X_{t}+(1-\theta )(\mu -X_{t})+\varepsilon _{t+1}}
  
, where 
  
    
      
        
          |
        
        θ
        
          |
        
        <
        1
        
      
    
    {\displaystyle |\theta |<1\,}
  
, 
  
    
      
        μ
        :=
        E
        (
        X
        )
      
    
    {\displaystyle \mu :=E(X)}
  
 is the model mean, and 
  
    
      
        {
        
          ϵ
          
            t
          
        
        }
      
    
    {\displaystyle \{\epsilon _{t}\}}
  
 is a white-noise process with zero mean and constant variance 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
.
By rewriting this as 
  
    
      
        
          X
          
            t
            +
            1
          
        
        =
        θ
        
          X
          
            t
          
        
        +
        (
        1
        −
        θ
        )
        μ
        +
        
          ε
          
            t
            +
            1
          
        
      
    
    {\displaystyle X_{t+1}=\theta X_{t}+(1-\theta )\mu +\varepsilon _{t+1}}
  
 and then deriving (by induction) 
  
    
      
        
          X
          
            t
            +
            n
          
        
        =
        
          θ
          
            n
          
        
        
          X
          
            t
          
        
        +
        (
        1
        −
        
          θ
          
            n
          
        
        )
        μ
        +
        
          Σ
          
            i
            =
            1
          
          
            n
          
        
        
          (
          
            
              θ
              
                n
                −
                i
              
            
            
              ϵ
              
                t
                +
                i
              
            
          
          )
        
      
    
    {\displaystyle X_{t+n}=\theta ^{n}X_{t}+(1-\theta ^{n})\mu +\Sigma _{i=1}^{n}\left(\theta ^{n-i}\epsilon _{t+i}\right)}
  
, one can show that

  
    
      
        E
        ⁡
        (
        
          X
          
            t
            +
            n
          
        
        
          |
        
        
          X
          
            t
          
        
        )
        =
        μ
        
          [
          
            1
            −
            
              θ
              
                n
              
            
          
          ]
        
        +
        
          X
          
            t
          
        
        
          θ
          
            n
          
        
      
    
    {\displaystyle \operatorname {E} (X_{t+n}|X_{t})=\mu \left[1-\theta ^{n}\right]+X_{t}\theta ^{n}}
  
 and

  
    
      
        Var
        ⁡
        (
        
          X
          
            t
            +
            n
          
        
        
          |
        
        
          X
          
            t
          
        
        )
        =
        
          σ
          
            2
          
        
        
          
            
              1
              −
              
                θ
                
                  2
                  n
                
              
            
            
              1
              −
              
                θ
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \operatorname {Var} (X_{t+n}|X_{t})=\sigma ^{2}{\frac {1-\theta ^{2n}}{1-\theta ^{2}}}}
  
.

Choosing the maximum lag
The partial autocorrelation of an AR(p) process equals zero at lags larger than p, so the appropriate maximum lag p is the one after which the partial autocorrelations are all zero.

Calculation of the AR parameters
There are many ways to estimate the coefficients, such as the ordinary least squares procedure or method of moments (through Yule–Walker equations).
The AR(p) model is given by the equation

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            p
          
        
        
          φ
          
            i
          
        
        
          X
          
            t
            −
            i
          
        
        +
        
          ε
          
            t
          
        
        .
        
      
    
    {\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}.\,}
  

It is based on parameters 
  
    
      
        
          φ
          
            i
          
        
      
    
    {\displaystyle \varphi _{i}}
  
 where i = 1, ..., p. There is a direct correspondence between these parameters and the covariance function of the process, and this correspondence can be inverted to determine the parameters from the autocorrelation function (which is itself obtained from the covariances). This is done using the Yule–Walker equations.

Yule–Walker equations
The Yule–Walker equations, named for Udny Yule and Gilbert Walker, are the following set of equations.

  
    
      
        
          γ
          
            m
          
        
        =
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          φ
          
            k
          
        
        
          γ
          
            m
            −
            k
          
        
        +
        
          σ
          
            ε
          
          
            2
          
        
        
          δ
          
            m
            ,
            0
          
        
        ,
      
    
    {\displaystyle \gamma _{m}=\sum _{k=1}^{p}\varphi _{k}\gamma _{m-k}+\sigma _{\varepsilon }^{2}\delta _{m,0},}
  

where m = 0, …, p, yielding p + 1 equations. Here 
  
    
      
        
          γ
          
            m
          
        
      
    
    {\displaystyle \gamma _{m}}
  
 is the autocovariance function of Xt, 
  
    
      
        
          σ
          
            ε
          
        
      
    
    {\displaystyle \sigma _{\varepsilon }}
  
 is the standard deviation of the input noise process, and 
  
    
      
        
          δ
          
            m
            ,
            0
          
        
      
    
    {\displaystyle \delta _{m,0}}
  
 is the Kronecker delta function.
Because the last part of an individual equation is non-zero only if m = 0, the set of equations can be solved by representing the equations for m > 0 in matrix form, thus getting the equation

  
    
      
        
          
            [
            
              
                
                  
                    γ
                    
                      1
                    
                  
                
              
              
                
                  
                    γ
                    
                      2
                    
                  
                
              
              
                
                  
                    γ
                    
                      3
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    γ
                    
                      p
                    
                  
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  
                    γ
                    
                      0
                    
                  
                
                
                  
                    γ
                    
                      −
                      1
                    
                  
                
                
                  
                    γ
                    
                      −
                      2
                    
                  
                
                
                  ⋯
                
              
              
                
                  
                    γ
                    
                      1
                    
                  
                
                
                  
                    γ
                    
                      0
                    
                  
                
                
                  
                    γ
                    
                      −
                      1
                    
                  
                
                
                  ⋯
                
              
              
                
                  
                    γ
                    
                      2
                    
                  
                
                
                  
                    γ
                    
                      1
                    
                  
                
                
                  
                    γ
                    
                      0
                    
                  
                
                
                  ⋯
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋱
                
              
              
                
                  
                    γ
                    
                      p
                      −
                      1
                    
                  
                
                
                  
                    γ
                    
                      p
                      −
                      2
                    
                  
                
                
                  
                    γ
                    
                      p
                      −
                      3
                    
                  
                
                
                  ⋯
                
              
            
            ]
          
        
        
          
            [
            
              
                
                  
                    φ
                    
                      1
                    
                  
                
              
              
                
                  
                    φ
                    
                      2
                    
                  
                
              
              
                
                  
                    φ
                    
                      3
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    φ
                    
                      p
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}\gamma _{1}\\\gamma _{2}\\\gamma _{3}\\\vdots \\\gamma _{p}\\\end{bmatrix}}={\begin{bmatrix}\gamma _{0}&\gamma _{-1}&\gamma _{-2}&\cdots \\\gamma _{1}&\gamma _{0}&\gamma _{-1}&\cdots \\\gamma _{2}&\gamma _{1}&\gamma _{0}&\cdots \\\vdots &\vdots &\vdots &\ddots \\\gamma _{p-1}&\gamma _{p-2}&\gamma _{p-3}&\cdots \\\end{bmatrix}}{\begin{bmatrix}\varphi _{1}\\\varphi _{2}\\\varphi _{3}\\\vdots \\\varphi _{p}\\\end{bmatrix}}}
  

which can be solved for all 
  
    
      
        {
        
          φ
          
            m
          
        
        ;
        m
        =
        1
        ,
        2
        ,
        …
        ,
        p
        }
        .
      
    
    {\displaystyle \{\varphi _{m};m=1,2,\dots ,p\}.}
  
 The remaining equation for m = 0 is

  
    
      
        
          γ
          
            0
          
        
        =
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          φ
          
            k
          
        
        
          γ
          
            −
            k
          
        
        +
        
          σ
          
            ε
          
          
            2
          
        
        ,
      
    
    {\displaystyle \gamma _{0}=\sum _{k=1}^{p}\varphi _{k}\gamma _{-k}+\sigma _{\varepsilon }^{2},}
  

which, once  
  
    
      
        {
        
          φ
          
            m
          
        
        ;
        m
        =
        1
        ,
        2
        ,
        …
        ,
        p
        }
      
    
    {\displaystyle \{\varphi _{m};m=1,2,\dots ,p\}}
  
 are known, can be solved for 
  
    
      
        
          σ
          
            ε
          
          
            2
          
        
        .
      
    
    {\displaystyle \sigma _{\varepsilon }^{2}.}
  

An alternative formulation is in terms of the autocorrelation function. The AR parameters are determined by the first p+1 elements 
  
    
      
        ρ
        (
        τ
        )
      
    
    {\displaystyle \rho (\tau )}
  
 of the autocorrelation function. The full autocorrelation function can then be derived by recursively calculating

  
    
      
        ρ
        (
        τ
        )
        =
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          φ
          
            k
          
        
        ρ
        (
        k
        −
        τ
        )
      
    
    {\displaystyle \rho (\tau )=\sum _{k=1}^{p}\varphi _{k}\rho (k-\tau )}
  

Examples for some Low-order AR(p) processes

p=1

  
    
      
        
          γ
          
            1
          
        
        =
        
          φ
          
            1
          
        
        
          γ
          
            0
          
        
      
    
    {\displaystyle \gamma _{1}=\varphi _{1}\gamma _{0}}
  

Hence 
  
    
      
        
          ρ
          
            1
          
        
        =
        
          γ
          
            1
          
        
        
          /
        
        
          γ
          
            0
          
        
        =
        
          φ
          
            1
          
        
      
    
    {\displaystyle \rho _{1}=\gamma _{1}/\gamma _{0}=\varphi _{1}}
  

p=2
The Yule–Walker equations for an AR(2) process are

  
    
      
        
          γ
          
            1
          
        
        =
        
          φ
          
            1
          
        
        
          γ
          
            0
          
        
        +
        
          φ
          
            2
          
        
        
          γ
          
            −
            1
          
        
      
    
    {\displaystyle \gamma _{1}=\varphi _{1}\gamma _{0}+\varphi _{2}\gamma _{-1}}
  

  
    
      
        
          γ
          
            2
          
        
        =
        
          φ
          
            1
          
        
        
          γ
          
            1
          
        
        +
        
          φ
          
            2
          
        
        
          γ
          
            0
          
        
      
    
    {\displaystyle \gamma _{2}=\varphi _{1}\gamma _{1}+\varphi _{2}\gamma _{0}}
  

Remember that 
  
    
      
        
          γ
          
            −
            k
          
        
        =
        
          γ
          
            k
          
        
      
    
    {\displaystyle \gamma _{-k}=\gamma _{k}}
  

Using the first equation yields 
  
    
      
        
          ρ
          
            1
          
        
        =
        
          γ
          
            1
          
        
        
          /
        
        
          γ
          
            0
          
        
        =
        
          
            
              φ
              
                1
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \rho _{1}=\gamma _{1}/\gamma _{0}={\frac {\varphi _{1}}{1-\varphi _{2}}}}
  

Using the recursion formula yields 
  
    
      
        
          ρ
          
            2
          
        
        =
        
          γ
          
            2
          
        
        
          /
        
        
          γ
          
            0
          
        
        =
        
          
            
              
                φ
                
                  1
                
                
                  2
                
              
              −
              
                φ
                
                  2
                
                
                  2
                
              
              +
              
                φ
                
                  2
                
              
            
            
              1
              −
              
                φ
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \rho _{2}=\gamma _{2}/\gamma _{0}={\frac {\varphi _{1}^{2}-\varphi _{2}^{2}+\varphi _{2}}{1-\varphi _{2}}}}

Estimation of AR parameters
The above equations (the Yule–Walker equations) provide several routes to estimating the parameters of an AR(p) model, by replacing the theoretical covariances with estimated values. Some of these variants can be described as follows:

Estimation of autocovariances or autocorrelations. Here each of these terms is estimated separately, using conventional estimates. There are different ways of doing this and the choice between these affects the properties of the estimation scheme. For example, negative estimates of the variance can be produced by some choices.
Formulation as a least squares regression problem in which an ordinary least squares prediction problem is constructed, basing prediction of values of Xt on the p previous values of the same series. This can be thought of as a forward-prediction scheme. The normal equations for this problem can be seen to correspond to an approximation of the matrix form of the Yule–Walker equations in which each appearance of an autocovariance of the same lag is replaced by a slightly different estimate.
Formulation as an extended form of ordinary least squares prediction problem. Here two sets of prediction equations are combined into a single estimation scheme and a single set of normal equations. One set is the set of forward-prediction equations and the other is a corresponding set of backward prediction equations, relating to the backward representation of the AR model:

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            p
          
        
        
          φ
          
            i
          
        
        
          X
          
            t
            +
            i
          
        
        +
        
          ε
          
            t
          
          
            ∗
          
        
        
        .
      
    
    {\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}X_{t+i}+\varepsilon _{t}^{*}\,.}
  

Here predicted values of Xt would be based on the p future values of the same series. This way of estimating the AR parameters is due to John Parker Burg, and is called the Burg method: Burg and later authors called these particular estimates "maximum entropy estimates", but the reasoning behind this applies to the use of any set of estimated AR parameters. Compared to the estimation scheme using only the forward prediction equations, different estimates of the autocovariances are produced, and the estimates have different stability properties. Burg estimates are particularly associated with maximum entropy spectral estimation.
Other possible approaches to estimation include maximum likelihood estimation. Two distinct variants of maximum likelihood are available: in one (broadly equivalent to the forward prediction least squares scheme) the likelihood function considered is that corresponding to the conditional distribution of later values in the series given the initial p values in the series; in the second, the likelihood function considered is that corresponding to the unconditional joint distribution of all the values in the observed series. Substantial differences in the results of these approaches can occur if the observed series is short, or if the process is close to non-stationarity.

Spectrum
The power spectral density (PSD) of an AR(p) process with noise variance 
  
    
      
        
          V
          a
          r
        
        (
        
          Z
          
            t
          
        
        )
        =
        
          σ
          
            Z
          
          
            2
          
        
      
    
    {\displaystyle \mathrm {Var} (Z_{t})=\sigma _{Z}^{2}}
  
 is

  
    
      
        S
        (
        f
        )
        =
        
          
            
              σ
              
                Z
              
              
                2
              
            
            
              
                |
              
              1
              −
              
                ∑
                
                  k
                  =
                  1
                
                
                  p
                
              
              
                φ
                
                  k
                
              
              
                e
                
                  −
                  i
                  2
                  π
                  f
                  k
                
              
              
                
                  |
                
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{|1-\sum _{k=1}^{p}\varphi _{k}e^{-i2\pi fk}|^{2}}}.}

AR(0)
For white noise (AR(0))

  
    
      
        S
        (
        f
        )
        =
        
          σ
          
            Z
          
          
            2
          
        
        .
      
    
    {\displaystyle S(f)=\sigma _{Z}^{2}.}

AR(1)
For AR(1)

  
    
      
        S
        (
        f
        )
        =
        
          
            
              σ
              
                Z
              
              
                2
              
            
            
              
                |
              
              1
              −
              
                φ
                
                  1
                
              
              
                e
                
                  −
                  2
                  π
                  i
                  f
                
              
              
                
                  |
                
                
                  2
                
              
            
          
        
        =
        
          
            
              σ
              
                Z
              
              
                2
              
            
            
              1
              +
              
                φ
                
                  1
                
                
                  2
                
              
              −
              2
              
                φ
                
                  1
                
              
              cos
              ⁡
              2
              π
              f
            
          
        
      
    
    {\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{|1-\varphi _{1}e^{-2\pi if}|^{2}}}={\frac {\sigma _{Z}^{2}}{1+\varphi _{1}^{2}-2\varphi _{1}\cos 2\pi f}}}
  

If 
  
    
      
        
          φ
          
            1
          
        
        >
        0
      
    
    {\displaystyle \varphi _{1}>0}
  
  there is a single spectral peak at f=0, often referred to as red noise. As 
  
    
      
        
          φ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
  becomes nearer 1, there is stronger power at low frequencies, i.e. larger time lags. This is then a low-pass filter, when applied to full spectrum light, everything except for the red light will be filtered.
If 
  
    
      
        
          φ
          
            1
          
        
        <
        0
      
    
    {\displaystyle \varphi _{1}<0}
  
 there is a minimum at f=0, often referred to as blue noise. This similarly acts as a high-pass filter, everything except for blue light will be filtered.

AR(2)
The behavior of an AR(2) process is determined entirely by the roots of it  characteristic equation, which is expressed in terms of the lag operator as:

  
    
      
        1
        −
        
          φ
          
            1
          
        
        B
        −
        
          φ
          
            2
          
        
        
          B
          
            2
          
        
        =
        0
        ,
      
    
    {\displaystyle 1-\varphi _{1}B-\varphi _{2}B^{2}=0,}
  

or equivalently by the poles of its transfer function, which is defined in the Z domain by:

  
    
      
        
          H
          
            z
          
        
        =
        (
        1
        −
        
          φ
          
            1
          
        
        
          z
          
            −
            1
          
        
        −
        
          φ
          
            2
          
        
        
          z
          
            −
            2
          
        
        
          )
          
            −
            1
          
        
        .
      
    
    {\displaystyle H_{z}=(1-\varphi _{1}z^{-1}-\varphi _{2}z^{-2})^{-1}.}
  

It follows that the poles are values of z satisfying:

  
    
      
        1
        −
        
          φ
          
            1
          
        
        
          z
          
            −
            1
          
        
        −
        
          φ
          
            2
          
        
        
          z
          
            −
            2
          
        
        =
        0
      
    
    {\displaystyle 1-\varphi _{1}z^{-1}-\varphi _{2}z^{-2}=0}
  
,
which yields:

  
    
      
        
          z
          
            1
          
        
        ,
        
          z
          
            2
          
        
        =
        
          
            1
            
              2
              
                φ
                
                  2
                
              
            
          
        
        
          (
          
            
              φ
              
                1
              
            
            ±
            
              
                
                  φ
                  
                    1
                  
                  
                    2
                  
                
                +
                4
                
                  φ
                  
                    2
                  
                
              
            
          
          )
        
      
    
    {\displaystyle z_{1},z_{2}={\frac {1}{2\varphi _{2}}}\left(\varphi _{1}\pm {\sqrt {\varphi _{1}^{2}+4\varphi _{2}}}\right)}
  
.

  
    
      
        
          z
          
            1
          
        
      
    
    {\displaystyle z_{1}}
  
 and 
  
    
      
        
          z
          
            2
          
        
      
    
    {\displaystyle z_{2}}
  
 are the reciprocals of the characteristic roots, as well as the eigenvalues of the temporal update matrix: 

  
    
      
        
          
            [
            
              
                
                  
                    φ
                    
                      1
                    
                  
                
                
                  
                    φ
                    
                      2
                    
                  
                
              
              
                
                  1
                
                
                  0
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}\varphi _{1}&\varphi _{2}\\1&0\end{bmatrix}}}
  

AR(2) processes can be split into three groups depending on the characteristics of their roots/poles:

When 
  
    
      
        
          φ
          
            1
          
          
            2
          
        
        +
        4
        
          φ
          
            2
          
        
        <
        0
      
    
    {\displaystyle \varphi _{1}^{2}+4\varphi _{2}<0}
  
, the process has a pair of complex-conjugate poles, creating a mid-frequency peak at:

  
    
      
        
          f
          
            ∗
          
        
        =
        
          
            1
            
              2
              π
            
          
        
        
          cos
          
            −
            1
          
        
        ⁡
        
          (
          
            
              
                φ
                
                  1
                
              
              
                2
                
                  
                    −
                    
                      φ
                      
                        2
                      
                    
                  
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle f^{*}={\frac {1}{2\pi }}\cos ^{-1}\left({\frac {\varphi _{1}}{2{\sqrt {-\varphi _{2}}}}}\right),}
  

with bandwidth about the peak inversely proportional to the moduli of the poles:

  
    
      
        
          |
        
        
          z
          
            1
          
        
        
          |
        
        =
        
          |
        
        
          z
          
            2
          
        
        
          |
        
        =
        
          
            −
            
              φ
              
                2
              
            
          
        
        .
      
    
    {\displaystyle |z_{1}|=|z_{2}|={\sqrt {-\varphi _{2}}}.}
  

The terms involving square roots are all real in the case of complex poles since they exist only when 
  
    
      
        
          φ
          
            2
          
        
        <
        0
      
    
    {\displaystyle \varphi _{2}<0}
  
.
Otherwise the process has real roots, and:

When 
  
    
      
        
          φ
          
            1
          
        
        >
        0
      
    
    {\displaystyle \varphi _{1}>0}
  
 it acts as a low-pass filter on the white noise with a spectral peak at 
  
    
      
        f
        =
        0
      
    
    {\displaystyle f=0}
  

When 
  
    
      
        
          φ
          
            1
          
        
        <
        0
      
    
    {\displaystyle \varphi _{1}<0}
  
 it acts as a high-pass filter on the white noise with a spectral peak at 
  
    
      
        f
        =
        1
        
          /
        
        2
      
    
    {\displaystyle f=1/2}
  
.
The process is non-stationary when the poles are on or outside the unit circle, or equivalently when the characteristic roots are on or inside the unit circle.
The process is stable when the poles are strictly within the unit circle (roots strictly outside the unit circle), or equivalently when the coefficients are in the triangle 
  
    
      
        −
        1
        ≤
        
          φ
          
            2
          
        
        ≤
        1
        −
        
          |
        
        
          φ
          
            1
          
        
        
          |
        
      
    
    {\displaystyle -1\leq \varphi _{2}\leq 1-|\varphi _{1}|}
  
.
The full PSD function can be expressed in real form as:

  
    
      
        S
        (
        f
        )
        =
        
          
            
              σ
              
                Z
              
              
                2
              
            
            
              1
              +
              
                φ
                
                  1
                
                
                  2
                
              
              +
              
                φ
                
                  2
                
                
                  2
                
              
              −
              2
              
                φ
                
                  1
                
              
              (
              1
              −
              
                φ
                
                  2
                
              
              )
              cos
              ⁡
              (
              2
              π
              f
              )
              −
              2
              
                φ
                
                  2
                
              
              cos
              ⁡
              (
              4
              π
              f
              )
            
          
        
      
    
    {\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{1+\varphi _{1}^{2}+\varphi _{2}^{2}-2\varphi _{1}(1-\varphi _{2})\cos(2\pi f)-2\varphi _{2}\cos(4\pi f)}}}

Implementations in statistics packages
R, the stats package includes an ar function.
R, the package astsa includes an sarima function to fit various models including AR models.
MATLAB's Econometrics Toolbox and System Identification Toolbox includes autoregressive models
Matlab and Octave: the TSA toolbox contains several estimation functions for uni-variate, multivariate and adaptive autoregressive models.
PyMC3: the Bayesian statistics and probabilistic programming framework supports autoregressive modes with p lags.
bayesloop supports parameter inference and model selection for the AR-1 process with time-varying parameters.
Python: implementation in statsmodels.

Impulse response
The impulse response of a system is the change in an evolving variable in response to a change in the value of a shock term k periods earlier, as a function of k. Since the AR model is a special case of the vector autoregressive model, the computation of the impulse response in vector autoregression#impulse response applies here.

n-step-ahead forecasting
Once the parameters of the autoregression

  
    
      
        
          X
          
            t
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            p
          
        
        
          φ
          
            i
          
        
        
          X
          
            t
            −
            i
          
        
        +
        
          ε
          
            t
          
        
        
      
    
    {\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}\,}
  

have been estimated, the autoregression can be used to forecast an arbitrary number of periods into the future. First use t to refer to the first period for which data is not yet available; substitute the known preceding values Xt-i for i=1, ..., p into the autoregressive equation while setting the error term 
  
    
      
        
          ε
          
            t
          
        
      
    
    {\displaystyle \varepsilon _{t}}
  
 equal to zero (because we forecast Xt to equal its expected value, and the expected value of the unobserved error term is zero). The output of the autoregressive equation is the forecast for the first unobserved period. Next, use t to refer to the next period for which data is not yet available; again the autoregressive equation is used to make the forecast, with one difference: the value of X one period prior to the one now being forecast is not known, so its expected value—the predicted value arising from the previous forecasting step—is used instead. Then for future periods the same procedure is used, each time using one more forecast value on the right side of the predictive equation until, after  p predictions, all p right-side values are predicted values from preceding steps.
There are four sources of uncertainty regarding predictions obtained in this manner: (1) uncertainty as to whether the autoregressive model is the correct model; (2) uncertainty about the accuracy of the forecasted values that are used as lagged values in the right side of the autoregressive equation; (3) uncertainty about the true values of the autoregressive coefficients; and (4) uncertainty about the value of the error term 
  
    
      
        
          ε
          
            t
          
        
        
      
    
    {\displaystyle \varepsilon _{t}\,}
  
 for the period being predicted. Each of the last three can be quantified and combined to give a confidence interval for the n-step-ahead predictions; the confidence interval will become wider as n increases because of the use of an increasing number of estimated values for the right-side variables.

See also
Moving average model
Linear difference equation
Predictive analytics
Linear predictive coding
Resonance
Levinson recursion
Ornstein–Uhlenbeck process
Infinite impulse response

Notes
References
Mills, Terence C. (1990). Time Series Techniques for Economists. Cambridge University Press. ISBN 9780521343398.
Percival, Donald B.; Walden, Andrew T. (1993). Spectral Analysis for Physical Applications. Cambridge University Press. Bibcode:1993sapa.book.....P.
Pandit, Sudhakar M.; Wu, Shien-Ming (1983). Time Series and System Analysis with Applications. John Wiley & Sons.

External links
AutoRegression Analysis (AR) by Paul Bourke
Econometrics lecture (topic: Autoregressive models) on YouTube by Mark Thoma
Microsoft Azure, or just Azure (/ˈæʒər, ˈeɪʒər/ AZH-ər, AY-zhər, UK also /ˈæzjʊər, ˈeɪzjʊər/ AZ-ure, AY-zure), is the cloud computing platform developed by Microsoft. It has management, access and development of applications and services to individuals, companies, and governments through its global infrastructure. It also provides capabilities that are usually not included within other cloud platforms, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.
Azure was first introduced at the Professional Developers Conference (PDC) in October 2008 under the codename "Project Red Dog". It was officially launched as Windows Azure in February 2010 and later renamed to Microsoft Azure on March 25, 2014.

Services
Microsoft Azure uses large-scale virtualization at Microsoft data centers worldwide and offers more than 600 services.

Computer services
Virtual machines, infrastructure as a service (IaaS), allowing users to launch general-purpose Microsoft Windows and Linux virtual machines, software as a service (SaaS), as well as preconfigured machine images for popular software packages.
Starting in 2022, these virtual machines are now powered by Ampere Cloud-native processors.
Most users run Linux on Azure, some of the many Linux distributions offered, including Microsoft's own Linux-based Azure Sphere.
App services, platform as a service (PaaS) environment, letting developers easily publish and manage websites.
Azure Web Sites allows developers to build sites using ASP.NET, PHP, Node.js, Java, or Python, which can be deployed using FTP, Git, Mercurial, Team Foundation Server, or uploaded through the user portal. This feature was announced in preview form in June 2012 at the Meet Microsoft Azure event. Customers can create websites in PHP, ASP.NET, Node.js, or Python, or select from several open-source applications from a gallery to deploy. This comprises one aspect of the platform as a service (PaaS) offerings for the Microsoft Azure Platform. It was renamed Web Apps in April 2015.
Web Jobs are applications that can be deployed to an App Service environment to implement background processing that can be invoked on a schedule, on-demand, or run continuously. The Blob, Table, and Queue services can be used to communicate between Web Apps and Web Jobs and to provide state.
Azure Kubernetes Service (AKS) provides the capability to deploy production-ready Kubernetes clusters in Azure.
In July 2023, watermarking support on Azure Virtual Desktop was announced as an optional feature of Screen Capture to provide additional security against data leakage.

Identity
Entra ID connect is used to synchronize on-premises directories and enable SSO (Single Sign On).
Entra ID B2C allows the use of consumer identity and access management in the cloud.
Entra Domain Services is used to join Azure virtual machines to a domain without domain controllers.
Azure information protection can be used to protect sensitive information.
Entra ID External Identities is a set of capabilities that allow organizations to collaborate with external users, including customers and partners.
On July 11, 2023, Microsoft announced the renaming of Azure AD to Microsoft Entra ID. The name change took place four days later.

Mobile services
Mobile Engagement collects real-time analytics that highlight users’ behavior. It also provides push notifications to mobile devices.
HockeyApp can be used to develop, distribute, and beta-test mobile apps.

Storage services
Storage Services provides REST and SDK APIs for storing and accessing data on the cloud.
Table Service lets programs store structured text in partitioned collections of entities that are accessed by the partition key and primary key. Azure Table Service is a NoSQL non-relational database.
Blob Service allows programs to store unstructured text and binary data as object storage blobs that can be accessed by an HTTP(S) path. Blob service also provides security mechanisms to control access to data.
Queue Service lets programs communicate asynchronously by message using queues.
File Service allows storing and access of data on the cloud using the REST APIs or the SMB protocol.

Communication services
Azure Communication Services offers an SDK for creating web and mobile communications applications that include SMS, video calling, VOIP and PSTN calling, and web-based chat.

Data management
Azure Data Explorer provides big data analytics and data-exploration capabilities.
Azure Search provides text search and a subset of OData's structured filters using REST or SDK APIs.
Cosmos DB is a NoSQL database service that implements a subset of the SQL SELECT statement on JSON documents.
Azure Cache for Redis is a managed implementation of Redis.
StorSimple manages storage tasks between on-premises devices and cloud storage.
Azure SQL Database works to create, scale, and extend applications into the cloud using Microsoft SQL Server technology. It also integrates with Active Directory, Microsoft System Center, and Hadoop.
Azure Synapse Analytics is a fully managed cloud data warehouse.
Azure Data Factory is a data integration service that allows creation of data-driven workflows in the cloud for orchestrating and automating data movement and data transformation.
Azure Data Lake is a scalable data storage and analytic service for big data analytics workloads that require developers to run massively parallel queries.
Azure HDInsight is a big data-relevant service that deploys Hortonworks Hadoop on Microsoft Azure and supports the creation of Hadoop clusters using Linux with Ubuntu.
Azure Stream Analytics is a Serverless scalable event-processing engine that enables users to develop and run real-time analytics on multiple streams of data from sources such as devices, sensors, websites, social media, and other applications.

Messaging
The Microsoft Azure Service Bus allows applications running on Azure premises or off-premises devices to communicate with Azure. This helps to build scalable and reliable applications in a service-oriented architecture (SOA). The Azure service bus supports four different types of communication mechanisms:

Event Hubs, which provides event and telemetry ingress to the cloud at a massive scale, with low latency and high reliability. For example, an event hub can be used to track data from cell phones such as coordinating with a GPS in real time.
Queues, which allows one-directional communication. A sender application would send the message to the service bus queue and a receiver would read from the queue. Though there can be multiple readers for the queue, only one would process a single message.
Topics, which provides one-directional communication using a subscriber pattern. It is similar to a queue; however, each subscriber will receive a copy of the message sent to a Topic. Optionally, the subscriber can filter out messages based on specific criteria defined by the subscriber.
Relays, which provides bi-directional communication. Unlike queues and topics, a relay does not store in-flight messages in its memory; instead, it just passes them on to the destination application.

Media services
A PaaS offering that can be used for encoding, content protection, streaming, or analytics.

CDN
Azure has a worldwide content delivery network (CDN) designed to efficiently deliver audio, video, applications, images, and other static files. It improves the performance of websites by caching static files closer to users, based on their geographic location. Users can manage the network using a REST-based HTTP API.
Azure has 118 point-of-presence locations across 100 cities worldwide (also known as Edge locations) as of January 2023.

Developer
Application Insights
Azure DevOps

Managements
With Azure Automation, users can easily automate repetitive and time-consuming tasks, often prone to cloud or enterprise setting errors. They can accomplish it using runbooks or desired state configurations for process automation.
Microsoft SMA

Azure AI
Microsoft Azure Machine Learning (Azure ML) provides tools and frameworks for developers to create their own machine learning and artificial intelligence (AI) services.
Azure AI Services by Microsoft comprises prebuilt APIs, SDKs, and services developers can customize. These services encompass perceptual and cognitive intelligence features such as speech recognition, speaker recognition, neural speech synthesis, face recognition, computer vision, OCR/form understanding, natural language processing, machine translation, and business decision services. Many AI characteristics in Microsoft's products and services, namely Bing, Office, Teams, Xbox, and Windows, are driven by Azure AI Services.
Azure AI Studio can be used for building and deploying generative AI applications, notably using OpenAI's foundation model GPT-4o.

Azure Blockchain Workbench
Through Azure Blockchain Workbench, Microsoft is providing the required infrastructure to set up a consortium network in multiple topologies using a variety of consensus mechanisms. Microsoft provides integration from these blockchain platforms to other Microsoft services to streamline the development of distributed applications. Microsoft supports many general-purpose blockchains, including Ethereum and Hyperledger Fabric and purpose-built blockchains like Corda.

Function
Azure functions are used in serverless computing architectures, where subscribers can execute code as an event-driven Function-as-a-Service (FaaS) without managing the underlying server resources. Customers using Azure functions are billed based on per-second resource consumption and executions.

Internet of Things (IoT)
Azure IoT Hub enables the connection, monitoring, and management of a large number of IoT assets. On February 4, 2016, Microsoft announced the General Availability of the Azure IoT Hub service.
Azure IoT Edge is a fully managed service built on IoT Hub that allows for cloud intelligence deployed locally on IoT edge devices.
Azure IoT Central is a fully managed SaaS app that makes it easy to connect, monitor, and manage IoT assets at scale. On December 5, 2017, Microsoft announced the Public Preview of Azure IoT Central, its Azure IoT SaaS service.
On October 4, 2017, Microsoft began shipping GA versions of the official Microsoft Azure IoT Developer Kit (Devkit) board, manufactured by MX Chip.
On April 16, 2018, Microsoft announced the launch of the Azure Sphere, an end-to-end IoT product that focuses on microcontroller-based devices and uses Linux.
On May 7, 2018, Microsoft announced the launch of Azure Maps, an enterprise maps API and SDK platform.
On June 27, 2018, Microsoft launched Azure IoT Edge, used to run Azure services and artificial intelligence on IoT devices.
On November 20, 2018, Microsoft launched the Open Enclave SDK for cross-platform systems such as ARM Trust Zone and Intel SGX.

Azure Stack HCI
Azure Stack HCI is a hyper-converged infrastructure (HCI) product that uses validated hardware to run virtualized workloads on-premises to consolidate aging infrastructure and connect to Azure for cloud services.

Azure Orbital
Launched in September 2020, Azure Orbital lets private industries and government agencies process satellite data quickly by connecting directly to cloud computing networks. Mobile cloud computing ground stations are also available to provide connectivity to remote locations without ground infrastructure. Third-party satellite systems, like SpaceX's Starlink and SES' O3b constellation, can be employed.
SES plans to use Microsoft's data centers to provide cloud connectivity to remote areas through its next generation O3b mPOWER MEO satellites alongside Microsoft's data centers. The company will deploy satellite control and uplink ground stations to achieve this. SES launched the first two O3b mPOWER satellites in December 2022; nine more are scheduled between 2023 and 2024. The service should begin in Q3 2023.
According to Microsoft, using satellites to connect to cloud data centers may provide faster speeds than complex fiber routes. For online media, entertainment, or gaming activities, connecting from home to the cloud can involve longer routes with multiple hops. Through their experiments with Xbox Cloud, Microsoft has discovered that satellite connections are faster than terrestrial networks in certain parts of the world, including specific locations in the USA.

Azure Container Storage
In August 2024, Azure introduced the industry’s first platform-managed container-native storage solution in the public cloud. This service supports Ephemeral Disks (Local NVMe/Temp SSD) and Azure Disks, offering a robust storage solution tailored for containerized applications.

Azure Quantum
Released for public preview in 2021. Azure Quantum provides access to quantum hardware and software. The public cloud computing platform includes multiple quantum hardware modalities including trapped ion, neutral atom, and superconducting systems.
Azure Quantum Elements software for computational chemistry and materials science combines AI, high-performance computing and quantum processors to run molecular simulations and calculations. The service includes Copilot, a GPT-4 based large language model tool to query and visualize data, write code, and initiate simulations.
In 2021, Microsoft developed the quantum programming language Q# (pronounced Q Sharp) and an open-source quantum development kit for algorithm development and simulation.
In 2023, Microsoft developed Quantum Intermediate Representation (QIR) from LLVM as a common interface between programming languages and target quantum processors. 
The Azure Quantum Resource Estimator estimates the resources required to execute a given quantum algorithm on a fault-tolerant quantum computer. It can also show how future quantum computers will impact today’s encryption algorithms.

Regional expansion
As of 2018, Azure was available in 54 regions, and Microsoft was the first primary cloud provider to establish facilities in Africa, with two regions in South Africa. Azure geographies consist of multiple Azure Regions, like "North Europe" (located in Dublin, Ireland) and "West Europe" (located in Amsterdam, Netherlands).
On June 19, 2019, Microsoft announced the launch of two new cloud regions in the United Arab Emirates – Microsoft's first in the Middle East.

Research partnerships
Microsoft has partners that sell its products. In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The specific computer programs used in the process fall under the Azure Machine Learning and the Azure IoT Hub platforms.

Design
Microsoft Azure utilizes a specialized operating system with the same name to power its "fabric layer". This cluster is hosted at Microsoft's data centers and is responsible for managing computing and storage resources and allocating them to applications running on the Microsoft Azure platform. It is a "cloud layer" built upon various Windows Server systems, including the customized Microsoft Azure Hypervisor, which is based on Windows Server 2008 and enables the virtualization of services.
The Microsoft Azure Fabric Controller maintains the scalability and dependability of services and environments in the data center. It prevents failure in server malfunction and manages users' web applications, including memory allocation and load balancing.
Azure provides an API built on REST, HTTP, and XML that allows a developer to interact with the services offered by Microsoft Azure. Microsoft also provides a client-side managed class library that encapsulates the functions of interacting with the services. It also integrates with Microsoft Visual Studio, Git, and Eclipse.
Users can manage Azure services in multiple ways, one of which is through the Web-based Azure Portal, which became generally available in December 2015. Apart from accessing services via API, users can browse active resources, adjust settings, launch new resources, and view primary monitoring data of functional virtual machines and services using the portal.

Deployment models
Regarding cloud resources, Microsoft Azure offers two deployment models: the "classic" model and the Azure Resource Manager. In the classic model, each resource, like a virtual machine or SQL database, had to be managed separately, but in 2014, Azure introduced the Azure Resource Manager, which allows users to group related services. This update makes it easier and more efficient to deploy, manage, and monitor resources that work closely together. The classic model will eventually be phased out.

History and timeline
In 2005, Microsoft took over Groove Networks, and Bill Gates made Groove's founder Ray Ozzie one of his 5 direct reports as one of 3 chief technology officers. Ozzie met with Amitabh Srivastava, which let Srivastava change course. They convinced Dave Cutler to postpone his retirement, and their teams developed a cloud operating system.

October 2008 (PDC LA) – Announced the Windows Azure Platform.
March 2009 – Announced SQL Azure Relational Database.
November 2009 – Updated Windows Azure CTP, Enabled full trust, PHP, Java, CDN CTP, and more.
February 1, 2010 – Windows Azure Platform commercially available.
June 2010 – Windows Azure Update, .NET Framework 4, OS Versioning, CDN, SQL Azure Update.
October 2010 (PDC) – Platform enhancements, Windows Azure Connect, improved Dev / IT Pro Experience.
December 2011 – Traffic manager, SQL Azure reporting, HPC scheduler.
June 2012 – Websites, Virtual machines for Windows and Linux, Python SDK, new portal, locally redundant storage.
April 2014 – Windows Azure renamed Microsoft Azure, ARM Portal introduced at Build 2014.
July 2014 – Azure Machine Learning public preview.
November 2014 – Outage affecting major websites, including MSN.com.
September 2015 – Azure Cloud Switch introduced as a cross-platform Linux distribution. Currently known as SONiC.
December 2015 – Azure ARM Portal (codename "Ibiza") released.
March 2016 – Azure Service Fabric is Generally Available (GA).
November 15, 2016 – Azure Functions is Generally Available (GA).
May 10, 2017 – Azure Cosmos DB is Generally Available (GA).
May 7, 2018 – Azure Maps is Generally Available (GA).
July 16, 2018 – Azure Service Fabric Mesh public preview.
September 24, 2018 – Microsoft Azure IoT Central is Generally Available (GA).
October 10, 2018 – Microsoft joins the Linux-oriented group Open Invention Network.
April 17, 2019 – Azure Front Door Service is now available.
March 2020 – Microsoft said that there was a 775% increase in Microsoft Teams usage in Italy due to the COVID-19 pandemic. The company estimates there are now 44 million daily active users of Teams worldwide.
January 17, 2023 – Azure OpenAI Service is Generally Available (GA).

Privacy
According to the Patriot Act, Microsoft has acknowledged that the U.S. government can access data even if the hosting company is not American and the data is outside the U.S. To address concerns related to privacy and security, Microsoft has established the Microsoft Azure Trust Center. Microsoft Azure offers services that comply with multiple compliance programs, including ISO 27001:2005 and HIPAA. A comprehensive and up-to-date list of these services is available on the Microsoft Azure Trust Center Compliance page. Microsoft Azure received JAB Provisional Authority to Operate (P-ATO) from the U.S. government under the Federal Risk and Authorization Management Program (FedRAMP) guidelines. This program provides a standardized approach to security assessment, authorization, and continuous monitoring for cloud services used by the federal government.

Security
In July 2023, U.S. Senator Ron Wyden called on the Cybersecurity and Infrastructure Security Agency (CISA), the Justice Department, and the Federal Trade Commission to hold Microsoft accountable for what he described as "negligent cybersecurity practices." This came in the wake of an alleged cyberattack orchestrated by Chinese hackers, who exploited a vulnerability in Microsoft's software to compromise U.S. government email systems. Similarly, Amit Yoran, the CEO of cybersecurity firm Tenable, Inc., lambasted Microsoft for what he termed "grossly irresponsible" actions, accusing the company of fostering a "culture of toxic obfuscation." The Cyber Safety Review Board produced a report that blamed Microsoft about a cascade of security failures that allowed the intrusion to succeed. Microsoft's security culture was called inadequate.

Significant outages
The following is a list of Microsoft Azure outages and service disruptions.

Certifications
A large variety of Azure certifications can be attained, each requiring one or multiple successfully completed examinations. Certification levels range from beginner, intermediate to expert.
Examples of common certifications include:

Azure Fundamentals
Azure Data Fundamentals
Azure AI Engineer Associate
Azure AI Fundamentals
Azure Cosmos DB Developer Specialty
Azure Administrator Associate
Azure Data Engineer Associate
Azure Data Scientist Associate
Azure Database Administrator Associate
Azure Developer Associate
Azure Enterprise Data Analyst Associate
Azure Security Engineer Associate
Azure Security Operations Analyst Associate
Azure Identity and Access Administrator Associate
Azure Security, Compliance, and Identity Fundamentals
Azure Network Engineer Associate
Azure Windows Server Hybrid Administrator Associate
Azure Virtual Desktop Specialty
Azure for SAP Workloads Specialty
Azure Customer Data Platform Specialty
Azure Cybersecurity Architect Expert
Azure Solutions Architect Expert
Azure Power Platform Solution Architect Expert
Azure DevOps Engineer Expert
Azure IoT Developer Specialty
Azure Stack Hub Operator Associate
Azure Machine Learning Specialty

Key people
Dave Cutler, Lead Developer, Microsoft Azure
Mark Russinovich, CTO, Microsoft Azure
Scott Guthrie, Executive Vice President of the Cloud and AI group in Microsoft
Jason Zander, Executive Vice President, Microsoft Azure
Julia White, Corporate Vice President, Microsoft Azure

See also
Cloud-computing comparison
Comparison of file hosting services
Microsoft Azure Dev Tools for Teaching
Azure Linux

References
Citations
Sources
Further reading
External links
Official website
Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learned by self-supervised learning to represent text as a sequence of vectors. It had the transformer encoder architecture. It was notable for its dramatic improvement over previous state of the art models, and as an early example of large language model. As of 2020, BERT was a ubiquitous baseline in natural language processing (NLP) experiments. 
BERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of "BERTology", which attempts to interpret what is learned by BERT.
BERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.

Architecture
BERT is an "encoder-only" transformer architecture. At a high level, BERT consists of 4 modules: 

Tokenizer: This module converts a piece of English text into a sequence of integers ("tokens").
Embedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.
Encoder: a stack of Transformer blocks with self-attention, but without causal masking.
Task head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an "un-embedding layer".
The task head is necessary for pre-training, but it is often unnecessary for so-called "downstream tasks," such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.

Embedding
This section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.
The tokenizer of BERT is WordPiece, which is a sub-word strategy like byte pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] ("unknown"). 

The first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. 

Token type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.
Position: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.
Segment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.
The three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.

Architectural family
The encoder stack of BERT has 2 free parameters: 
  
    
      
        L
      
    
    {\displaystyle L}
  
, the number of layers, and 
  
    
      
        H
      
    
    {\displaystyle H}
  
, the hidden size. There are always 
  
    
      
        H
        
          /
        
        64
      
    
    {\displaystyle H/64}
  
 self-attention heads, and the feed-forward/filter size is always 
  
    
      
        4
        H
      
    
    {\displaystyle 4H}
  
. By varying these two numbers, one obtains an entire family of BERT models.
For BERT

the feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.
the hidden size and embedding size are synonymous. Both of them denote the number of real numbers used to represent a token.
The notation for encoder stack is written as L/H. For example, BERTBASE is written as 12L/768H, BERTLARGE as 24L/1024H, and BERTTINY as 2L/128H.

Training
Pre-training
BERT was pre-trained simultaneously on two tasks.

Masked language modeling
In masked language modeling, 15% of tokens would be randomly selected for masked-prediction task, and the training objective was to predict the masked token given its context. In more detail, the selected token is 

replaced with a [MASK] token with probability 80%,
replaced with a random word token with probability 10%,
not replaced with probability 10%.
The reason not all selected tokens are masked is to avoid the dataset shift problem. The dataset shift problem arises when the distribution of inputs seen during training differs significantly from the distribution encountered during inference. A trained BERT model might be applied to word representation (like Word2Vec), where it would be run over sentences not containing any [MASK] tokens. It is later found that more diverse training objectives are generally better.
As an illustrative example, consider the sentence "my dog is cute". It would first be divided into tokens like "my1 dog2 is3 cute4". Then a random token in the sentence would be picked. Let it be the 4th one "cute4". Next, there would be three possibilities:

with probability 80%, the chosen token is masked, resulting in "my1 dog2 is3 [MASK]4";
with probability 10%, the chosen token is replaced by a uniformly sampled random token, such as "happy", resulting in "my1 dog2 is3 happy4";
with probability 10%, nothing is done, resulting in "my1 dog2 is3 cute4".
After processing the input text, the model's 4th output vector is passed to its decoder layer, which outputs a probability distribution over its 30,000-dimensional vocabulary space.

Next sentence prediction
Given two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for "classify"). The two spans are separated by a special token [SEP] (for "separate"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].

For example, given "[CLS] my dog is cute [SEP] he likes playing" the model should output token [IsNext].
Given "[CLS] my dog is cute [SEP] how do magnets work" the model should output token [NotNext].

Fine-tuning
BERT is meant as a general pretrained model for various applications in natural language processing. That is, after pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as natural language inference and text classification, and sequence-to-sequence-based language generation tasks such as question answering and conversational response generation.
The original BERT paper published results demonstrating that a small amount of finetuning (for BERTLARGE, 1 hour on 1 Cloud TPU) allowed it to achieved state-of-the-art performance on a number of natural language understanding tasks:

GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks);
SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0;
SWAG (Situations With Adversarial Generations).
In the original paper, all parameters of BERT are finetuned, and recommended that, for downstream applications that are text classifications, the output token at the [CLS] input token is fed into a linear-softmax layer to produce the label outputs.
The original code base defined the final linear layer as a "pooler layer", in analogy with global pooling in computer vision, even though it simply discards all output tokens except the one corresponding to  [CLS] .

Cost
BERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.
Training BERTBASE  on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. Training BERTLARGE on 16 cloud TPU (64 TPU chips total) took 4 days.

Interpretation
Language models like ELMo, GPT-2, and BERT, spawned the study of "BERTology", which attempts to interpret what is learned by these models. Their performance on these natural language understanding tasks are not yet well understood. Several research publications in 2018 and 2019 focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.
The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.
However it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt. As an illustrative example, if one wishes to use BERT to continue a sentence fragment "Today, I went to", then naively one would mask out all the tokens as "Today, I went to  [MASK]  [MASK]  [MASK] ...  [MASK] ." where the number of  [MASK]  is the length of the sentence one wishes to extend to. However, this constitutes a dataset shift, as during training, BERT has never seen sentences with that many tokens masked out. Consequently, its performance degrades. More sophisticated techniques allow text generation, but at a high computational cost.

History
BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for "running" will have the same word2vec vector representation for both of its occurrences in the sentences "He is running a company" and "He is running a marathon", BERT will provide a contextualized embedding that will be different according to the sentence.
On October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.

Variants
The BERT models were influential and inspired many variants.
RoBERTa (2019) was an engineering improvement. It preserves BERT's architecture (slightly larger, at 355M parameters), but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes. 
DistilBERT (2019) distills BERTBASE to a model with just 60% of its parameters (66M), while preserving 95% of its benchmark scores. Similarly, TinyBERT (2019) is a distilled model with just 28% of its parameters.
ALBERT (2019) used shared-parameter across layers, and experimented with independently varying the hidden size and the word-embedding layer's output size as two hyperparameters. They also replaced the next sentence prediction task with the sentence-order prediction (SOP) task, where the model must distinguish the correct order of two consecutive text segments from their reversed order. 
ELECTRA (2020) applied the idea of generative adversarial networks to the MLM task. Instead of masking out tokens, a small language model generates random plausible plausible substitutions, and a larger network identify these replaced tokens. The small model aims to fool the large model.

DeBERTa
DeBERTa (2020) is a significant architectural variant, with disentangled attention. Its key idea is to treat the positional and token encodings separately throughout the attention mechanism. Instead of combining the positional encoding (
  
    
      
        
          x
          
            p
            o
            s
            i
            t
            i
            o
            n
          
        
      
    
    {\displaystyle x_{position}}
  
) and token encoding (
  
    
      
        
          x
          
            token
          
        
      
    
    {\displaystyle x_{\text{token}}}
  
) into a single input vector (
  
    
      
        
          x
          
            i
            n
            p
            u
            t
          
        
        =
        
          x
          
            p
            o
            s
            i
            t
            i
            o
            n
          
        
        +
        
          x
          
            t
            o
            k
            e
            n
          
        
      
    
    {\displaystyle x_{input}=x_{position}+x_{token}}
  
), DeBERTa keeps them separate as a tuple: (
  
    
      
        (
        
          x
          
            p
            o
            s
            i
            t
            i
            o
            n
          
        
        ,
        
          x
          
            t
            o
            k
            e
            n
          
        
        )
      
    
    {\displaystyle (x_{position},x_{token})}
  
). Then, at each self-attention layer, DeBERTa computes three distinct attention matrices, rather than the single attention matrix used in BERT:

The three attention matrices are added together element-wise, then passed through a softmax layer and multiplied by a projection matrix.
Absolute position encoding is included in the final self-attention layer as additional input.

Notes
References
Further reading
Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What we know about how BERT works". arXiv:2002.12327 [cs.CL].

External links
Official GitHub repository
BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.
Its inventors claim BIRCH to be the "first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively", beating DBSCAN by two months. The BIRCH algorithm received the SIGMOD 10 year test of time award in 2006.

Problem with previous methods
Previous clustering algorithms performed less effectively over very large databases and did not adequately consider the case wherein a data-set was too large to fit in main memory. As a result, there was a lot of overhead maintaining high clustering quality while minimizing the cost of additional IO (input/output) operations. Furthermore, most of BIRCH's predecessors inspect all data points (or all currently existing clusters) equally for each 'clustering decision' and do not perform heuristic weighting based on the distance between these data points.

Advantages with BIRCH
It is local in that each clustering decision is made without scanning all data points and currently existing clusters.
It exploits the observation that the data space is not usually uniformly occupied and not every data point is equally important.
It makes full use of available memory to derive the finest possible sub-clusters while minimizing I/O costs.
It is also an incremental method that does not require the whole data set in advance.

Algorithm
The BIRCH algorithm takes as input a set of N data points, represented as real-valued vectors, and a desired number of clusters K. It operates in four phases, the second of which is optional.
The first phase builds a clustering feature (
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  
) tree out of the data points, a height-balanced tree data structure, defined as follows:

Given a set of N d-dimensional data points, the clustering feature 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  
 of the set is defined as the triple 
  
    
      
        C
        F
        =
        (
        N
        ,
        
          
            
              L
              S
            
            →
          
        
        ,
        S
        S
        )
      
    
    {\displaystyle CF=(N,{\overrightarrow {LS}},SS)}
  
, where

  
    
      
        
          
            
              L
              S
            
            →
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          
            
              X
              
                i
              
            
            →
          
        
      
    
    {\displaystyle {\overrightarrow {LS}}=\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}
  
 is the linear sum.

  
    
      
        S
        S
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        (
        
          
            
              X
              
                i
              
            
            →
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle SS=\sum _{i=1}^{N}({\overrightarrow {X_{i}}})^{2}}
  
 is the square sum of data points.
Clustering features are organized in a CF tree, a height-balanced tree with two parameters: branching factor 
  
    
      
        B
      
    
    {\displaystyle B}
  
 and threshold 
  
    
      
        T
      
    
    {\displaystyle T}
  
. Each non-leaf node contains at most 
  
    
      
        B
      
    
    {\displaystyle B}
  
 entries of the form 
  
    
      
        [
        C
        
          F
          
            i
          
        
        ,
        c
        h
        i
        l
        
          d
          
            i
          
        
        ]
      
    
    {\displaystyle [CF_{i},child_{i}]}
  
, where 
  
    
      
        c
        h
        i
        l
        
          d
          
            i
          
        
      
    
    {\displaystyle child_{i}}
  
 is a pointer to its 
  
    
      
        i
      
    
    {\displaystyle i}
  
th child node and 
  
    
      
        C
        
          F
          
            i
          
        
      
    
    {\displaystyle CF_{i}}
  
 the clustering feature representing the associated subcluster. A leaf node contains at most 
  
    
      
        L
      
    
    {\displaystyle L}
  
 entries each of the form 
  
    
      
        [
        C
        
          F
          
            i
          
        
        ]
      
    
    {\displaystyle [CF_{i}]}
  
 . It also has two pointers prev and next which are used to chain all leaf nodes together. The tree size depends on the parameter 
  
    
      
        T
      
    
    {\displaystyle T}
  
. A node is required to fit in a page of size 
  
    
      
        P
      
    
    {\displaystyle P}
  
. 
  
    
      
        B
      
    
    {\displaystyle B}
  
 and 
  
    
      
        L
      
    
    {\displaystyle L}
  
 are determined by 
  
    
      
        P
      
    
    {\displaystyle P}
  
. So 
  
    
      
        P
      
    
    {\displaystyle P}
  
 can be varied for performance tuning. It is a very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster.
In the second step, the algorithm scans all the leaf entries in the initial 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  
 tree to rebuild a smaller 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  
 tree, while removing outliers and grouping crowded subclusters into larger ones. This step is marked optional in the original presentation of BIRCH.
In step three an existing clustering algorithm is used to cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  
 vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution pattern in the data. However, there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain a new set of clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.

Calculations with the clustering features
Given only the clustering feature 
  
    
      
        C
        F
        =
        [
        N
        ,
        
          
            
              L
              S
            
            →
          
        
        ,
        S
        S
        ]
      
    
    {\displaystyle CF=[N,{\overrightarrow {LS}},SS]}
  
, the same measures can be calculated without the knowledge of the underlying actual values.

Centroid: 
  
    
      
        
          
            C
            →
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  N
                
              
              
                
                  
                    X
                    
                      i
                    
                  
                  →
                
              
            
            N
          
        
        =
        
          
            
              
                L
                S
              
              →
            
            N
          
        
      
    
    {\displaystyle {\overrightarrow {C}}={\frac {\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}{N}}={\frac {\overrightarrow {LS}}{N}}}
  

Radius: 
  
    
      
        R
        =
        
          
            
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    N
                  
                
                (
                
                  
                    
                      X
                      
                        i
                      
                    
                    →
                  
                
                −
                
                  
                    C
                    →
                  
                
                
                  )
                  
                    2
                  
                
              
              N
            
          
        
        =
        
          
            
              
                N
                ⋅
                
                  
                    
                      C
                      →
                    
                  
                  
                    2
                  
                
                +
                S
                S
                −
                2
                ⋅
                
                  
                    C
                    →
                  
                
                ⋅
                
                  
                    
                      L
                      S
                    
                    →
                  
                
              
              N
            
          
        
        =
        
          
            
              
                
                  S
                  S
                
                N
              
            
            −
            (
            
              
                
                  
                    L
                    S
                  
                  →
                
                N
              
            
            
              )
              
                2
              
            
          
        
      
    
    {\displaystyle R={\sqrt {\frac {\sum _{i=1}^{N}({\overrightarrow {X_{i}}}-{\overrightarrow {C}})^{2}}{N}}}={\sqrt {\frac {N\cdot {\overrightarrow {C}}^{2}+SS-2\cdot {\overrightarrow {C}}\cdot {\overrightarrow {LS}}}{N}}}={\sqrt {{\frac {SS}{N}}-({\frac {\overrightarrow {LS}}{N}})^{2}}}}
  

Average Linkage Distance between clusters 
  
    
      
        C
        
          F
          
            1
          
        
        =
        [
        
          N
          
            1
          
        
        ,
        
          
            
              L
              
                S
                
                  1
                
              
            
            →
          
        
        ,
        S
        
          S
          
            1
          
        
        ]
      
    
    {\displaystyle CF_{1}=[N_{1},{\overrightarrow {LS_{1}}},SS_{1}]}
  
 and 
  
    
      
        C
        
          F
          
            2
          
        
        =
        [
        
          N
          
            2
          
        
        ,
        
          
            
              L
              
                S
                
                  2
                
              
            
            →
          
        
        ,
        S
        
          S
          
            2
          
        
        ]
      
    
    {\displaystyle CF_{2}=[N_{2},{\overrightarrow {LS_{2}}},SS_{2}]}
  
:
  
    
      
        
          D
          
            2
          
        
        =
        
          
            
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    
                      N
                      
                        1
                      
                    
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    
                      N
                      
                        2
                      
                    
                  
                
                (
                
                  
                    
                      X
                      
                        i
                      
                    
                    →
                  
                
                −
                
                  
                    
                      Y
                      
                        j
                      
                    
                    →
                  
                
                
                  )
                  
                    2
                  
                
              
              
                
                  N
                  
                    1
                  
                
                ⋅
                
                  N
                  
                    2
                  
                
              
            
          
        
        =
        
          
            
              
                
                  N
                  
                    1
                  
                
                ⋅
                S
                
                  S
                  
                    2
                  
                
                +
                
                  N
                  
                    2
                  
                
                ⋅
                S
                
                  S
                  
                    1
                  
                
                −
                2
                ⋅
                
                  
                    
                      L
                      
                        S
                        
                          1
                        
                      
                    
                    →
                  
                
                ⋅
                
                  
                    
                      L
                      
                        S
                        
                          2
                        
                      
                    
                    →
                  
                
              
              
                
                  N
                  
                    1
                  
                
                ⋅
                
                  N
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle D_{2}={\sqrt {\frac {\sum _{i=1}^{N_{1}}\sum _{j=1}^{N_{2}}({\overrightarrow {X_{i}}}-{\overrightarrow {Y_{j}}})^{2}}{N_{1}\cdot N_{2}}}}={\sqrt {\frac {N_{1}\cdot SS_{2}+N_{2}\cdot SS_{1}-2\cdot {\overrightarrow {LS_{1}}}\cdot {\overrightarrow {LS_{2}}}}{N_{1}\cdot N_{2}}}}}
  

In multidimensional cases the square root should be replaced with a suitable norm.
BIRCH uses the distances DO to D3 to find the nearest leaf, then the radius R or the diameter D to decide whether to absorb the data into the existing leaf or whether to add a new leaf.

Numerical issues in BIRCH clustering features
Unfortunately, there are numerical issues associated with the use of the term 
  
    
      
        S
        S
      
    
    {\displaystyle SS}
  
 in BIRCH. When subtracting 
  
    
      
        
          
            
              S
              S
            
            N
          
        
        −
        
          
            (
          
        
        
          
            
              
                
                  L
                  S
                
                →
              
            
            N
          
        
        
          
            
              )
            
          
          
            2
          
        
      
    
    {\displaystyle {\frac {SS}{N}}-{\big (}{\frac {\vec {LS}}{N}}{\big )}^{2}}
  
 or similar in the other distances such as 
  
    
      
        
          D
          
            2
          
        
      
    
    {\displaystyle D_{2}}
  
, catastrophic cancellation can occur and yield a poor precision, and which can in some cases even cause the result to be negative (and the square root then become undefined). This can be resolved by using BETULA cluster features 
  
    
      
        C
        F
        =
        (
        N
        ,
        μ
        ,
        S
        )
      
    
    {\displaystyle CF=(N,\mu ,S)}
  
 instead, which store the count 
  
    
      
        N
      
    
    {\displaystyle N}
  
, mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
, and sum of squared deviations instead based on numerically more reliable online algorithms to calculate variance. For these features, a similar additivity theorem holds. When storing a vector respectively a matrix for the squared deviations, the resulting BIRCH CF-tree can also be used to accelerate Gaussian Mixture Modeling with the expectation–maximization algorithm, besides k-means clustering and hierarchical agglomerative clustering.
Instead of storing the linear sum and the sum of squares, we can instead store the mean and the squared deviation from the mean in each cluster feature 
  
    
      
        C
        
          F
          ′
        
        =
        (
        N
        ,
        μ
        ,
        S
        )
      
    
    {\displaystyle CF'=(N,\mu ,S)}
  
, where

  
    
      
        n
      
    
    {\displaystyle n}
  
 is the node weight (number of points)

  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 is the node center vector (arithmetic mean, centroid)

  
    
      
        S
      
    
    {\displaystyle S}
  
 is the sum of squared deviations from the mean (either a vector, or a sum to conserve memory, depending on the application)
The main difference here is that S is computed relative to the center, instead of relative to the origin.
A single point 
  
    
      
        x
      
    
    {\displaystyle x}
  
 can be cast into a cluster feature 
  
    
      
        C
        
          F
          
            x
          
        
        =
        (
        1
        ,
        x
        ,
        0
        )
      
    
    {\displaystyle CF_{x}=(1,x,0)}
  
. In order to combine two cluster features 
  
    
      
        C
        
          F
          
            A
            B
          
        
        =
        C
        
          F
          
            A
          
        
        +
        C
        
          F
          
            B
          
        
      
    
    {\displaystyle CF_{AB}=CF_{A}+CF_{B}}
  
, we use

  
    
      
        
          N
          
            A
            B
          
        
        =
        
          N
          
            A
          
        
        +
        
          N
          
            B
          
        
      
    
    {\displaystyle N_{AB}=N_{A}+N_{B}}
  

  
    
      
        
          μ
          
            A
            B
          
        
        =
        
          μ
          
            A
          
        
        +
        
          
            
              N
              
                B
              
            
            
              N
              
                A
                B
              
            
          
        
        (
        
          μ
          
            B
          
        
        −
        
          μ
          
            A
          
        
        )
      
    
    {\displaystyle \mu _{AB}=\mu _{A}+{\frac {N_{B}}{N_{AB}}}(\mu _{B}-\mu _{A})}
  
 (incremental update of the mean)

  
    
      
        
          S
          
            A
            B
          
        
        =
        
          S
          
            A
          
        
        +
        
          S
          
            B
          
        
        +
        
          N
          
            B
          
        
        (
        
          μ
          
            B
          
        
        −
        
          μ
          
            A
          
        
        )
        ∘
        (
        
          μ
          
            B
          
        
        −
        
          μ
          
            A
            B
          
        
        )
      
    
    {\displaystyle S_{AB}=S_{A}+S_{B}+N_{B}(\mu _{B}-\mu _{A})\circ (\mu _{B}-\mu _{AB})}
  
 in vector form using the element-wise product, respectively

  
    
      
        
          S
          
            A
            B
          
        
        =
        
          S
          
            A
          
        
        +
        
          S
          
            B
          
        
        +
        
          N
          
            B
          
        
        (
        
          μ
          
            B
          
        
        −
        
          μ
          
            A
          
        
        
          )
          
            T
          
        
        (
        
          μ
          
            B
          
        
        −
        
          μ
          
            A
            B
          
        
        )
      
    
    {\displaystyle S_{AB}=S_{A}+S_{B}+N_{B}(\mu _{B}-\mu _{A})^{T}(\mu _{B}-\mu _{AB})}
  
 to update a scalar sum of squared deviations
These computations use numerically more reliable computations (c.f. online computation of the variance) that avoid the subtraction of two similar squared values. The centroid is simply the node center vector 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
, and can directly be used for distance computations using, e.g., the Euclidean or Manhattan distances. The radius simplifies to 
  
    
      
        R
        =
        
          
            
              
                1
                N
              
            
            S
          
        
      
    
    {\displaystyle R={\sqrt {{\frac {1}{N}}S}}}
  
 and the diameter to 
  
    
      
        D
        =
        
          
            
              
                2
                
                  N
                  −
                  1
                
              
            
            S
          
        
      
    
    {\displaystyle D={\sqrt {{\frac {2}{N-1}}S}}}
  
.
We can now compute the different distances D0 to D4 used in the BIRCH algorithm as:

Euclidean distance 
  
    
      
        
          D
          
            0
          
        
        =
        ‖
        
          μ
          
            A
          
        
        −
        
          μ
          
            B
          
        
        ‖
      
    
    {\displaystyle D_{0}=\|\mu _{A}-\mu _{B}\|}
  
 and Manhattan distance 
  
    
      
        
          D
          
            1
          
        
        =
        ‖
        
          μ
          
            A
          
        
        −
        
          μ
          
            B
          
        
        
          ‖
          
            1
          
        
      
    
    {\displaystyle D_{1}=\|\mu _{A}-\mu _{B}\|_{1}}
  
 are computed using the CF centers 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  

Inter-cluster distance 
  
    
      
        
          D
          
            2
          
        
        =
        
          
            
              
                1
                
                  N
                  
                    A
                  
                
              
            
            
              S
              
                A
              
            
            +
            
              
                1
                
                  N
                  
                    B
                  
                
              
            
            
              S
              
                B
              
            
            +
            
              
                ‖
              
            
            
              μ
              
                A
              
            
            −
            
              μ
              
                B
              
            
            
              
                
                  ‖
                
              
              
                2
              
            
          
        
      
    
    {\displaystyle D_{2}={\sqrt {{\frac {1}{N_{A}}}S_{A}+{\frac {1}{N_{B}}}S_{B}+{\big \|}\mu _{A}-\mu _{B}{\big \|}^{2}}}}
  

Intra-cluster distance 
  
    
      
        
          D
          
            3
          
        
        =
        
          
            
              
                2
                
                  
                    N
                    
                      A
                      B
                    
                  
                  (
                  
                    N
                    
                      A
                      B
                    
                  
                  −
                  1
                  )
                
              
            
            
              (
              
                
                  N
                  
                    A
                    B
                  
                
                (
                
                  S
                  
                    A
                  
                
                +
                
                  S
                  
                    B
                  
                
                )
                +
                
                  N
                  
                    A
                  
                
                
                  N
                  
                    B
                  
                
                
                  
                    ‖
                  
                
                
                  μ
                  
                    A
                  
                
                −
                
                  μ
                  
                    B
                  
                
                
                  
                    
                      ‖
                    
                  
                  
                    2
                  
                
              
              )
            
          
        
      
    
    {\displaystyle D_{3}={\sqrt {{\frac {2}{N_{AB}(N_{AB}-1)}}\left(N_{AB}(S_{A}+S_{B})+N_{A}N_{B}{\big \|}\mu _{A}-\mu _{B}{\big \|}^{2}\right)}}}
  

Variance-increase distance 
  
    
      
        
          D
          
            4
          
        
        =
        
          
            
              
                
                  
                    N
                    
                      A
                    
                  
                  
                    N
                    
                      B
                    
                  
                
                
                  N
                  
                    A
                    B
                  
                
              
            
            
              
                ‖
              
            
            
              μ
              
                A
              
            
            −
            
              μ
              
                B
              
            
            
              
                
                  ‖
                
              
              
                2
              
            
          
        
      
    
    {\displaystyle D_{4}={\sqrt {{\frac {N_{A}N_{B}}{N_{AB}}}{\big \|}\mu _{A}-\mu _{B}{\big \|}^{2}}}}
  

These distances can also be used to initialize the distance matrix for hierarchical clustering, depending on the chosen linkage. For accurate hierarchical clustering and k-means clustering, we also need to use the node weight 
  
    
      
        N
      
    
    {\displaystyle N}
  
.

Clustering Step
The CF-tree provides a compressed summary of the data set, but the leaves themselves only provide a very poor data clustering.
In a second step, the leaves can be clustered using, e.g.,

k-means clustering, where leaves are weighted by the numbers of points, N.
k-means++, by sampling cluster features proportional to 
  
    
      
        S
        +
        N
        
          min
          
            i
          
        
        
          |
        
        
          |
        
        μ
        −
        
          c
          
            i
          
        
        
          |
        
        
          |
        
      
    
    {\displaystyle S+N\min _{i}||\mu -c_{i}||}
  
 where the 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
  
 are the previously chosen centers, and 
  
    
      
        (
        N
        ,
        μ
        ,
        S
        )
      
    
    {\displaystyle (N,\mu ,S)}
  
 is the BETULA cluster feature.
Gaussian mixture modeling, where also the variance S can be taken into account, and if the leaves store covariances, also the covariances.
Hierarchical agglomerative clustering, where the linkage can be initialized using the following equivalence of linkages to BIRCH distances:

Availability
ELKI contains BIRCH and BETULA.
scikit-learn contains a limited version of BIRCH, which only supports D0 distance, static thresholds, and which uses only the centroids of the leaves in the clustering step.


== References ==
BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model, as well as the code base and the data used to train it, are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.
BLOOM is the main outcome of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS), on which it was trained.
BLOOM's training corpus, named ROOTS, combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages.


== References ==
A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a "homunculus computer"—a tiny computer-within-a-computer such as that found in Intel's AMT technology). Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.
A backdoor may take the form of a hidden part of a program, a separate program (e.g. Back Orifice may subvert the system through a rootkit), code in the firmware of the hardware, or parts of an operating system such as Windows. Trojan horses can be used to create vulnerabilities in a device. A Trojan horse may appear to be an entirely legitimate program, but when executed, it triggers an activity that may install a backdoor. Although some are secretly installed, other backdoors are deliberate and widely known. These kinds of backdoors have "legitimate" uses such as providing the manufacturer with a way to restore user passwords.
Many systems that store information within the cloud fail to create accurate security measures. If many systems are connected within the cloud, hackers can gain access to all other platforms through the most vulnerable system. Default passwords (or other default credentials) can function as backdoors if they are not changed by the user. Some debugging features can also act as backdoors if they are not removed in the release version. In 1993, the United States government attempted to deploy an encryption system, the Clipper chip, with an explicit backdoor for law enforcement and national security access. The chip was unsuccessful.
Recent proposals to counter backdoors include creating a database of backdoors' triggers and then using neural networks to detect them.

Overview
The threat of backdoors surfaced when multiuser and networked operating systems became widely adopted. Petersen and Turn discussed computer subversion in a paper published in the proceedings of the 1967 AFIPS Conference. They noted a class of active infiltration attacks that use "trapdoor" entry points into the system to bypass security facilities and permit direct access to data. The use of the word trapdoor here clearly coincides with more recent definitions of a backdoor. However, since the advent of public key cryptography the term trapdoor has acquired a different meaning (see trapdoor function), and thus the term "backdoor" is now preferred, only after the term trapdoor went out of use. More generally, such security breaches were discussed at length in a RAND Corporation task force report published under DARPA sponsorship by J.P. Anderson and D.J. Edwards in 1970.
While initially targeting the computer vision domain, backdoor attacks have expanded to encompass various other domains, including text, audio, ML-based computer-aided design, and ML-based wireless signal classification. Additionally, vulnerabilities in backdoors have been demonstrated in deep generative models, reinforcement learning (e.g., AI GO), and deep graph models. These broad-ranging potential risks have prompted concerns from national security agencies regarding their potentially disastrous consequences.
A backdoor in a login system might take the form of a hard coded user and password combination which gives access to the system. An example of this sort of backdoor was used as a plot device in the 1983 film WarGames, in which the architect of the "WOPR" computer system had inserted a hardcoded password-less account which gave the user access to the system, and to undocumented parts of the system (in particular, a video game-like simulation mode and direct interaction with the artificial intelligence).
Although the number of backdoors in systems using proprietary software (software whose source code is not publicly available) is not widely credited, they are nevertheless frequently exposed. Programmers have even succeeded in secretly installing large amounts of benign code as Easter eggs in programs, although such cases may involve official forbearance, if not actual permission.

Politics and attribution
There are a number of cloak and dagger considerations that come into play when apportioning responsibility.
Covert backdoors sometimes masquerade as inadvertent defects (bugs) for reasons of plausible deniability. In some cases, these might begin life as an actual bug (inadvertent error), which, once discovered are then deliberately left unfixed and undisclosed, whether by a rogue employee for personal advantage, or with C-level executive awareness and oversight.
It is also possible for an entirely above-board corporation's technology base to be covertly and untraceably tainted by external agents (hackers), though this level of sophistication is thought to exist mainly at the level of nation state actors. For example, if a photomask obtained from a photomask supplier differs in a few gates from its photomask specification, a chip manufacturer would be hard-pressed to detect this if otherwise functionally silent; a covert rootkit running in the photomask etching equipment could enact this discrepancy unbeknown to the photomask manufacturer, either, and by such means, one backdoor potentially leads to another.
In general terms, the long dependency-chains in the modern, highly specialized technological economy and innumerable human-elements process control-points make it difficult to conclusively pinpoint responsibility at such time as a covert backdoor becomes unveiled.
Even direct admissions of responsibility must be scrutinized carefully if the confessing party is beholden to other powerful interests.

Examples
Worms
Many computer worms, such as Sobig and Mydoom, install a backdoor on the affected computer (generally a PC on broadband running Microsoft Windows and Microsoft Outlook). Such backdoors appear to be installed so that spammers can send junk e-mail from the infected machines. Others, such as the Sony/BMG rootkit, placed secretly on millions of music CDs through late 2005, are intended as DRM measures—and, in that case, as data-gathering agents, since both surreptitious programs they installed routinely contacted central servers.
A sophisticated attempt to plant a backdoor in the Linux kernel, exposed in November 2003, added a small and subtle code change by subverting the revision control system. In this case, a two-line change appeared to check root access permissions of a caller to the sys_wait4 function, but because it used assignment = instead of equality checking ==, it actually granted permissions to the system. This difference is easily overlooked, and could even be interpreted as an accidental typographical error, rather than an intentional attack.

In January 2014, a backdoor was discovered in certain Samsung Android products, like the Galaxy devices. The Samsung proprietary Android versions are fitted with a backdoor that provides remote access to the data stored on the device. In particular, the Samsung Android software that is in charge of handling the communications with the modem, using the Samsung IPC protocol, implements a class of requests known as remote file server (RFS) commands, that allows the backdoor operator to perform via modem remote I/O operations on the device hard disk or other storage. As the modem is running Samsung proprietary Android software, it is likely that it offers over-the-air remote control that could then be used to issue the RFS commands and thus to access the file system on the device.

Object code backdoors
Harder to detect backdoors involve modifying object code, rather than source code—object code is much harder to inspect, as it is designed to be machine-readable, not human-readable. These backdoors can be inserted either directly in the on-disk object code, or inserted at some point during compilation, assembly linking, or loading—in the latter case the backdoor never appears on disk, only in memory. Object code backdoors are difficult to detect by inspection of the object code, but are easily detected by simply checking for changes (differences), notably in length or in checksum, and in some cases can be detected or analyzed by disassembling the object code. Further, object code backdoors can be removed (assuming source code is available) by simply recompiling from source on a trusted system.
Thus for such backdoors to avoid detection, all extant copies of a binary must be subverted, and any validation checksums must also be compromised, and source must be unavailable, to prevent recompilation. Alternatively, these other tools (length checks, diff, checksumming, disassemblers) can themselves be compromised to conceal the backdoor, for example detecting that the subverted binary is being checksummed and returning the expected value, not the actual value. To conceal these further subversions, the tools must also conceal the changes in themselves—for example, a subverted checksummer must also detect if it is checksumming itself (or other subverted tools) and return false values. This leads to extensive changes in the system and tools being needed to conceal a single change.
As object code can be regenerated by recompiling (reassembling, relinking) the original source code, making a persistent object code backdoor (without modifying source code) requires subverting the compiler itself—so that when it detects that it is compiling the program under attack it inserts the backdoor—or alternatively the assembler, linker, or loader. As this requires subverting the compiler, this in turn can be fixed by recompiling the compiler, removing the backdoor insertion code. This defense can in turn be subverted by putting a source meta-backdoor in the compiler, so that when it detects that it is compiling itself it then inserts this meta-backdoor generator, together with the original backdoor generator for the original program under attack. After this is done, the source meta-backdoor can be removed, and the compiler recompiled from original source with the compromised compiler executable: the backdoor has been bootstrapped. This attack dates to a 1974 paper by Karger and Schell, and was popularized in Thompson's 1984 article, entitled "Reflections on Trusting Trust"; it is hence colloquially known as the "Trusting Trust" attack. See compiler backdoors, below, for details. Analogous attacks can target lower levels of the system,
such as the operating system, and can be inserted during the system booting process; these are also mentioned by Karger and Schell in 1974, and now exist in the form of boot sector viruses.

Asymmetric backdoors
A traditional backdoor is a symmetric backdoor: anyone that finds the backdoor can in turn use it. The notion of an asymmetric backdoor was introduced by Adam Young and Moti Yung in the Proceedings of Advances in Cryptology – Crypto '96. An asymmetric backdoor can only be used by the attacker who plants it, even if the full implementation of the backdoor becomes public (e.g. via publishing, being discovered and disclosed by reverse engineering, etc.). Also, it is computationally intractable to detect the presence of an asymmetric backdoor under black-box queries. This class of attacks have been termed kleptography; they can be carried out in software, hardware (for example, smartcards), or a combination of the two. The theory of asymmetric backdoors is part of a larger field now called cryptovirology. Notably, NSA inserted a kleptographic backdoor into the Dual EC DRBG standard.
There exists an experimental asymmetric backdoor in RSA key generation. This OpenSSL RSA backdoor, designed by Young and Yung, utilizes a twisted pair of elliptic curves, and has been made available.

Compiler backdoors
A sophisticated form of black box backdoor is a compiler backdoor, where not only is a compiler subverted—to insert a backdoor in some other program, such as a login program—but it is further modified to detect when it is compiling itself and then inserts both the backdoor insertion code (targeting the other program) and the code-modifying self-compilation, like the mechanism through which retroviruses infect their host. This can be done by modifying the source code, and the resulting compromised compiler (object code) can compile the original (unmodified) source code and insert itself: the exploit has been boot-strapped.
This attack was originally presented in Karger & Schell (1974), which was a United States Air Force security analysis of Multics, where they described such an attack on a PL/I compiler, and call it a "compiler trap door". They also mention a variant where the system initialization code is modified to insert a backdoor during booting, as this is complex and poorly understood, and call it an "initialization trapdoor"; this is now known as a boot sector virus.
This attack was then actually implemented by Ken Thompson, and popularized in his Turing Award acceptance speech in 1983, "Reflections on Trusting Trust", which points out that trust is relative, and the only software one can truly trust is code where every step of the bootstrapping has been inspected. This backdoor mechanism is based on the fact that people only review source (human-written) code, and not compiled machine code (object code). A program called a compiler is used to create the second from the first, and the compiler is usually trusted to do an honest job.
Thompson's paper describes a modified version of the Unix C compiler that would put an invisible backdoor in the Unix login command when it noticed that the login program was being compiled, and would also add this feature undetectably to future compiler versions upon their compilation as well. As the compiler itself was a compiled program, users would be extremely unlikely to notice the machine code instructions that performed these tasks. (Because of the second task, the compiler's source code would appear "clean".) What's worse, in Thompson's proof of concept implementation, the subverted compiler also subverted the analysis program (the disassembler), so that anyone who examined the binaries in the usual way would not actually see the real code that was running, but something else instead.
Karger and Schell gave an updated analysis of the original exploit in 2002, and, in 2009, Wheeler wrote a historical overview and survey of the literature. In 2023, Cox published an annotated version of Thompson's backdoor source code.

Occurrences
Thompson's version was, officially, never released into the wild. However, it is believed that a version was distributed to BBN and at least one use of the backdoor was recorded. There are scattered anecdotal reports of such backdoors in subsequent years.
In August 2009, an attack of this kind was discovered by Sophos labs. The W32/Induc-A virus infected the program compiler for Delphi, a Windows programming language. The virus introduced its own code to the compilation of new Delphi programs, allowing it to infect and propagate to many systems, without the knowledge of the software programmer. The virus looks for a Delphi installation, modifies the SysConst.pas file, which is the source code of a part of the standard library and compiles it. After that, every program compiled by that Delphi installation will contain the virus. An attack that propagates by building its own Trojan horse can be especially hard to discover. It resulted in many software vendors releasing infected executables without realizing it, sometimes claiming false positives. After all, the executable was not tampered with, the compiler was. It is believed that the Induc-A virus had been propagating for at least a year before it was discovered.
In 2015, a malicious copy of Xcode, XcodeGhost, also performed a similar attack and infected iOS apps from a dozen of software companies in China. Globally, 4,000 apps were found to be affected. It was not a true Thompson Trojan, as it does not infect development tools themselves, but it did prove that toolchain poisoning can cause substantial damages.

Countermeasures
Once a system has been compromised with a backdoor or Trojan horse, such as the Trusting Trust compiler, it is very hard for the "rightful" user to regain control of the system – typically one should rebuild a clean system and transfer data (but not executables) over. However, several practical weaknesses in the Trusting Trust scheme have been suggested.  For example, a sufficiently motivated user could painstakingly review the machine code of the untrusted compiler before using it. As mentioned above, there are ways to hide the Trojan horse, such as subverting the disassembler; but there are ways to counter that defense, too, such as writing a disassembler from scratch.
A generic method to counter trusting trust attacks is called diverse double-compiling. The method requires a different compiler and the source code of the compiler-under-test. That source, compiled with both compilers, results in two different stage-1 compilers, which however should have the same behavior. Thus the same source compiled with both stage-1 compilers must then result in two identical stage-2 compilers. A formal proof is given that the latter comparison guarantees that the purported source code and executable of the compiler-under-test correspond, under some assumptions. This method was applied by its author to verify that the C compiler of the GCC suite (v. 3.0.4) contained no trojan, using icc (v. 11.0) as the different compiler.
In practice such verifications are not done by end users, except in extreme circumstances of intrusion detection and analysis, due to the rarity of such sophisticated attacks, and because programs are typically distributed in binary form. Removing backdoors (including compiler backdoors) is typically done by simply rebuilding a clean system. However, the sophisticated verifications are of interest to operating system vendors, to ensure that they are not distributing a compromised system, and in high-security settings, where such attacks are a realistic concern.

List of known backdoors
Back Orifice was created in 1998 by hackers from Cult of the Dead Cow group as a remote administration tool. It allowed Windows computers to be remotely controlled over a network and parodied the name of Microsoft's BackOffice.
The Dual EC DRBG cryptographically secure pseudorandom number generator was revealed in 2013 to possibly have a kleptographic backdoor deliberately inserted by NSA, who also had the private key to the backdoor.
Several backdoors in the unlicensed copies of WordPress plug-ins were discovered in March 2014. They were inserted as obfuscated JavaScript code and silently created, for example, an admin account in the website database. A similar scheme was later exposed in a Joomla plugin.
Borland Interbase versions 4.0 through 6.0 had a hard-coded backdoor, put there by the developers. The server code contains a compiled-in backdoor account (username: politically, password: correct), which could be accessed over a network connection; a user logging in with this backdoor account could take full control over all Interbase databases. The backdoor was detected in 2001 and a patch was released.
Juniper Networks backdoor inserted in the year 2008 into the versions of firmware ScreenOS from 6.2.0r15 to 6.2.0r18 and from 6.3.0r12 to 6.3.0r20 that gives any user administrative access when using a special master password.
Several backdoors were discovered in C-DATA Optical Line Termination (OLT) devices. Researchers released the findings without notifying C-DATA because they believe the backdoors were intentionally placed by the vendor.
A backdoor in versions 5.6.0 and 5.6.1 of the popular Linux utility XZ Utils was discovered in March 2024 by software developer Andres Freund. The backdoor gives an attacker who possesses a specific Ed448 private key remote code execution capabilities on the affected Linux systems. The issue has been assigned a CVSS score of 10.0, the highest possible score.

See also
Backdoor:Win32.Hupigon
Hardware backdoor
Titanium (malware)

Notes
References
External links
Finding and Removing Backdoors
Three Archaic Backdoor Trojan Programs That Still Serve Great Pranks Archived 2015-05-27 at the Wayback Machine
Backdoors removal — List of backdoors and their removal instructions.
FAQ Farm's Backdoors FAQ: wiki question and answer forum
List of backdoors and Removal
In machine learning, backpropagation is a gradient estimation method commonly used for training neural networks to compute the network parameter updates.
It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.
Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adam. 
Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include "reverse mode of automatic differentiation" or "reverse accumulation".

Overview
Backpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:

  
    
      
        x
      
    
    {\displaystyle x}
  
: input (vector of features)

  
    
      
        y
      
    
    {\displaystyle y}
  
: target output
For classification, output will be a vector of class probabilities (e.g., 
  
    
      
        (
        0.1
        ,
        0.7
        ,
        0.2
        )
      
    
    {\displaystyle (0.1,0.7,0.2)}
  
, and target output is a specific class, encoded by the one-hot/dummy variable (e.g., 
  
    
      
        (
        0
        ,
        1
        ,
        0
        )
      
    
    {\displaystyle (0,1,0)}
  
).

  
    
      
        C
      
    
    {\displaystyle C}
  
: loss function or "cost function"
For classification, this is usually cross-entropy (XC, log loss), while for regression it is usually squared error loss (SEL).

  
    
      
        L
      
    
    {\displaystyle L}
  
: the number of layers

  
    
      
        
          W
          
            l
          
        
        =
        (
        
          w
          
            j
            k
          
          
            l
          
        
        )
      
    
    {\displaystyle W^{l}=(w_{jk}^{l})}
  
: the weights between layer 
  
    
      
        l
        −
        1
      
    
    {\displaystyle l-1}
  
 and 
  
    
      
        l
      
    
    {\displaystyle l}
  
, where 
  
    
      
        
          w
          
            j
            k
          
          
            l
          
        
      
    
    {\displaystyle w_{jk}^{l}}
  
 is the weight between the 
  
    
      
        k
      
    
    {\displaystyle k}
  
-th node in layer 
  
    
      
        l
        −
        1
      
    
    {\displaystyle l-1}
  
 and the 
  
    
      
        j
      
    
    {\displaystyle j}
  
-th node in layer 
  
    
      
        l
      
    
    {\displaystyle l}
  

  
    
      
        
          f
          
            l
          
        
      
    
    {\displaystyle f^{l}}
  
: activation functions at layer 
  
    
      
        l
      
    
    {\displaystyle l}
  

For classification the last layer is usually the logistic function for binary classification, and softmax (softargmax) for multi-class classification, while for the hidden layers this was traditionally a sigmoid function (logistic function or others) on each node (coordinate), but today is more varied, with rectifier (ramp, ReLU) being common.

  
    
      
        
          a
          
            j
          
          
            l
          
        
      
    
    {\displaystyle a_{j}^{l}}
  
: activation of the 
  
    
      
        j
      
    
    {\displaystyle j}
  
-th node in layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
.
In the derivation of backpropagation, other intermediate quantities are used by introducing them as needed below. Bias terms are not treated specially since they correspond to a weight with a fixed input of 1. For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include sigmoid, tanh, and ReLU. swish mish, and other activation functions have since been proposed as well.
The overall network is a combination of function composition and matrix multiplication:

  
    
      
        g
        (
        x
        )
        :=
        
          f
          
            L
          
        
        (
        
          W
          
            L
          
        
        
          f
          
            L
            −
            1
          
        
        (
        
          W
          
            L
            −
            1
          
        
        ⋯
        
          f
          
            1
          
        
        (
        
          W
          
            1
          
        
        x
        )
        ⋯
        )
        )
      
    
    {\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))}
  

For a training set there will be a set of input–output pairs, 
  
    
      
        
          {
          
            (
            
              x
              
                i
              
            
            ,
            
              y
              
                i
              
            
            )
          
          }
        
      
    
    {\displaystyle \left\{(x_{i},y_{i})\right\}}
  
. For each input–output pair 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},y_{i})}
  
 in the training set, the loss of the model on that pair is the cost of the difference between the predicted output 
  
    
      
        g
        (
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle g(x_{i})}
  
 and the target output 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
:

  
    
      
        C
        (
        
          y
          
            i
          
        
        ,
        g
        (
        
          x
          
            i
          
        
        )
        )
      
    
    {\displaystyle C(y_{i},g(x_{i}))}
  

Note the distinction: during model evaluation the weights are fixed while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training the input–output pair is fixed while the weights vary, and the network ends with the loss function.
Backpropagation computes the gradient for a fixed input–output pair 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},y_{i})}
  
, where the weights 
  
    
      
        
          w
          
            j
            k
          
          
            l
          
        
      
    
    {\displaystyle w_{jk}^{l}}
  
 can vary. Each individual component of the gradient, 
  
    
      
        ∂
        C
        
          /
        
        ∂
        
          w
          
            j
            k
          
          
            l
          
        
        ,
      
    
    {\displaystyle \partial C/\partial w_{jk}^{l},}
  
 can be computed by the chain rule; but doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer – specifically the gradient of the weighted input of each layer, denoted by 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 – from back to front.
Informally, the key point is that since the only way a weight in 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W^{l}}
  
 affects the loss is through its effect on the next layer, and it does so linearly, 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 are the only data you need to compute the gradients of the weights at layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
, and then the previous layer can be computed 
  
    
      
        
          δ
          
            l
            −
            1
          
        
      
    
    {\displaystyle \delta ^{l-1}}
  
 and repeated recursively. This avoids inefficiency in two ways. First, it avoids duplication because when computing the gradient at layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
, it is unnecessary to recompute all derivatives on later layers 
  
    
      
        l
        +
        1
        ,
        l
        +
        2
        ,
        …
      
    
    {\displaystyle l+1,l+2,\ldots }
  
 each time. Second, it avoids unnecessary intermediate calculations, because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights 
  
    
      
        ∂
        
          a
          
            
              j
              ′
            
          
          
            
              l
              ′
            
          
        
        
          /
        
        ∂
        
          w
          
            j
            k
          
          
            l
          
        
      
    
    {\displaystyle \partial a_{j'}^{l'}/\partial w_{jk}^{l}}
  
.
Backpropagation can be expressed for simple feedforward networks in terms of matrix multiplication, or more generally in terms of the adjoint graph.

Matrix multiplication
For the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication. Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left – "backwards" – with the gradient of the weights between each layer being a simple modification of the partial products (the "backwards propagated error").
Given an input–output pair 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
  
, the loss is:

  
    
      
        C
        (
        y
        ,
        
          f
          
            L
          
        
        (
        
          W
          
            L
          
        
        
          f
          
            L
            −
            1
          
        
        (
        
          W
          
            L
            −
            1
          
        
        ⋯
        
          f
          
            2
          
        
        (
        
          W
          
            2
          
        
        
          f
          
            1
          
        
        (
        
          W
          
            1
          
        
        x
        )
        )
        ⋯
        )
        )
        )
      
    
    {\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{2}(W^{2}f^{1}(W^{1}x))\cdots )))}
  

To compute this, one starts with the input 
  
    
      
        x
      
    
    {\displaystyle x}
  
 and works forward; denote the weighted input of each hidden layer as 
  
    
      
        
          z
          
            l
          
        
      
    
    {\displaystyle z^{l}}
  
 and the output of hidden layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
 as the activation 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a^{l}}
  
. For backpropagation, the activation 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a^{l}}
  
 as well as the derivatives 
  
    
      
        (
        
          f
          
            l
          
        
        
          )
          ′
        
      
    
    {\displaystyle (f^{l})'}
  
 (evaluated at 
  
    
      
        
          z
          
            l
          
        
      
    
    {\displaystyle z^{l}}
  
) must be cached for use during the backwards pass.
The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input 
  
    
      
        x
      
    
    {\displaystyle x}
  
:

  
    
      
        
          
            
              d
              C
            
            
              d
              
                a
                
                  L
                
              
            
          
        
        ⋅
        
          
            
              d
              
                a
                
                  L
                
              
            
            
              d
              
                z
                
                  L
                
              
            
          
        
        ⋅
        
          
            
              d
              
                z
                
                  L
                
              
            
            
              d
              
                a
                
                  L
                  −
                  1
                
              
            
          
        
        ⋅
        
          
            
              d
              
                a
                
                  L
                  −
                  1
                
              
            
            
              d
              
                z
                
                  L
                  −
                  1
                
              
            
          
        
        ⋅
        
          
            
              d
              
                z
                
                  L
                  −
                  1
                
              
            
            
              d
              
                a
                
                  L
                  −
                  2
                
              
            
          
        
        ⋅
        …
        ⋅
        
          
            
              d
              
                a
                
                  1
                
              
            
            
              d
              
                z
                
                  1
                
              
            
          
        
        ⋅
        
          
            
              ∂
              
                z
                
                  1
                
              
            
            
              ∂
              x
            
          
        
        ,
      
    
    {\displaystyle {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdot \ldots \cdot {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}},}
  

where 
  
    
      
        
          
            
              d
              
                a
                
                  L
                
              
            
            
              d
              
                z
                
                  L
                
              
            
          
        
      
    
    {\displaystyle {\frac {da^{L}}{dz^{L}}}}
  
 is a diagonal matrix.
These terms are: the derivative of the loss function; the derivatives of the activation functions; and the matrices of weights:

  
    
      
        
          
            
              d
              C
            
            
              d
              
                a
                
                  L
                
              
            
          
        
        ∘
        (
        
          f
          
            L
          
        
        
          )
          ′
        
        ⋅
        
          W
          
            L
          
        
        ∘
        (
        
          f
          
            L
            −
            1
          
        
        
          )
          ′
        
        ⋅
        
          W
          
            L
            −
            1
          
        
        ∘
        ⋯
        ∘
        (
        
          f
          
            1
          
        
        
          )
          ′
        
        ⋅
        
          W
          
            1
          
        
        .
      
    
    {\displaystyle {\frac {dC}{da^{L}}}\circ (f^{L})'\cdot W^{L}\circ (f^{L-1})'\cdot W^{L-1}\circ \cdots \circ (f^{1})'\cdot W^{1}.}
  

The gradient 
  
    
      
        ∇
      
    
    {\displaystyle \nabla }
  
 is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:

  
    
      
        
          ∇
          
            x
          
        
        C
        =
        (
        
          W
          
            1
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            1
          
        
        
          )
          ′
        
        ∘
        …
        ∘
        (
        
          W
          
            L
            −
            1
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            L
            −
            1
          
        
        
          )
          ′
        
        ∘
        (
        
          W
          
            L
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            L
          
        
        
          )
          ′
        
        ∘
        
          ∇
          
            
              a
              
                L
              
            
          
        
        C
        .
      
    
    {\displaystyle \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\circ \ldots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}
  

Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights is not just a subexpression: there's an extra multiplication.
Introducing the auxiliary quantity 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 for the partial products (multiplying from right to left), interpreted as the "error at level 
  
    
      
        l
      
    
    {\displaystyle l}
  
" and defined as the gradient of the input values at level 
  
    
      
        l
      
    
    {\displaystyle l}
  
:

  
    
      
        
          δ
          
            l
          
        
        :=
        (
        
          f
          
            l
          
        
        
          )
          ′
        
        ∘
        (
        
          W
          
            l
            +
            1
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            l
            +
            1
          
        
        
          )
          ′
        
        ∘
        ⋯
        ∘
        (
        
          W
          
            L
            −
            1
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            L
            −
            1
          
        
        
          )
          ′
        
        ∘
        (
        
          W
          
            L
          
        
        
          )
          
            T
          
        
        ⋅
        (
        
          f
          
            L
          
        
        
          )
          ′
        
        ∘
        
          ∇
          
            
              a
              
                L
              
            
          
        
        C
        .
      
    
    {\displaystyle \delta ^{l}:=(f^{l})'\circ (W^{l+1})^{T}\cdot (f^{l+1})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}
  

Note that 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 is a vector, of length equal to the number of nodes in level 
  
    
      
        l
      
    
    {\displaystyle l}
  
; each component is interpreted as the "cost attributable to (the value of) that node".
The gradient of the weights in layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
 is then:

  
    
      
        
          ∇
          
            
              W
              
                l
              
            
          
        
        C
        =
        
          δ
          
            l
          
        
        (
        
          a
          
            l
            −
            1
          
        
        
          )
          
            T
          
        
        .
      
    
    {\displaystyle \nabla _{W^{l}}C=\delta ^{l}(a^{l-1})^{T}.}
  

The factor of 
  
    
      
        
          a
          
            l
            −
            1
          
        
      
    
    {\displaystyle a^{l-1}}
  
 is because the weights 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W^{l}}
  
 between level 
  
    
      
        l
        −
        1
      
    
    {\displaystyle l-1}
  
 and 
  
    
      
        l
      
    
    {\displaystyle l}
  
 affect level 
  
    
      
        l
      
    
    {\displaystyle l}
  
 proportionally to the inputs (activations): the inputs are fixed, the weights vary.
The 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 can easily be computed recursively, going from right to left, as:

  
    
      
        
          δ
          
            l
            −
            1
          
        
        :=
        (
        
          f
          
            l
            −
            1
          
        
        
          )
          ′
        
        ∘
        (
        
          W
          
            l
          
        
        
          )
          
            T
          
        
        ⋅
        
          δ
          
            l
          
        
        .
      
    
    {\displaystyle \delta ^{l-1}:=(f^{l-1})'\circ (W^{l})^{T}\cdot \delta ^{l}.}
  

The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.
Compared with naively computing forwards (using the 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 for illustration):

  
    
      
        
          
            
              
                
                  δ
                  
                    1
                  
                
              
              
                
                =
                (
                
                  f
                  
                    1
                  
                
                
                  )
                  ′
                
                ∘
                (
                
                  W
                  
                    2
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    2
                  
                
                
                  )
                  ′
                
                ∘
                ⋯
                ∘
                (
                
                  W
                  
                    L
                    −
                    1
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    L
                    −
                    1
                  
                
                
                  )
                  ′
                
                ∘
                (
                
                  W
                  
                    L
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    L
                  
                
                
                  )
                  ′
                
                ∘
                
                  ∇
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
                
                  δ
                  
                    2
                  
                
              
              
                
                =
                (
                
                  f
                  
                    2
                  
                
                
                  )
                  ′
                
                ∘
                ⋯
                ∘
                (
                
                  W
                  
                    L
                    −
                    1
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    L
                    −
                    1
                  
                
                
                  )
                  ′
                
                ∘
                (
                
                  W
                  
                    L
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    L
                  
                
                
                  )
                  ′
                
                ∘
                
                  ∇
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
              
                
                ⋮
              
            
            
              
                
                  δ
                  
                    L
                    −
                    1
                  
                
              
              
                
                =
                (
                
                  f
                  
                    L
                    −
                    1
                  
                
                
                  )
                  ′
                
                ∘
                (
                
                  W
                  
                    L
                  
                
                
                  )
                  
                    T
                  
                
                ⋅
                (
                
                  f
                  
                    L
                  
                
                
                  )
                  ′
                
                ∘
                
                  ∇
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
                
                  δ
                  
                    L
                  
                
              
              
                
                =
                (
                
                  f
                  
                    L
                  
                
                
                  )
                  ′
                
                ∘
                
                  ∇
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\delta ^{1}&=(f^{1})'\circ (W^{2})^{T}\cdot (f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{2}&=(f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\&\vdots \\\delta ^{L-1}&=(f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{L}&=(f^{L})'\circ \nabla _{a^{L}}C,\end{aligned}}}
  

There are two key differences with backpropagation:

Computing 
  
    
      
        
          δ
          
            l
            −
            1
          
        
      
    
    {\displaystyle \delta ^{l-1}}
  
 in terms of 
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
 avoids the obvious duplicate multiplication of layers 
  
    
      
        l
      
    
    {\displaystyle l}
  
 and beyond.
Multiplying starting from 
  
    
      
        
          ∇
          
            
              a
              
                L
              
            
          
        
        C
      
    
    {\displaystyle \nabla _{a^{L}}C}
  
 – propagating the error backwards – means that each step simply multiplies a vector (
  
    
      
        
          δ
          
            l
          
        
      
    
    {\displaystyle \delta ^{l}}
  
) by the matrices of weights 
  
    
      
        (
        
          W
          
            l
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle (W^{l})^{T}}
  
 and derivatives of activations 
  
    
      
        (
        
          f
          
            l
            −
            1
          
        
        
          )
          ′
        
      
    
    {\displaystyle (f^{l-1})'}
  
. By contrast, multiplying forwards, starting from the changes at an earlier layer, means that each multiplication multiplies a matrix by a matrix. This is much more expensive, and corresponds to tracking every possible path of a change in one layer 
  
    
      
        l
      
    
    {\displaystyle l}
  
 forward to changes in the layer 
  
    
      
        l
        +
        2
      
    
    {\displaystyle l+2}
  
 (for multiplying 
  
    
      
        
          W
          
            l
            +
            1
          
        
      
    
    {\displaystyle W^{l+1}}
  
 by 
  
    
      
        
          W
          
            l
            +
            2
          
        
      
    
    {\displaystyle W^{l+2}}
  
, with additional multiplications for the derivatives of the activations), which unnecessarily computes the intermediate quantities of how weight changes affect the values of hidden nodes.

Adjoint graph
For more general graphs, and other advanced variations, backpropagation can be understood in terms of automatic differentiation, where backpropagation is a special case of reverse accumulation (or "reverse mode").

Intuition
Motivation
The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.

Learning as an optimization problem
To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input. 
Initially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consist of a set of tuples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        t
        )
      
    
    {\displaystyle (x_{1},x_{2},t)}
  
 where 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  
 and 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  
 are the inputs to the network and t is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  
 and 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  
, will compute an output y that likely differs from t (given random weights). A loss function 
  
    
      
        L
        (
        t
        ,
        y
        )
      
    
    {\displaystyle L(t,y)}
  
 is used for measuring the discrepancy between the target output t and the computed output y. For regression analysis problems the squared error can be used as a loss function, for classification the categorical cross-entropy can be used.
As an example consider a regression problem using the square error as a loss:

  
    
      
        L
        (
        t
        ,
        y
        )
        =
        (
        t
        −
        y
        
          )
          
            2
          
        
        =
        E
        ,
      
    
    {\displaystyle L(t,y)=(t-y)^{2}=E,}
  

where E is the discrepancy or error.

Consider the network on a single training case: 
  
    
      
        (
        1
        ,
        1
        ,
        0
        )
      
    
    {\displaystyle (1,1,0)}
  
. Thus, the input 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  
 and 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  
 are 1 and 1 respectively and the correct output, t is 0. Now if the relation is plotted between the network's output y on the horizontal axis and the error E on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output y which minimizes the error E. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output y that exactly matches the target output t. Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error. 
However, the output of a neuron depends on the weighted sum of all its inputs:

  
    
      
        y
        =
        
          x
          
            1
          
        
        
          w
          
            1
          
        
        +
        
          x
          
            2
          
        
        
          w
          
            2
          
        
        ,
      
    
    {\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}
  

where 
  
    
      
        
          w
          
            1
          
        
      
    
    {\displaystyle w_{1}}
  
 and 
  
    
      
        
          w
          
            2
          
        
      
    
    {\displaystyle w_{2}}
  
 are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.
In this example, upon injecting the training data 
  
    
      
        (
        1
        ,
        1
        ,
        0
        )
      
    
    {\displaystyle (1,1,0)}
  
, the loss function becomes

  
    
      
        E
        =
        (
        t
        −
        y
        
          )
          
            2
          
        
        =
        
          y
          
            2
          
        
        =
        (
        
          x
          
            1
          
        
        
          w
          
            1
          
        
        +
        
          x
          
            2
          
        
        
          w
          
            2
          
        
        
          )
          
            2
          
        
        =
        (
        
          w
          
            1
          
        
        +
        
          w
          
            2
          
        
        
          )
          
            2
          
        
        .
      
    
    {\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}
  

Then, the loss function 
  
    
      
        E
      
    
    {\displaystyle E}
  
 takes the form of a parabolic cylinder with its base directed along 
  
    
      
        
          w
          
            1
          
        
        =
        −
        
          w
          
            2
          
        
      
    
    {\displaystyle w_{1}=-w_{2}}
  
. Since all sets of weights that satisfy 
  
    
      
        
          w
          
            1
          
        
        =
        −
        
          w
          
            2
          
        
      
    
    {\displaystyle w_{1}=-w_{2}}
  
 minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.
One commonly used algorithm to find the set of weights that minimizes the error is gradient descent. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.

Derivation
The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared error function is

  
    
      
        E
        =
        L
        (
        t
        ,
        y
        )
      
    
    {\displaystyle E=L(t,y)}
  

where

  
    
      
        L
      
    
    {\displaystyle L}
  
 is the loss for the output 
  
    
      
        y
      
    
    {\displaystyle y}
  
 and target value 
  
    
      
        t
      
    
    {\displaystyle t}
  
,

  
    
      
        t
      
    
    {\displaystyle t}
  
 is the target output for a training sample, and

  
    
      
        y
      
    
    {\displaystyle y}
  
 is the actual output of the output neuron.
For each neuron 
  
    
      
        j
      
    
    {\displaystyle j}
  
, its output 
  
    
      
        
          o
          
            j
          
        
      
    
    {\displaystyle o_{j}}
  
 is defined as

  
    
      
        
          o
          
            j
          
        
        =
        φ
        (
        
          
            net
          
          
            j
          
        
        )
        =
        φ
        
          (
          
            
              ∑
              
                k
                =
                1
              
              
                n
              
            
            
              w
              
                k
                j
              
            
            
              x
              
                k
              
            
          
          )
        
        ,
      
    
    {\displaystyle o_{j}=\varphi ({\text{net}}_{j})=\varphi \left(\sum _{k=1}^{n}w_{kj}x_{k}\right),}
  

where the activation function 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 is non-linear and differentiable over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the logistic function:

  
    
      
        φ
        (
        z
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  z
                
              
            
          
        
      
    
    {\displaystyle \varphi (z)={\frac {1}{1+e^{-z}}}}
  

which has a convenient derivative of:

  
    
      
        
          
            
              d
              φ
            
            
              d
              z
            
          
        
        =
        φ
        (
        z
        )
        (
        1
        −
        φ
        (
        z
        )
        )
      
    
    {\displaystyle {\frac {d\varphi }{dz}}=\varphi (z)(1-\varphi (z))}
  

The input 
  
    
      
        
          
            net
          
          
            j
          
        
      
    
    {\displaystyle {\text{net}}_{j}}
  
 to a neuron is the weighted sum of outputs 
  
    
      
        
          o
          
            k
          
        
      
    
    {\displaystyle o_{k}}
  
 of previous neurons. If the neuron is in the first layer after the input layer, the 
  
    
      
        
          o
          
            k
          
        
      
    
    {\displaystyle o_{k}}
  
 of the input layer are simply the inputs 
  
    
      
        
          x
          
            k
          
        
      
    
    {\displaystyle x_{k}}
  
 to the network. The number of input units to the neuron is 
  
    
      
        n
      
    
    {\displaystyle n}
  
. The variable 
  
    
      
        
          w
          
            k
            j
          
        
      
    
    {\displaystyle w_{kj}}
  
 denotes the weight between neuron 
  
    
      
        k
      
    
    {\displaystyle k}
  
 of the previous layer and neuron 
  
    
      
        j
      
    
    {\displaystyle j}
  
 of the current layer.

Finding the derivative of the error
Calculating the partial derivative of the error with respect to a weight 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 is done using the chain rule twice:

In the last factor of the right-hand side of the above, only one term in the sum 
  
    
      
        
          
            net
          
          
            j
          
        
      
    
    {\displaystyle {\text{net}}_{j}}
  
 depends on 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
, so that

If the neuron is in the first layer after the input layer, 
  
    
      
        
          o
          
            i
          
        
      
    
    {\displaystyle o_{i}}
  
 is just 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
.
The derivative of the output of neuron 
  
    
      
        j
      
    
    {\displaystyle j}
  
 with respect to its input is simply the partial derivative of the activation function:

which for the logistic activation function 

  
    
      
        
          
            
              ∂
              
                o
                
                  j
                
              
            
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        =
        
          
            ∂
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        φ
        (
        
          
            net
          
          
            j
          
        
        )
        =
        φ
        (
        
          
            net
          
          
            j
          
        
        )
        (
        1
        −
        φ
        (
        
          
            net
          
          
            j
          
        
        )
        )
        =
        
          o
          
            j
          
        
        (
        1
        −
        
          o
          
            j
          
        
        )
      
    
    {\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial }{\partial {\text{net}}_{j}}}\varphi ({\text{net}}_{j})=\varphi ({\text{net}}_{j})(1-\varphi ({\text{net}}_{j}))=o_{j}(1-o_{j})}
  

This is the reason why backpropagation requires that the activation function be differentiable. (Nevertheless, the ReLU activation function, which is non-differentiable at 0, has become quite popular, e.g. in AlexNet)
The first factor is straightforward to evaluate if the neuron is in the output layer, because then 
  
    
      
        
          o
          
            j
          
        
        =
        y
      
    
    {\displaystyle o_{j}=y}
  
 and

If half of the square error is used as loss function we can rewrite it as

  
    
      
        
          
            
              ∂
              E
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        =
        
          
            
              ∂
              E
            
            
              ∂
              y
            
          
        
        =
        
          
            ∂
            
              ∂
              y
            
          
        
        
          
            1
            2
          
        
        (
        t
        −
        y
        
          )
          
            2
          
        
        =
        y
        −
        t
      
    
    {\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}={\frac {\partial }{\partial y}}{\frac {1}{2}}(t-y)^{2}=y-t}
  

However, if 
  
    
      
        j
      
    
    {\displaystyle j}
  
 is in an arbitrary inner layer of the network, finding the derivative 
  
    
      
        E
      
    
    {\displaystyle E}
  
 with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
    {\displaystyle o_{j}}
  
 is less obvious.
Considering 
  
    
      
        E
      
    
    {\displaystyle E}
  
 as a function with the inputs being all neurons 
  
    
      
        L
        =
        {
        u
        ,
        v
        ,
        …
        ,
        w
        }
      
    
    {\displaystyle L=\{u,v,\dots ,w\}}
  
 receiving input from neuron 
  
    
      
        j
      
    
    {\displaystyle j}
  
,

  
    
      
        
          
            
              ∂
              E
              (
              
                o
                
                  j
                
              
              )
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        =
        
          
            
              ∂
              E
              (
              
                
                  n
                  e
                  t
                
                
                  u
                
              
              ,
              
                
                  net
                
                
                  v
                
              
              ,
              …
              ,
              
                
                  n
                  e
                  t
                
                
                  w
                
              
              )
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial E(o_{j})}{\partial o_{j}}}={\frac {\partial E(\mathrm {net} _{u},{\text{net}}_{v},\dots ,\mathrm {net} _{w})}{\partial o_{j}}}}
  

and taking the total derivative with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
    {\displaystyle o_{j}}
  
, a recursive expression for the derivative is obtained:

Therefore, the derivative with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
    {\displaystyle o_{j}}
  
 can be calculated if all the derivatives with respect to the outputs 
  
    
      
        
          o
          
            ℓ
          
        
      
    
    {\displaystyle o_{\ell }}
  
 of the next layer – the ones closer to the output neuron – are known. [Note, if any of the neurons in set 
  
    
      
        L
      
    
    {\displaystyle L}
  
 were not connected to neuron 
  
    
      
        j
      
    
    {\displaystyle j}
  
, they would be independent of 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 and the corresponding partial derivative under the summation would vanish to 0.]
Substituting Eq. 2, Eq. 3 Eq.4 and Eq. 5 in Eq. 1 we obtain:

  
    
      
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        =
        
          
            
              ∂
              E
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        
          
            
              ∂
              
                o
                
                  j
                
              
            
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        
          
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        =
        
          
            
              ∂
              E
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        
          
            
              ∂
              
                o
                
                  j
                
              
            
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        
          o
          
            i
          
        
      
    
    {\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}o_{i}}
  

  
    
      
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        =
        
          o
          
            i
          
        
        
          δ
          
            j
          
        
      
    
    {\displaystyle {\frac {\partial E}{\partial w_{ij}}}=o_{i}\delta _{j}}
  

with

  
    
      
        
          δ
          
            j
          
        
        =
        
          
            
              ∂
              E
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        
          
            
              ∂
              
                o
                
                  j
                
              
            
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        =
        
          
            {
            
              
                
                  
                    
                      
                        ∂
                        L
                        (
                        t
                        ,
                        
                          o
                          
                            j
                          
                        
                        )
                      
                      
                        ∂
                        
                          o
                          
                            j
                          
                        
                      
                    
                  
                  
                    
                      
                        d
                        φ
                        (
                        
                          
                            net
                          
                          
                            j
                          
                        
                        )
                      
                      
                        d
                        
                          
                            net
                          
                          
                            j
                          
                        
                      
                    
                  
                
                
                  
                    if 
                  
                  j
                  
                     is an output neuron,
                  
                
              
              
                
                  (
                  
                    ∑
                    
                      ℓ
                      ∈
                      L
                    
                  
                  
                    w
                    
                      j
                      ℓ
                    
                  
                  
                    δ
                    
                      ℓ
                    
                  
                  )
                  
                    
                      
                        d
                        φ
                        (
                        
                          
                            net
                          
                          
                            j
                          
                        
                        )
                      
                      
                        d
                        
                          
                            net
                          
                          
                            j
                          
                        
                      
                    
                  
                
                
                  
                    if 
                  
                  j
                  
                     is an inner neuron.
                  
                
              
            
            
          
        
      
    
    {\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}{\frac {\partial L(t,o_{j})}{\partial o_{j}}}{\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell }){\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}
  

if 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 is the logistic function, and the error is the square error:

  
    
      
        
          δ
          
            j
          
        
        =
        
          
            
              ∂
              E
            
            
              ∂
              
                o
                
                  j
                
              
            
          
        
        
          
            
              ∂
              
                o
                
                  j
                
              
            
            
              ∂
              
                
                  net
                
                
                  j
                
              
            
          
        
        =
        
          
            {
            
              
                
                  (
                  
                    o
                    
                      j
                    
                  
                  −
                  
                    t
                    
                      j
                    
                  
                  )
                  
                    o
                    
                      j
                    
                  
                  (
                  1
                  −
                  
                    o
                    
                      j
                    
                  
                  )
                
                
                  
                    if 
                  
                  j
                  
                     is an output neuron,
                  
                
              
              
                
                  (
                  
                    ∑
                    
                      ℓ
                      ∈
                      L
                    
                  
                  
                    w
                    
                      j
                      ℓ
                    
                  
                  
                    δ
                    
                      ℓ
                    
                  
                  )
                  
                    o
                    
                      j
                    
                  
                  (
                  1
                  −
                  
                    o
                    
                      j
                    
                  
                  )
                
                
                  
                    if 
                  
                  j
                  
                     is an inner neuron.
                  
                
              
            
            
          
        
      
    
    {\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell })o_{j}(1-o_{j})&{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}
  

To update the weight 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 using gradient descent, one must choose a learning rate, 
  
    
      
        η
        >
        0
      
    
    {\displaystyle \eta >0}
  
. The change in weight needs to reflect the impact on 
  
    
      
        E
      
    
    {\displaystyle E}
  
 of an increase or decrease in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
. If 
  
    
      
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        >
        0
      
    
    {\displaystyle {\frac {\partial E}{\partial w_{ij}}}>0}
  
, an increase in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 increases 
  
    
      
        E
      
    
    {\displaystyle E}
  
; conversely, if 
  
    
      
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        <
        0
      
    
    {\displaystyle {\frac {\partial E}{\partial w_{ij}}}<0}
  
, an increase in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 decreases 
  
    
      
        E
      
    
    {\displaystyle E}
  
. The new 
  
    
      
        Δ
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle \Delta w_{ij}}
  
 is added to the old weight, and the product of the learning rate and the gradient, multiplied by 
  
    
      
        −
        1
      
    
    {\displaystyle -1}
  
 guarantees that 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 changes in a way that always decreases 
  
    
      
        E
      
    
    {\displaystyle E}
  
. In other words, in the equation immediately below, 
  
    
      
        −
        η
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
      
    
    {\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}}
  
 always changes 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
  
 in such a way that 
  
    
      
        E
      
    
    {\displaystyle E}
  
 is decreased:

  
    
      
        Δ
        
          w
          
            i
            j
          
        
        =
        −
        η
        
          
            
              ∂
              E
            
            
              ∂
              
                w
                
                  i
                  j
                
              
            
          
        
        =
        −
        η
        
          o
          
            i
          
        
        
          δ
          
            j
          
        
      
    
    {\displaystyle \Delta w_{ij}=-\eta {\frac {\partial E}{\partial w_{ij}}}=-\eta o_{i}\delta _{j}}

Second-order gradient descent
Using a Hessian matrix of second-order derivatives of the error function, the Levenberg–Marquardt algorithm often converges faster than first-order gradient descent, especially when the topology of the error function is complicated. It may also find solutions in smaller node counts for which other methods might not converge. The Hessian can be approximated by the Fisher information matrix.

Loss function
The loss function is a function that maps values of one or more variables onto a real number intuitively representing some "cost" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.

Assumptions
The mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation. The first is that it can be written as an average 
  
    
      
        E
        =
        
          
            1
            n
          
        
        
          ∑
          
            x
          
        
        
          E
          
            x
          
        
      
    
    {\textstyle E={\frac {1}{n}}\sum _{x}E_{x}}
  
 over error functions 
  
    
      
        
          E
          
            x
          
        
      
    
    {\textstyle E_{x}}
  
, for 
  
    
      
        n
      
    
    {\textstyle n}
  
 individual training examples, 
  
    
      
        x
      
    
    {\textstyle x}
  
. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.

Example loss function
Let 
  
    
      
        y
        ,
        
          y
          ′
        
      
    
    {\displaystyle y,y'}
  
 be vectors in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  
.
Select an error function 
  
    
      
        E
        (
        y
        ,
        
          y
          ′
        
        )
      
    
    {\displaystyle E(y,y')}
  
 measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors 
  
    
      
        y
      
    
    {\displaystyle y}
  
 and 
  
    
      
        
          y
          ′
        
      
    
    {\displaystyle y'}
  
:
  
    
      
        E
        (
        y
        ,
        
          y
          ′
        
        )
        =
        
          
            
              1
              2
            
          
        
        ‖
        y
        −
        
          y
          ′
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle E(y,y')={\tfrac {1}{2}}\lVert y-y'\rVert ^{2}}
  
The error function over 
  
    
      
        n
      
    
    {\textstyle n}
  
 training examples can then be written as an average of losses over individual examples:
  
    
      
        E
        =
        
          
            1
            
              2
              n
            
          
        
        
          ∑
          
            x
          
        
        ‖
        (
        y
        (
        x
        )
        −
        
          y
          ′
        
        (
        x
        )
        )
        
          ‖
          
            2
          
        
      
    
    {\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y'(x))\rVert ^{2}}

Limitations
Gradient descent with backpropagation is not guaranteed to find the global minimum of the error function, but only a local minimum; also, it has trouble crossing plateaus in the error function landscape. This issue, caused by the non-convexity of error functions in neural networks, was long thought to be a major drawback, but Yann LeCun et al. argue that in many practical problems, it is not.
Backpropagation learning does not require normalization of input vectors; however, normalization could improve performance.
Backpropagation requires the derivatives of activation functions to be known at network design time.

History
Precursors
Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676) to neural networks.
The terminology "back-propagating error correction" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this. In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.
Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation. Hecht-Nielsen credits the Robbins–Monro algorithm (1951) and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960, and Arthur E. Bryson (1961). In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. In 1973, he adapted parameters of controllers in proportion to error gradients. Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.
The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.

Modern backpropagation
Modern backpropagation was first published by Seppo Linnainmaa as "reverse mode of automatic differentiation" (1970) for discrete connected networks of nested differentiable functions.
In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's "flow of psychic energy". He faced repeated difficulty in publishing the work, only managing in 1981.
Around 1982,: 376  David E. Rumelhart independently developed: 252  backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique. These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.
In 1985, the method was also described by David Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.
Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were "known" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton.

Early successes
Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.
In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk.: 324  The NETtalk program became a popular success, appearing on the Today show.
In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.
The LeNet was published in 1989 to recognize handwritten zip codes.
In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.
In 1993, Eric Wan won an international pattern recognition contest through backpropagation.

After backpropagation
During the 2000s it fell out of favour, but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.)
Error backpropagation has been suggested to explain human brain event-related potential (ERP) components like the N400 and P600.
In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.

See also
Artificial neural network
Neural circuit
Catastrophic interference
Ensemble learning
AdaBoost
Overfitting
Neural backpropagation
Backpropagation through time

Notes
References
Further reading
Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). "6.5 Back-Propagation and Other Differentiation Algorithms". Deep Learning. MIT Press. pp. 200–220. ISBN 9780262035613.
Nielsen, Michael A. (2015). "How the backpropagation algorithm works". Neural Networks and Deep Learning. Determination Press.
McCaffrey, James (October 2012). "Neural Network Back-Propagation for Programmers". MSDN Magazine.
Rojas, Raúl (1996). "The Backpropagation Algorithm" (PDF). Neural Networks : A Systematic Introduction. Berlin: Springer. ISBN 3-540-60505-3.

External links
Backpropagation neural network tutorial at the Wikiversity
Bernacki, Mariusz; Włodarczyk, Przemysław (2004). "Principles of training multi-layer neural network using backpropagation".
Karpathy, Andrej (2016). "Lecture 4: Backpropagation, Neural Networks 1". CS231n. Stanford University. Archived from the original on 2021-12-12 – via YouTube.
"What is Backpropagation Really Doing?". 3Blue1Brown. November 3, 2017. Archived from the original on 2021-12-12 – via YouTube.
Putta, Sudeep Raja (2022). "Yet Another Derivation of Backpropagation in Matrix Form".
Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution. In many instances, bank fraud is a criminal offence. 
While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.

Types of bank fraud
Accounting fraud
In order to hide serious financial problems, some businesses have been known to use fraudulent bookkeeping to overstate sales and income, inflate the worth of the company's assets, or state a profit when the company is operating at a loss. These tampered records are then used to seek investment in the company's bond or security issues or to make fraudulent loan applications in a final attempt to obtain more money to delay the inevitable collapse of an unprofitable or mismanaged firm. Examples of accounting frauds include the Enron scandal, World Com and Ocala Funding. These companies "cooked the books" in order to appear as though they had profits each quarter, when in fact they were deeply in debt.

Demand draft fraud
Demand draft (DD) fraud typically involves one or more corrupt bank employees. Firstly, such employees remove a few DD leaves or DD books from stock and write them like a regular DD. Since they are insiders, they know the coding and punching of a demand draft. Such fraudulent demand drafts are usually drawn payable at a distant city without debiting an account. The draft is cashed at the payable branch. The fraud is discovered only when the bank's head office does the branch-wide reconciliation, which normally take six months, by which time the money is gone.

Remotely created check fraud
Remotely created checks are orders of payment created by the payee and authorized by the customer remotely, using a telephone or the internet by providing the required information including the MICR code from a valid check. They do not bear the signatures of the customers like ordinary cheques. Instead, they bear a legend statement "Authorized by Drawer". This type of instrument is usually used by credit card companies, utility companies, or telemarketers. The lack of signature makes them susceptible to fraud. The fraud is considered Demand Draft fraud in the US.

Uninsured deposits
A bank soliciting public deposits may be uninsured or not licensed to operate at all. The objective is usually to solicit for deposits to this uninsured "bank," although some may also sell stock representing ownership of the "bank." Sometimes the names appear very official or very similar to those of legitimate banks. For instance, the unlicensed "Chase Trust Bank" of Washington D.C. appeared in 2002, bearing no affiliation to its seemingly apparent namesake; the real Chase Manhattan Bank is based in New York.
Accounting fraud has also been used to conceal other theft taking place within a company.

Bill discounting fraud
Essentially a confidence trick, a fraudster uses a company at their disposal to gain the bank's confidence, by posing as a genuine, profitable customer.  To give the illusion of being a desired customer, the company regularly and repeatedly uses the bank to get payment from one or more of its customers. These payments are always made, as the customers in question are part of the fraud, actively paying any and all bills the bank attempts to collect.  After the fraudster has gained the bank's trust, the company requests that the bank begin paying the company up front for bills it will collect from the customers later. Many banks will agree but are not likely to go whole hog right away. So again, business continues as normal for the fraudulent company, its fraudulent customers, and the unwitting bank. As the bank grows more comfortable with the arrangement, it will trust the company more and more and be willing to give it larger and larger sums of money up front. Eventually, when the outstanding balance between the bank and the company is sufficiently large, the company and its customers disappear, taking the money the bank paid up front and leaving no one to pay the bills issued by the bank.

Duplication or skimming of card information
This takes a number of forms, ranging from merchants copying clients' credit card numbers for use in later illegal activities or criminals using carbon copies from old mechanical card imprint machines to steal the info, to the use of tampered credit or debit card readers to copy the magnetic stripe from a payment card while a hidden camera captures the numbers on the face of the card.
Some fraudsters have attached fraudulent card stripe readers to publicly accessible ATMs to gain unauthorised access to the contents of the magnetic stripe as well as hidden cameras to illegally record users' authorisation codes. The data recorded by the cameras and fraudulent card stripe readers are subsequently used to produce duplicate cards that could then be used to make ATM withdrawals from the victims' accounts.

Cheque kiting
Cheque kiting exploits a banking system known as "the float" wherein money is temporarily counted twice. When a cheque is deposited to an account at Bank X, the money is made available immediately in that account even though the corresponding amount of money is not immediately removed from the account at Bank Y at which the cheque is drawn. Thus both banks temporarily count the cheque amount as an asset until the cheque formally clears at Bank Y. The float serves a legitimate purpose in banking, but intentionally exploiting the float when funds at Bank Y are insufficient to cover the amount withdrawn from Bank X is a form of fraud.

Forged or fraudulent documents
Forged documents are often used to conceal other thefts; banks tend to count their money meticulously so every penny must be accounted for. A document claiming that a sum of money has been borrowed as a loan, withdrawn by an individual depositor or transferred or invested can therefore be valuable to someone who wishes to conceal the fact that the bank's money has in fact been stolen and is now gone.

Forgery and altered cheques
Fraudsters have altered cheques to change the name (in order to deposit cheques intended for payment to someone else) or the amount on the face of cheques, simple altering can change $100.00 into $100,000.00. (However, transactions for such large values are routinely investigated as a matter of policy to prevent fraud.)
Instead of tampering with a real cheque, fraudsters may alternatively attempt to forge a depositor's signature on a blank cheque or even print their own cheques drawn on accounts owned by others, non-existent accounts, etc. They would subsequently cash the fraudulent cheque through another bank and withdraw the money before the banks realise that the cheque was a fraud.

Fraudulent loan applications
These take a number of forms varying from individuals using false information to hide a credit history filled with financial problems and unpaid loans to corporations using accounting fraud to overstate profits in order to make a risky loan appear to be a sound investment for the bank.

Fraudulent loans
One way to remove money from a bank is to take out a loan, which bankers are more than willing to encourage if they have good reason to believe that the money will be repaid in full with interest. A fraudulent loan, however, is one in which the borrower is a business entity controlled by a dishonest bank officer or an accomplice; the "borrower" then declares bankruptcy or vanishes and the money is gone. The borrower may even be a non-existent entity and the loan is merely an artifice to conceal a theft of a large sum of money from the bank. This can also be seen as a component within mortgage fraud (Bell, 2010).

Empty ATM envelope deposits
A criminal overdraft can result due to the account holder making a worthless or misrepresented deposit at an automated teller machine in order to obtain more cash than present in the account or to prevent a check from being returned due to non-sufficient funds. United States banking law makes the first $100 immediately available and it may be possible for much more uncollected funds to be lost by the bank the following business day before this type of fraud is discovered. The crime could also be perpetrated against another person's account in an "account takeover" or with a counterfeit ATM card, or an account opened in another person's name as part of an identity theft scam. The emergence of ATM deposit technology that scans currency and checks without using an envelope may prevent this type of fraud in the future.

Identity theft or Impersonation
Identity theft operates by obtaining information about an individual, then using the information to apply for identity cards, accounts and credit in that person's name. Often little more than name, parents' name, date and place of birth are sufficient to obtain a birth certificate; each document obtained then is used as identification in order to obtain more identity documents. Government-issued standard identification numbers such as "social security numbers" are also valuable to the fraudster.

Money laundering
The term "money laundering" dates back to the days of Al Capone; money laundering has since been used to describe any scheme by which the true origin of funds is hidden or concealed.
Money laundering is the process by which large amounts of illegally obtained money (from drug trafficking, terrorist activity or other serious crimes) is given the appearance of having originated from a legitimate source.

Payment card fraud
Credit card fraud is widespread as a means of stealing from banks, merchants and clients.

Phishing or Internet fraud
Phishing, also known as Internet fraud, operates by sending forged e-mail, impersonating an online bank, auction or payment site; the e-mail directs the user to a forged web site which is designed to look like the login to the legitimate site but which claims that the user must update personal info. The information thus stolen is then used in other frauds, such as theft of identity or online auction fraud.
A number of malicious "Trojan horse" programmes have also been used to snoop on Internet users while online, capturing keystrokes or confidential data in order to send it to outside sites.
Fake websites can trick a visitor into downloading computer viruses that steal personal information. A visitor encounter security messages claiming his machine has viruses and instructing him to download new software, which is actually a virus.

Prime bank fraud
The "prime bank" operation which claims to offer an urgent, exclusive opportunity to cash in on the best-kept secret in the banking industry, guaranteed deposits in "primebanks", "constitutional banks", "bank notes and bank-issued debentures from top 500 world banks", "bank guarantees and standby letters of credit" which generate spectacular returns at no risk and are "endorsed by the World Bank" or various national governments and central bankers. However, these official-sounding phrases and more are the hallmark of the so-called "prime bank" fraud; they may sound great on paper, but the guaranteed offshore investment with the vague claims of an easy 100% monthly return are all fictitious financial instruments intended to defraud individuals.

Rogue traders
A rogue trader is a trader at a financial institution who engages in unauthorized trading; at times to recoup the loss they incurred in earlier trades. In those instances, out of fear and desperation, they manipulate the internal controls to circumvent detection to buy more time.
Unauthorized trading activities invariably produce more losses due to time constraints; most rogue traders are discovered at an early stage with losses ranging from $1 million to $100 million, but a very few working out of institutions with extremely lax controls were not discovered until the loss had reached well over a billion dollars. Rogue traders may not have criminal intent to defraud their employer to enrich themselves; they may be merely trying to recoup the loss to make their firm whole and salvage their employment.

Some of the largest unauthorized trading losses were discovered at Barings Bank (Nick Leeson), Daiwa Bank (Toshihide Iguchi), Sumitomo Corporation (Yasuo Hamanaka), Allfirst Bank (John Rusnak), Société Générale (Jérôme Kerviel), UBS (Kweku Adoboli), and JPMorgan Chase (Bruno Iksil).

Wire transfer fraud
Wire transfer networks such as the international SWIFT interbank fund transfer system are tempting as targets as a transfer, once made, is difficult or impossible to reverse. As these networks are used by banks to settle accounts with each other, rapid or overnight wire transfer of large amounts of money are commonplace; while banks have put checks and balances in place, there is the risk that insiders may attempt to use fraudulent or forged documents which claim to request a bank depositor's money be wired to another bank, often an offshore account in some distant foreign country.
There is a very high risk of fraud when dealing with unknown or uninsured institutions.
The risk is greatest when dealing with offshore or Internet banks (as this allows selection of countries with lax banking regulations), but not by any means limited to these institutions. There is an annual list of unlicensed banks on the US Treasury Department web site which currently is fifteen pages in length.
Also, a person may send a wire transfer from country to country. Since this takes a few days for the transfer to "clear" and be available to withdraw, the other person may still be able to withdraw the money from the other bank. A new teller or corrupt officer may approve the withdrawal since it is in pending status which then the other person cancels the wire transfer and the bank institution takes a monetary loss.

Banking fraud by country
Australia
The Commonwealth Fraud Control Framework outlines the preventions, detection, investigation and reporting obligations set by the Australian Government for fraud control. The framework includes three documents called The Fraud Rule, Fraud Policy and Fraud Guidance 
The Fraud Rule is a legislative instrument binding all Commonwealth entities setting out the key requirements of fraud control.
The Fraud Policy is a government policy binding non-corporate Commonwealth entities setting out the procedural requirements for specific areas of fraud control such as investigations and reporting.
The Fraud Guidance preventing, detecting and dealing with fraud, supports best practice guidance for the Fraud Rule and Fraud Policy setting out the government's expectations for fraud control arrangements within all Commonwealth entities.
Other important acts and regulations in the Australian Government's fraud control framework include the:

CrimesAct 1914, which sets out criminal offences against the Commonwealth, such as fraud
Criminal Code 1995, which sets out criminal offences against the Commonwealth, such as fraudulent conduct
Public Service Act 1999 and the Public Service Regulations 1999, which provide for the establishment and management of the Australian Public Service and its employees
Proceeds of Crime Act 2002 and the Proceeds of Crime Regulations 2002, which provide for the confiscation of the proceeds of crime.

United States
Under federal law, bank fraud in the United States is defined, and made illegal, primarily by the bank fraud statute in Title 18 of the U.S. Code. 18 U.S.C. § 1344 states:

Whoever knowingly executes, or attempts to execute, a scheme or artifice—
(1) to defraud a financial institution; or
(2) to obtain any of the moneys, funds, credits, assets, securities, or other property owned by, or under the custody or control of, a financial institution, by means of false or fraudulent pretenses, representations, or promises;
shall be fined not more than $1,000,000 or imprisoned not more than 30 years, or both.
State law may also criminalize the same, or similar acts.
The bank fraud statute was enacted by Congress in response to the Supreme Court's decision in Williams v. United States, 458 U.S. 279 (1982), in which the Court held that check-kiting schemes did not constitute making false statements to financial institutions (18 U.S.C. § 1014). Section 1344 has subsequently been bolstered by the Financial Institutions Reform, Recovery and Enforcement Act of 1989 (FIRREA), Pub. L. No. 101-73, 103 Stat. 500.
The bank fraud statute federally criminalizes check-kiting, check forging, non-disclosure on loan applications, diversion of funds, unauthorized use of automated teller machines (ATMs), credit card fraud, and other similar offenses. Section 1344 does not cover certain forms of money laundering, bribery, and passing bad checks. Other provisions cover these offenses.
The Supreme Court has embraced a broad interpretation of both of the numbered clauses within section 1344.  The Supreme Court has held that the first clause only requires the prosecution to show that the crime involved accounts controlled by a bank; the prosecution need not show actual financial loss to the bank or intent to cause such loss.  The Supreme Court has also held that the second clause does not require a showing of intent to defraud a financial institution.
In the United States, consumer liability for unauthorized electronic money transfers on debit cards is covered by Regulation E of the Federal Deposit Insurance Corporation. The extent of consumer liability, as detailed in section 205.6, is determined by the speed with which the consumer notifies the bank. If the bank is notified within 2 business days, the consumer is liable for $50. Over two business days the consumer is liable for $500, and over 60 business days, the consumer liability is unlimited. In contrast, all major credit card companies have a zero liability policy, effectively eliminating consumer liability in the case of fraud.

Notable cases
1873 Bank of England forgeries
Gone in 60 Seconds (bank fraud)
2014 Moldovan bank fraud scandal, $1 billion
Russian Laundromat, $20–80 billion from 2010 to 2014
The Resistance Banker, movie about WWII banker financing Dutch resistance
San Francisco Department of Public Works corruption scandal, at least three people pleaded guilty or were convicted of bank fraud

See also
References
External links
"Bank Fraud". Crimes of Persuasion.
"Bank Fraud Gallery". Bankers Online. Archived from the original on 2004-10-15. Retrieved 2004-10-05.
A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans. Lending activities can be directly performed by the bank or indirectly through capital markets.
Whereas banks play an important role in financial stability and the economy of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional-reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.
Banking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but, in many ways, functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties –  notably, the Medicis, the Pazzi, the Fuggers, the Welsers, the Berenbergs, and the Rothschilds –  have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590).

History
Banking as an archaic activity (or quasi-banking) is thought to have begun as early as the end of the 4th millennium BCE, to the 3rd millennia BCE.

Medieval
The present era of banking can be traced to medieval and early Renaissance Italy, to the rich cities in the centre and north like Florence, Lucca, Siena, Venice and Genoa. The Bardi and Peruzzi families dominated banking in 14th-century Florence, establishing branches in many other parts of Europe. Giovanni di Bicci de' Medici set up one of the most famous Italian banks, the Medici Bank, in 1397. The Republic of Genoa founded the earliest-known state deposit bank, and Banco di San Giorgio (Bank of St. George), in 1407 at Genoa, Italy.

Early modern
Fractional reserve banking and the issue of banknotes emerged in the 17th and 18th centuries. Merchants started to store their gold with the goldsmiths of London, who possessed private vaults, and who charged a fee for that service. In exchange for each deposit of precious metal, the goldsmiths issued receipts certifying the quantity and purity of the metal they held as a bailee; these receipts could not be assigned, only the original depositor could collect the stored goods.
Gradually the goldsmiths began to lend money out on behalf of the depositor, and promissory notes, which evolved into banknotes, were issued for money deposited as a loan to the goldsmith. Thus, by the 19th century, we find in ordinary cases of deposits, of money with banking corporations, or bankers, the transaction amounts to a mere loan, or mutuum, and the bank is to restore, not the same money, but an equivalent sum, whenever it is demanded
and money, when paid into a bank, ceases altogether to be the money of the principal (see Parker v. Marchant, 1 Phillips 360); it is then the money of the banker, who is bound to return an equivalent, by paying a similar sum to that deposited with him, when he is asked for it.

The goldsmith paid interest on deposits. Since the promissory notes were payable on demand, and the advances (loans) to the goldsmith's customers were repayable over a longer time-period, this was an early form of fractional reserve banking. The promissory notes developed into an assignable instrument which could circulate as a safe and convenient form of money
backed by the goldsmith's promise to pay,
allowing goldsmiths to advance loans with little risk of default. Thus the goldsmiths of London became the forerunners of banking by creating new money based on credit.

The Bank of England originated the permanent issue of banknotes in 1695. The Royal Bank of Scotland established the first overdraft facility in 1728. By the beginning of the 19th century Lubbock's Bank had established a bankers' clearing house in London to allow multiple banks to clear transactions. The Rothschilds pioneered international finance on a large scale, financing the purchase of shares in the Suez canal for the British government in 1875.

Etymology
The word bank was taken into Middle English from Middle French banque, from Old Italian banco, meaning "table", from Old High German banc, bank "bench, counter". Benches were used as makeshift desks or exchange counters during the Renaissance by Florentine bankers, who used to make their transactions atop desks covered by green tablecloths.

Definition
The definition of a bank varies from country to country. See the relevant country pages for more information.
Under English common law, a banker is defined as a person who carries on the business of banking by conducting current accounts for their customers, paying cheques drawn on them and also collecting cheques for their customers.

In most common law jurisdictions there is a Bills of Exchange Act that codifies the law in relation to negotiable instruments, including cheques, and this Act contains a statutory definition of the term banker: banker includes a body of persons, whether incorporated or not, who carry on the business of banking' (Section 2, Interpretation). Although this definition seems circular, it is actually functional, because it ensures that the legal basis for bank transactions such as cheques does not depend on how the bank is structured or regulated.
The business of banking is in many common law countries not defined by statute but by common law, the definition above. In other English common law jurisdictions there are statutory definitions of the business of banking or banking business. When looking at these definitions it is important to keep in mind that they are defining the business of banking for the purposes of the legislation, and not necessarily in general. In particular, most of the definitions are from legislation that has the purpose of regulating and supervising banks rather than regulating the actual business of banking. However, in many cases, the statutory definition closely mirrors the common law one. Examples of statutory definitions:

"banking business" means the business of receiving money on current or deposit account, paying and collecting cheques drawn by or paid in by customers, the making of advances to customers, and includes such other business as the Authority may prescribe for the purposes of this Act; (Banking Act (Singapore), Section 2, Interpretation).
"banking business" means the business of either or both of the following:
receiving from the general public money on current, deposit, savings or other similar account repayable on demand or within less than [3 months] ... or with a period of call or notice of less than that period;
paying or collecting cheques drawn by or paid in by customers.
Since the advent of EFTPOS (Electronic Funds Transfer at Point Of Sale), direct credit, direct debit and internet banking, the cheque has lost its primacy in most banking systems as a payment instrument. This has led legal theorists to suggest that the cheque based definition should be broadened to include financial institutions that conduct current accounts for customers and enable customers to pay and be paid by third parties, even if they do not pay and collect cheques .

Standard business
Banks act as payment agents by conducting checking or current accounts for customers, paying cheques drawn by customers in the bank, and collecting cheques deposited to customers' current accounts. Banks also enable customer payments via other payment methods such as Automated Clearing House (ACH), Wire transfers or telegraphic transfer, EFTPOS, and automated teller machines (ATMs).
Banks borrow money by accepting funds deposited on current accounts, by accepting term deposits, and by issuing debt securities such as banknotes and bonds. Banks lend money by making advances to customers on current accounts, by making installment loans, and by investing in marketable debt securities and other forms of money lending.
Banks provide different payment services, and a bank account is considered indispensable by most businesses and individuals. Non-banks that provide payment services such as remittance companies are normally not considered as an adequate substitute for a bank account.
Banks issue new money when they make loans. In contemporary banking systems, regulators set a minimum level of reserve funds that banks must hold against the deposit liabilities created by the funding of these loans, in order to ensure that the banks can meet demands for payment of such deposits. These reserves can be acquired through the acceptance of new deposits, sale of other assets, or borrowing from other banks including the central bank.

Range of activities
Activities undertaken by banks include personal banking, corporate banking, investment banking, private banking, transaction banking, insurance, consumer finance, trade finance and other related.

Channels
Banks offer many different channels to access their banking and other services:

Branch, in-person banking in a retail location
Automated teller machine banking adjacent to or remote from the bank
Bank by mail: Most banks accept cheque deposits via mail and use mail to communicate to their customers
Online banking over the Internet to perform multiple types of transactions
Mobile banking is using one's mobile phone to conduct banking transactions
Telephone banking allows customers to conduct transactions over the telephone with an automated attendant, or when requested, with a telephone operator
Video banking performs banking transactions or professional banking consultations via a remote video and audio connection. Video banking can be performed via purpose built banking transaction machines (similar to an Automated teller machine) or via a video conference enabled bank branch clarification
Relationship manager, mostly for private banking or business banking, who visits customers at their homes or businesses
Direct Selling Agent, who works for the bank based on a contract, whose main job is to increase the customer base for the bank

Business models
A bank can generate revenue in a variety of different ways including interest, transaction fees and financial advice. Traditionally, the most significant method is via charging interest on the capital it lends out to customers. The bank profits from the difference between the level of interest it pays for deposits and other sources of funds, and the level of interest it charges in its lending activities.
This difference is referred to as the spread between the cost of funds and the loan interest rate. Historically, profitability from lending activities has been cyclical and dependent on the needs and strengths of loan customers and the stage of the economic cycle. Fees and financial advice constitute a more stable revenue stream and banks have therefore placed more emphasis on these revenue lines to smooth their financial performance.
In the past 20 years, American banks have taken many measures to ensure that they remain profitable while responding to increasingly changing market conditions.

First, this includes the Gramm–Leach–Bliley Act, which allows banks again to merge with investment and insurance houses. Merging banking, investment, and insurance functions allows traditional banks to respond to increasing consumer demands for "one-stop shopping" by enabling cross-selling of products (which, the banks hope, will also increase profitability).
Second, they have expanded the use of risk-based pricing from business lending to consumer lending, which means charging higher interest rates to those customers that are considered to be a higher credit risk and thus increased chance of default on loans. This helps to offset the losses from bad loans, lowers the price of loans to those who have better credit histories, and offers credit products to high risk customers who would otherwise be denied credit.
Third, they have sought to increase the methods of payment processing available to the general public and business clients. These products include debit cards, prepaid cards, smart cards, and credit cards. They make it easier for consumers to conveniently make transactions and smooth their consumption over time (in some countries with underdeveloped financial systems, it is still common to deal strictly in cash, including carrying suitcases filled with cash to purchase a home).
However, with the convenience of easy credit, there is also an increased risk that consumers will mismanage their financial resources and accumulate excessive debt. Banks make money from card products through interest charges and fees charged to credit and debit card holders, and transaction fees to retailers who accept the bank's cards for payments.
This helps in making a profit and facilitates economic development as a whole.
Recently, as banks have been faced with pressure from fintechs, new and additional business models have been suggested such as freemium, monetisation of data, white-labeling of banking and payment applications, or the cross-selling of complementary products.

Products
Retail
ATM card
Credit card
Debit card
Savings account
Recurring deposit account
Fixed deposit account
Money market account
Certificate of deposit (CD)
Individual retirement account (IRA)
Mortgage
Mutual fund
Personal loan (Secured and Unsecured Personal loan)
Time deposits
Current accounts
Cheque books
Automated teller machine (ATM)
National Electronic Fund Transfer (NEFT)
Real-time gross settlement (RTGS)

Business (or commercial/investment) banking
Business loan
Capital raising (equity / debt / hybrids)
Revolving credit
Risk management (foreign exchange (FX), interest rates, commodities, derivatives)
Term loan
Cash management services (lock box, remote deposit capture, merchant processing)
Credit services
Securities Services

Capital and risk
Banks face a number of risks in order to conduct their business, and how well these risks are managed and understood is a key driver behind profitability, and how much capital a bank is required to hold. Bank capital consists principally of equity, retained earnings and subordinated debt.
Some of the main risks faced by banks include:

Credit risk: risk of loss arising from a borrower who does not make payments as promised.
Liquidity risk: risk that a given security or asset cannot be traded quickly enough in the market to prevent a loss (or make the required profit).
Market risk: risk that the value of a portfolio, either an investment portfolio or a trading portfolio, will decrease due to the change in value of the market risk factors.
Operational risk: risk arising from the execution of a company's business functions.
Reputational risk: a type of risk related to the trustworthiness of the business.
Macroeconomic risk: risks related to the aggregate economy the bank is operating in.
The capital requirement is a bank regulation, which sets a framework within which a bank or depository institution must manage its balance sheet. The categorisation of assets and capital is highly standardised so that it can be risk weighted.
After the financial crisis of 2007–2008, regulators force banks to issue Contingent convertible bonds (CoCos). These are hybrid capital securities that absorb losses in accordance with their contractual terms when the capital of the issuing bank falls below a certain level. Then debt is reduced and bank capitalisation gets a boost. Owing to their capacity to absorb losses, CoCos have the potential to satisfy regulatory capital requirement.

Banks in the economy
Economic functions
The economic functions of banks include:

Issue of money, in the form of banknotes and current accounts subject to cheque or payment at the customer's order. These claims on banks can act as money because they are negotiable or repayable on demand, and hence valued at par. They are effectively transferable by mere delivery, in the case of banknotes, or by drawing a cheque that the payee may bank or cash.
Netting and settlement of payments – banks act as both collection and paying agents for customers, participating in interbank clearing and settlement systems to collect, present, be presented with, and pay payment instruments. This enables banks to economise on reserves held for settlement of payments since inward and outward payments offset each other. It also enables the offsetting of payment flows between geographical areas, reducing the cost of settlement between them.
Credit quality improvement – banks lend money to ordinary commercial and personal borrowers (ordinary credit quality), but are high quality borrowers. The improvement comes from diversification of the bank's assets and capital which provides a buffer to absorb losses without defaulting on its obligations. However, banknotes and deposits are generally unsecured; if the bank gets into difficulty and pledges assets as security, to raise the funding it needs to continue to operate, this puts the note holders and depositors in an economically subordinated position.
Asset liability mismatch/Maturity transformation – banks borrow more on demand debt and short term debt, but provide more long-term loans. In other words, they borrow short and lend long. With a stronger credit quality than most other borrowers, banks can do this by aggregating issues (e.g. accepting deposits and issuing banknotes) and redemptions (e.g. withdrawals and redemption of banknotes), maintaining reserves of cash, investing in marketable securities that can be readily converted to cash if needed, and raising replacement funding as needed from various sources (e.g. wholesale cash markets and securities markets).
Money creation/destruction – whenever a bank gives out a loan in a fractional-reserve banking system, a new sum of money is created and conversely, whenever the principal on that loan is repaid money is destroyed.

Bank crisis
Banks are susceptible to many forms of risk which have triggered occasional systemic crises. These include liquidity risk (where many depositors may request withdrawals in excess of available funds), credit risk (the chance that those who owe money to the bank will not repay it), and interest rate risk (the possibility that the bank will become unprofitable, if rising interest rates force it to pay relatively more on its deposits than it receives on its loans).
Banking crises have developed many times throughout history when one or more risks have emerged for the banking sector as a whole. Prominent examples include the bank run that occurred during the Great Depression, the U.S. Savings and Loan crisis in the 1980s and early 1990s, the Japanese banking crisis during the 1990s, and the sub-prime mortgage crisis in the 2000s.
The 2023 global banking crisis is the latest of these crises: In March 2023, liquidity shortages and bank insolvencies led to three bank failures in the United States, and within two weeks, several of the world's largest banks failed or were shut down by regulators

Size of global banking industry
Assets of the largest 1,000 banks in the world grew by 6.8% in the 2008–2009 financial year to a record US$96.4 trillion while profits declined by 85% to US$115 billion. Growth in assets in adverse market conditions was largely a result of recapitalisation. EU banks held the largest share of the total, 56% in 2008–2009, down from 61% in the previous year. Asian banks' share increased from 12% to 14% during the year, while the share of US banks increased from 11% to 13%. Fee revenue generated by global investment in banking totalled US$66.3 billion in 2009, up 12% on the previous year.
The United States has the most banks in the world in terms of institutions (5,330 as of 2015) and possibly branches (81,607 as of 2015). This is an indicator of the geography and regulatory structure of the US, resulting in a large number of small to medium-sized institutions in its banking system. As of November 2009, China's top four banks have in excess of 67,000 branches (ICBC:18000+, BOC:12000+, CCB:13000+, ABC:24000+) with an additional 140 smaller banks with an undetermined number of branches.
Japan had 129 banks and 12,000 branches. In 2004, Germany, France, and Italy each had more than 30,000 branches – more than double the 15,000 branches in the United Kingdom.

Mergers and acquisitions
Between 1985 and 2018 banks engaged in around 28,798 mergers or acquisitions, either as the acquirer or the target company. The overall known value of these deals cumulates to around 5,169 bil. USD. In terms of value, there have been two major waves (1999 and 2007) which both peaked at around 460 bil. USD followed by a steep decline (−82% from 2007 until 2018).
Here is a list of the largest deals in history in terms of value with participation from at least one bank:

Regulation
Currently, commercial banks are regulated in most jurisdictions by government entities and require a special bank license to operate.

Usually, the definition of the business of banking for the purposes of regulation is extended to include acceptance of deposits, even if they are not repayable to the customer's order – although money lending, by itself, is generally not included in the definition.
Unlike most other regulated industries, the regulator is typically also a participant in the market, being either publicly or privately governed central bank. Central banks also typically have a monopoly on the business of issuing banknotes. However, in some countries, this is not the case. In the UK, for example, the Financial Services Authority licenses banks, and some commercial banks (such as the Bank of Scotland) issue their own banknotes in addition to those issued by the Bank of England, the UK government's central bank.

Banking law is based on a contractual analysis of the relationship between the bank (defined above) and the customer – defined as any entity for which the bank agrees to conduct an account.
The law implies rights and obligations into this relationship as follows:

The bank account balance is the financial position between the bank and the customer: when the account is in credit, the bank owes the balance to the customer; when the account is overdrawn, the customer owes the balance to the bank.

The bank agrees to pay the customer's checks up to the amount standing to the credit of the customer's account, plus any agreed overdraft limit.

The bank may not pay from the customer's account without a mandate from the customer, e.g. a cheque drawn by the customer.

The bank agrees to promptly collect the cheques deposited to the customer's account as the customer's agent and to credit the proceeds to the customer's account.

And, the bank has a right to combine the customer's accounts since each account is just an aspect of the same credit relationship.

The bank has a lien on cheques deposited to the customer's account, to the extent that the customer is indebted to the bank.

The bank must not disclose details of transactions through the customer's account – unless the customer consents, there is a public duty to disclose, the bank's interests require it, or the law demands it.

The bank must not close a customer's account without reasonable notice, since cheques are outstanding in the ordinary course of business for several days.
These implied contractual terms may be modified by express agreement between the customer and the bank. The statutes and regulations in force within a particular jurisdiction may also modify the above terms or create new rights, obligations, or limitations relevant to the bank-customer relationship.
Some types of financial institutions, such as building societies and credit unions, may be partly or wholly exempt from bank license requirements, and therefore regulated under separate rules.
The requirements for the issue of a bank license vary between jurisdictions but typically include:

Minimum capital
Minimum capital ratio
'Fit and Proper' requirements for the bank's controllers, owners, directors, or senior officers
Approval of the bank's business plan as being sufficiently prudent and plausible.

Different types of banking
Banks' activities can be divided into:

retail banking, dealing directly with individuals and small businesses;
business banking, providing services to mid-market business;
corporate banking, directed at large business entities;
private banking, providing wealth management services to high-net-worth individuals and families;
investment banking, relating to activities on the financial markets.
Most banks are profit-making, private enterprises. However, some are owned by the government, or are non-profit organisations.

Types of banks
Commercial banks: the term used for a normal bank to distinguish it from an investment bank. After the Great Depression, the U.S. Congress required that banks only engage in banking activities, whereas investment banks were limited to capital market activities. Since the two no longer have to be under separate ownership, some use the term "commercial bank" to refer to a bank or a division of a bank that mostly deals with deposits and loans from corporations or large businesses.
Community banks: locally operated financial institutions that empower employees to make local decisions to serve their customers and partners.
Community development banks: regulated banks that provide financial services and credit to under-served markets or populations.
Land development banks: The special banks providing long-term loans are called land development banks (LDB). The history of LDB is quite old. The first LDB was started at Jhang in Punjab in 1920. The main objective of the LDBs is to promote the development of land, agriculture and increase the agricultural production. The LDBs provide long-term finance to members directly through their branches.
Credit unions or co-operative banks: not-for-profit cooperatives owned by the depositors and often offering rates more favourable than for-profit banks. Typically, membership is restricted to employees of a particular company, residents of a defined area, members of a certain union or religious organisations, and their immediate families.
Postal savings banks: savings banks associated with national postal systems.
Private banks: banks that manage the assets of high-net-worth individuals. Historically a minimum of US$1 million was required to open an account, however, over the last years, many private banks have lowered their entry hurdles to US$350,000 for private investors.
Offshore banks: banks located in jurisdictions with low taxation and regulation. Many offshore banks are essentially private banks.
Savings banks: in Europe, savings banks took their roots in the 19th or sometimes even in the 18th century. Their original objective was to provide easily accessible savings products to all strata of the population. In some countries, savings banks were created on public initiative; in others, socially committed individuals created foundations to put in place the necessary infrastructure. Nowadays, European savings banks have kept their focus on retail banking: payments, savings products, credits, and insurances for individuals or small and medium-sized enterprises. Apart from this retail focus, they also differ from commercial banks by their broadly decentralised distribution network, providing local and regional outreach – and by their socially responsible approach to business and society.
Ethical banks: banks that prioritize the transparency of all operations and make only what they consider to be socially responsible investments.
A direct or internet-only bank is a banking operation without any physical bank branches. Transactions are usually accomplished using ATMs and electronic transfers and direct deposits through an online interface.

Types of investment banks
Investment banks "underwrite" (guarantee the sale of) stock and bond issues, provide investment management, and advise corporations on capital market activities such as M&A, trade for their own accounts, make markets, provide securities services to institutional clients.
Merchant banks were traditionally banks which engaged in trade finance. The modern definition, however, refers to banks which provide capital to firms in the form of shares rather than loans. Unlike venture caps, they tend not to invest in new companies.

Combination banks
Universal banks, more commonly known as financial services companies, engage in several of these activities. These big banks are very diversified groups that, among other services, also distribute insurance –  hence the term bancassurance, a portmanteau word combining "banque or bank" and "assurance", signifying that both banking and insurance are provided by the same corporate entity.

Other types of banks
Central banks are normally government-owned and charged with quasi-regulatory responsibilities, such as supervising commercial banks, or controlling the cash interest rate. They generally provide liquidity to the banking system and act as the lender of last resort in event of a crisis.
Islamic banks adhere to the concepts of Islamic law. This form of banking revolves around several well-established principles based on Islamic laws. All banking activities must avoid interest, a concept that is forbidden in Islam. Instead, the bank earns profit (markup) and fees on the financing facilities that it extends to customers.

Challenges within the banking industry
United States
The United States banking industry is one of the most heavily regulated and guarded in the world, with multiple specialised and focused regulators. All banks with FDIC-insured deposits have the Federal Deposit Insurance Corporation (FDIC) as a regulator. However, for soundness examinations (i.e., whether a bank is operating in a sound manner), the Federal Reserve is the primary federal regulator for Fed-member state banks; the Office of the Comptroller of the Currency (OCC) is the primary federal regulator for national banks. State non-member banks are examined by the state agencies as well as the FDIC.: 236  National banks have one primary regulator – the OCC.
Each regulatory agency has its own set of rules and regulations to which banks and thrifts must adhere.
The Federal Financial Institutions Examination Council (FFIEC) was established in 1979 as a formal inter-agency body empowered to prescribe uniform principles, standards, and report forms for the federal examination of financial institutions. Although the FFIEC has resulted in a greater degree of regulatory consistency between the agencies, the rules and regulations are constantly changing.
In addition to changing regulations, changes in the industry have led to consolidations within the Federal Reserve, FDIC, OTS, and OCC. Offices have been closed, supervisory regions have been merged, staff levels have been reduced and budgets have been cut. The remaining regulators face an increased burden with an increased workload and more banks per regulator. While banks struggle to keep up with the changes in the regulatory environment, regulators struggle to manage their workload and effectively regulate their banks. The impact of these changes is that banks are receiving less hands-on assessment by the regulators, less time spent with each institution, and the potential for more problems slipping through the cracks, potentially resulting in an overall increase in bank failures across the United States.
The changing economic environment has a significant impact on banks and thrifts as they struggle to effectively manage their interest rate spread in the face of low rates on loans, rate competition for deposits and the general market changes, industry trends and economic fluctuations. It has been a challenge for banks to effectively set their growth strategies with the recent economic market. A rising interest rate environment may seem to help financial institutions, but the effect of the changes on consumers and businesses is not predictable and the challenge remains for banks to grow and effectively manage the spread to generate a return to their shareholders.
The management of the banks' asset portfolios also remains a challenge in today's economic environment. Loans are a bank's primary asset category and when loan quality becomes suspect, the foundation of a bank is shaken to the core. While always an issue for banks, declining asset quality has become a big problem for financial institutions.

There are several reasons for this, one of which is the lax attitude some banks have adopted because of the years of "good times." The potential for this is exacerbated by the reduction in the regulatory oversight of banks and in some cases depth of management. Problems are more likely to go undetected, resulting in a significant impact on the bank when they are discovered. In addition, banks, like any business, struggle to cut costs and have consequently eliminated certain expenses, such as adequate employee training programs.
Banks also face a host of other challenges such as ageing ownership groups. Across the country, many banks' management teams and boards of directors are ageing. Banks also face ongoing pressure from shareholders, both public and private, to achieve earnings and growth projections. Regulators place added pressure on banks to manage the various categories of risk. Banking is also an extremely competitive industry. Competing in the financial services industry has become tougher with the entrance of such players as insurance agencies, credit unions, cheque cashing services, credit card companies, etc.
As a reaction, banks have developed their activities in financial instruments, through financial market operations such as brokerage and have become big players in such activities.
Another major challenge is the ageing infrastructure, also called legacy IT. Backend systems were built decades ago and are incompatible with new applications. Fixing bugs and creating interfaces costs huge sums, as knowledgeable programmers become scarce.

Loan activities of banks
To be able to provide home buyers and builders with the funds needed, banks must compete for deposits. The phenomenon of disintermediation had to dollars moving from savings accounts and into direct market instruments such as U.S. Department of Treasury obligations, agency securities, and corporate debt. One of the greatest factors in recent years in the movement of deposits was the tremendous growth of money market funds whose higher interest rates attracted consumer deposits.
To compete for deposits, US savings institutions offer many different types of plans:

Passbook or ordinary deposit accounts  –  permit any amount to be added to or withdrawn from the account at any time.
NOW and Super NOW accounts  –  function like checking accounts but earn interest. A minimum balance may be required on Super NOW accounts.
Money market accounts  –  carry a monthly limit of preauthorised transfers to other accounts or persons and may require a minimum or average balance.
Certificate accounts  –  subject to loss of some or all interest on withdrawals before maturity.
Notice accounts  –  the equivalent of certificate accounts with an indefinite term. Savers agree to notify the institution a specified time before withdrawal.
Individual retirement accounts (IRAs) and Keogh plans  –  a form of retirement savings in which the funds deposited and interest earned are exempt from income tax until after withdrawal.
Checking accounts  –  offered by some institutions under definite restrictions.
All withdrawals and deposits are completely the sole decision and responsibility of the account owner unless the parent or guardian is required to do otherwise for legal reasons.
Club accounts and other savings accounts  –  designed to help people save regularly to meet certain goals.

Types of accounts
Bank statements are accounting records produced by banks under the various accounting standards of the world. Under GAAP there are two kinds of accounts: debit and credit. Credit accounts are Revenue, Equity and Liabilities. Debit Accounts are Assets and Expenses. The bank credits a credit account to increase its balance, and debits a credit account to decrease its balance.
The customer debits his or her savings/bank (asset) in his ledger when making a deposit (and the account is normally in debit), while the customer credits a credit card (liability) account in his ledger every time he spends money (and the account is normally in credit). When the customer reads his bank statement, the statement will show a credit to the account for deposits, and debits for withdrawals of funds. The customer with a positive balance will see this balance reflected as a credit balance on the bank statement. If the customer is overdrawn, he will have a negative balance, reflected as a debit balance on the bank statement.

Brokered deposits
One source of deposits for banks is deposit brokers who deposit large sums of money on behalf of investors through trust corporations. This money will generally go to the banks which offer the most favourable terms, often better than those offered local depositors. It is possible for a bank to engage in business with no local deposits at all, all funds being brokered deposits. Accepting a significant quantity of such deposits, or "hot money" as it is sometimes called, puts a bank in a difficult and sometimes risky position, as the funds must be lent or invested in a way that yields a return sufficient to pay the high interest being paid on the brokered deposits. This may result in risky decisions and even in eventual failure of the bank. Banks which failed during 2008 and 2009 in the United States during the global financial crisis had, on average, four times more brokered deposits as a percent of their deposits than the average bank. Such deposits, combined with risky real estate investments, factored into the savings and loan crisis of the 1980s. Regulation of brokered deposits is opposed by banks on the grounds that the practice can be a source of external funding to growing communities with insufficient local deposits. There are different types of accounts: saving, recurring and current accounts.

Custodial accounts
Custodial accounts are accounts in which assets are held for a third party. For example, businesses that accept custody of funds for clients prior to their conversion, return, or transfer may have a custodial account at a bank for these purposes.

Globalisation in the banking industry
In modern times there have been huge reductions to the barriers of global competition in the banking industry. Increases in telecommunications and other financial technologies, such as Bloomberg, have allowed banks to extend their reach all over the world since they no longer have to be near customers to manage both their finances and their risk. The growth in cross-border activities has also increased the demand for banks that can provide various services across borders to different nationalities. Despite these reductions in barriers and growth in cross-border activities, the banking industry is nowhere near as globalised as some other industries. In the US, for instance, very few banks even worry about the Riegle–Neal Act, which promotes more efficient interstate banking. In the vast majority of nations around the globe, the market share for foreign owned banks is currently less than a tenth of all market shares for banks in a particular nation. One reason the banking industry has not been fully globalised is that it is more convenient to have local banks provide loans to small businesses and individuals. On the other hand, for large corporations, it is not as important in what nation the bank is in since the corporation's financial information is available around the globe.

See also
References
Further reading
Born, Karl Erich. International Banking in the 19th and 20th Centuries (St Martin's, 1983) online

External links

Guardian Datablog – World's Biggest Banks
Banking, Banks, and Credit Unions from UCB Libraries GovPubs (archived 11 January 2012)
A Guide to the National Banking System (PDF). Office of the Comptroller of the Currency (OCC), Washington, D.C. Provides an overview of the national banking system of the US, its regulation, and the OCC.
Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI's ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.
LaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.
Gemini has received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with conservative commentators decrying its alleged bias as "wokeness".

Background
In November 2022, OpenAI launched ChatGPT, a chatbot based on the GPT-3 family of large language models (LLMs). ChatGPT gained worldwide attention following its release, becoming a viral Internet sensation. Alarmed by ChatGPT's potential threat to Google Search, Google executives issued a "code red" alert, reassigning several teams to assist in the company's artificial intelligence (AI) efforts. Sundar Pichai, the CEO of Google and parent company Alphabet, was widely reported to have issued the alert, but Pichai later denied this to The New York Times. In a rare move, Google co-founders Larry Page and Sergey Brin, who had stepped down from their roles as co-CEOs of Alphabet in 2019, were summoned to emergency meetings with company executives to discuss Google's response to ChatGPT. Brin requested access to Google's code in February 2023, for the first time in years.
Earlier in 2021, the company had unveiled LaMDA, a prototype LLM, but did not release it to the public. When asked by employees at an all-hands meeting whether LaMDA was a missed opportunity for Google to compete with ChatGPT, Pichai and Google AI chief Jeff Dean stated that while the company had similar capabilities to ChatGPT, moving too quickly in that arena would represent a major "reputational risk" due to Google being substantially larger than OpenAI. In January 2023, Google sister company DeepMind CEO Demis Hassabis hinted at plans for a ChatGPT rival, and Google employees were instructed to accelerate progress on a ChatGPT competitor, intensively testing "Apprentice Bard" and other chatbots. Pichai assured investors during Google's quarterly earnings investor call in February that the company had plans to expand LaMDA's availability and applications.

History
Announcement
On February 6, 2023, Google announced Bard, a generative artificial intelligence chatbot powered by LaMDA. Bard was first rolled out to a select group of 10,000 "trusted testers", before a wide release scheduled at the end of the month. The project was overseen by product lead Jack Krawczyk, who described the product as a "collaborative AI service" rather than a search engine, while Pichai detailed how Bard would be integrated into Google Search. Reuters calculated that adding ChatGPT-like features to Google Search could cost the company $6 billion in additional expenses by 2024, while research and consulting firm SemiAnalysis calculated that it would cost Google $3 billion. The technology was developed under the codename "Atlas", with the name "Bard" in reference to the Celtic term for a storyteller and chosen to "reflect the creative nature of the algorithm underneath".
Multiple media outlets and financial analysts described Google as "rushing" Bard's announcement to preempt rival Microsoft's planned February 7 event unveiling its partnership with OpenAI to integrate ChatGPT into its Bing search engine in the form of Bing Chat (later rebranded as Microsoft Copilot), as well as to avoid playing "catch-up" to Microsoft. Microsoft CEO Satya Nadella told The Verge: "I want people to know that we made them dance." Tom Warren of The Verge and Davey Alba of Bloomberg News noted that this marked the beginning of another clash between the two Big Tech companies over "the future of search", after their six-year "truce" expired in 2021; Chris Stokel-Walker of The Guardian, Sara Morrison of Recode, and analyst Dan Ives of investment firm Wedbush Securities labeled this an AI arms race between the two.
After an "underwhelming" February 8 livestream in Paris showcasing Bard, Google's stock fell eight percent, equivalent to a $100 billion loss in market value, and the YouTube video of the livestream was made private. Many viewers also pointed out an error during the demo in which Bard gives inaccurate information about the James Webb Space Telescope in response to a query. Google employees criticized Pichai's "rushed" and "botched" announcement of Bard on Memgen, the company's internal forum, while Maggie Harrison of Futurism called the rollout "chaos". Pichai defended his actions by saying that Google had been "deeply working on AI for a long time", rejecting the notion that Bard's launch was a knee-jerk reaction. Alphabet chairman John Hennessy acknowledged that Bard was not fully product-ready, but expressed excitement at the technology's potential.
A week after the Paris livestream, Pichai asked employees to dedicate two to four hours to dogfood testing Bard, while Google executive Prabhakar Raghavan encouraged employees to correct any errors Bard makes. 80,000 employees responded to Pichai's call to action. In the following weeks, Google employees roundly criticized Bard in internal messages, citing a variety of safety and ethical concerns and calling on company leaders not to launch the service. Seeking to prioritize keeping up with competitors, Google executives decided to proceed with the launch anyway, overruling an unsympathetic risk assessment report conducted by its AI ethics team. After Pichai suddenly laid off 12,000 employees later that month due to slowing revenue growth, remaining workers shared memes and snippets of their humorous exchanges with Bard soliciting its "opinion" on the layoffs. Google employees began testing a more sophisticated version of Bard with larger parameters, dubbed "Big Bard", in mid-March.

Launch
Google opened up early access for Bard on March 21, 2023, in a limited capacity, allowing users in the U.S. and UK to join a waitlist. Unlike Microsoft's approach with Bing Chat, Bard was launched as a standalone web application featuring a text box and a disclaimer that the chatbot "may display inaccurate or offensive information that doesn't represent Google's views". Three responses are then provided to each question, with users prompted to submit feedback on the usefulness of each answer. Google vice presidents Sissie Hsiao and Eli Collins framed Bard as a complement to Google Search and stated that the company had not determined how to make the service profitable. Among those granted early access were those enrolled in Google's "Pixel Superfans" loyalty program, users of its Pixel and Nest devices, and Google One subscribers.
Bard is trained by third-party contractors hired by Google, including Appen and Accenture workers, whom Business Insider and Bloomberg News reported were placed under extreme pressure, overworked, and underpaid. Bard is also trained on data from publicly available sources, which Google disclosed by amending its privacy policy. Shortly after Bard's initial launch, Google reorganized the team behind Google Assistant, the company's virtual assistant, to focus on Bard instead. Google researcher Jacob Devlin resigned from the company after claiming that Bard had surreptitiously leveraged data from ChatGPT; Google denied the allegations. Meanwhile, a senior software engineer at the company published an internal memo warning that Google was falling behind in the AI "arms race", not to OpenAI but to independent researchers in open-source communities. Pichai revealed on March 31 that the company intended to "upgrade" Bard by basing it on PaLM, a newer and more powerful LLM from Google, rather than LaMDA. The same day, Krawczyk announced that Google had added "math and logic capabilities" to Bard. Bard gained the ability to assist in coding in April, being compatible with more than 20 programming languages at launch. Microsoft also began running advertisements in the address bar of a developer build of the Edge browser, urging users to try Bing whenever they visit the Bard web app. Google is working to integrate Bard into its ChromeOS operating system and Pixel devices.

Updates
Bard took center stage during the annual Google I/O keynote in May 2023, with Pichai and Hsiao announcing a series of updates to Bard, including the adoption of PaLM 2, integration with other Google products and third-party services, expansion to 180 countries, support for additional languages, and new features. In stark contrast to previous years, the Assistant was barely mentioned during the event. The expanded rollout did not include any nations in the European Union (EU), possibly reflecting concerns about compliance with the General Data Protection Regulation. Those with Google Workspace accounts also gained access to the service. Google attempted to launch Bard in the EU in June but was blocked by the Irish Data Protection Commission, who requested a "data protection impact assessment" from the company. In July, Google launched Bard in the EU and Brazil, added support for dozens of new languages, and introduced multiple new personalization and productivity features. An invite-only chatroom ("server") on Discord was created in July, consisting of users who heavily use Bard. Over the next few months, the chatroom was flooded with comments questioning the usefulness of Bard.
Reflecting on Bard's launch in an interview with Wired in September, Pichai acknowledged that Google had been "cautious" to release LaMDA because of "the responsibility that comes with getting it right", complimenting OpenAI for ChatGPT's launch and firing back at Nadella's comment about making Google dance. Google released a major update to the chatbot later that month, integrating it into many of its products through "extensions", adding a button to fact-check AI-generated responses through Google Search, and allowing users to share conversation threads. Google also introduced the "Google-Extended" web crawler as part of its search engine's robots.txt indexing file to allow web publishers to opt-out of allowing Bard to scan them for training. Online users later discovered that Google Search was indexing Bard conversation threads on which users had enabled sharing. Google stated that this was an error and quickly moved to rectify the leaks.
In October, during the company's annual Made by Google event in which it announced the Pixel 8 series and the Pixel Watch 2, Hsiao unveiled "Assistant with Bard", an upgraded version of the Google Assistant which was deeply integrated with Bard, following in the footsteps of Amazon's approach with Alexa. When the U.S. Copyright Office solicited public comment on potential new regulation on generative AI technologies, Google joined with OpenAI and Microsoft in arguing that the responsibility for generating copyrighted material lay with the user, not the developer. Accenture contractors voted to join the Alphabet Workers Union in November, in protest of suboptimal working conditions, while the company filed a lawsuit in the U.S. District Court for the Northern District of California against a group of unidentified scammers who had been advertising malware disguised as a downloadable version of Bard.

Relaunch
On December 6, 2023, Google announced Gemini, a multimodal and more powerful LLM touted as the company's "largest and most capable AI model". A specially tuned version of the mid-tier Gemini Pro was integrated into Bard, while the larger Gemini Ultra was set to power "Bard Advanced" in 2024. The Wall Street Journal reported that Bard was then averaging around 220 million monthly visitors. Google ended its contract with Appen in January 2024, while Bard gained the long-awaited ability to generate images the next month, powered by Google Brain's Imagen 2 text-to-image model.
On February 8, 2024, Bard and Duet AI were unified under the Gemini brand, with a mobile app launched on Android and the service integrated into the Google app on iOS. On Android, users who downloaded the app saw Gemini replace Assistant as their device's default virtual assistant, though Assistant remained a standalone service. Google also launched "Gemini Advanced with Ultra 1.0", available via a "Google One AI Premium" subscription, incorporated Gemini into its Messages app on Android, and announced a partnership with Stack Overflow.
Gemini once again took center stage at the 2024 Google I/O keynote, with traditionally emphasized topics such as Android 15 and the Pixel 8a relegated to separate events the next day and prior week, respectively. Google announced Gemini integrations into a variety of products, including Android, Chrome, Photos, and Workspace. The Washington Post described the presentation as a "tsunami of new AI features". Gemini Advanced was upgraded to the "Gemini 1.5 Pro" language model, with Google previewing Gemini Live, a voice chat mode, and Gems, the ability to create custom chatbots. Beginning with the Pixel 9 series, Gemini replaced the Google Assistant as the default virtual assistant on Pixel devices, while Gemini Live debuted on the phones.

Reception
Critical response
Gemini, then known as Bard, received mixed reviews upon its initial release. James Vincent of The Verge found it faster than ChatGPT and Bing Chat, but noted that the lack of Bing-esque footnotes was "both a blessing and a curse", encouraging Google to be bolder when experimenting with AI. His colleague David Pierce was unimpressed by its uninteresting and sometimes inaccurate responses, adding that despite Google's insistence that Bard was not a search engine, its user interface resembled that of one, which could cause problems for Google. Cade Metz of The New York Times described Bard as "more cautious" than ChatGPT, while Shirin Ghaffary of Vox called it "dry and uncontroversial" due to the reserved nature of its responses.
The Washington Post columnist Geoffrey A. Fowler found Bard a mixed bag, noting that it acted cautiously but could show Internet-influenced bias. Writing for ZDNET, Sabrina Ortiz believed ChatGPT and Bing Chat were "more capable overall" in comparison to Bard, while Wired journalist Lauren Goode found her conversation with Bard "the most bizarre" of the three. After the introduction of extensions, The New York Times' Kevin Roose found the update underwhelming and "a bit of a mess", while Business Insider's Lakshmi Varanasi found that Bard often leaned more into flattery than facts.
In a 60 Minutes conversation with Hsiao, Google senior vice president James Manyika, and Pichai, CBS News correspondent Scott Pelley found Gemini "unsettling". Associate professor Ethan Mollick of the Wharton School of the University of Pennsylvania was underwhelmed by its artistic ineptitude. The New York Times conducted a test with ChatGPT and Gemini regarding their ability to handle tasks expected of human assistants, and concluded that ChatGPT's performance was vastly superior to that of Gemini. NewsGuard, a tool that rates the credibility of news articles, found that Gemini was more skilled at debunking known conspiracy theories than ChatGPT. A report published by the Associated Press cautioned that Gemini and other chatbots were prone to generate "false and misleading information that threaten[ed] to disenfranchise voters".

Image generation controversy
In February 2024, social media users reported that Gemini was generating images that featured people of color and women in historically inaccurate contexts—such as Vikings, Nazi soldiers, and the Founding Fathers—and refusing prompts to generate images of white people. These images were derided on social media, including by conservatives and libertarians who cited them as evidence of Google's "wokeness". The business magnate Elon Musk, whose company xAI operates the chatbot Grok, was among those who criticized Google, denouncing its suite of products as biased and racist. Musk and other users targeted Krawczyk, resurfacing his past comments discussing race, leading Krawczyk to withdraw from X (Twitter) and LinkedIn. The conservative-leaning tabloid New York Post ran a cover story on the incident in the print edition of its newspaper.
In response, Krawczyk said that Google was "working to improve these kinds of depictions immediately", and Google paused Gemini's ability to generate images of people. Raghavan released a lengthy statement addressing the controversy, explaining that Gemini had "overcompensate[d]" amid its efforts to strive for diversity and acknowledging that the images were "embarrassing and wrong". In an internal memo to employees, Pichai called the debacle offensive and unacceptable, promising structural and technical changes. Several employees in Google's trust and safety team were laid off days later. Hassabis stated that Gemini's ability to generate images of people would be restored within two weeks; it was ultimately relaunched in late August, powered by its new Imagen 3 model.
The market reacted negatively, with Google's stock falling by 4.4 percent. Pichai faced growing calls to resign, including from technology analysts Ben Thompson and Om Malik. House Republicans led by Jim Jordan subpoenaed Google, accusing the company of colluding with the Biden administration to censor speech. In light of the fiasco and Google's overall response to OpenAI, Business Insider's Hugh Langley and Lara O'Reilly declared that Google was fast going "from vanguard to dinosaur". Bloomberg columnist Parmy Olson suggested that Google's "rushed" rollout of Gemini was the cause of its woes, not "wokeness". Martin Peers, writing for The Information, opined that Google needed a leader like Mark Zuckerberg to defuse the situation. Hugging Face scientist Sasha Luccioni and Surrey University professor Alan Woodward believed that the incident had "deeply embedded" roots in Gemini's training corpus and algorithms, making it difficult to rectify. Jeremy Kahn of Fortune called for researchers focused on safety and responsibility to work together to develop better guardrails. New York magazine contributor John Herrman wrote: "It's a spectacular unforced error, a slapstick rake-in-the-face moment, and a testament to how panicked Google must be by the rise of OpenAI and the threat of AI to its search business."

Other incidents
In the aftermath of the image generation controversy, some users began accusing Gemini's text responses of being biased toward the left. In one such example that circulated online, Gemini said that it was "difficult to say definitively" whether Musk or the Nazi dictator Adolf Hitler had more negatively affected society. Others users reported that Gemini tended to promote left-wing politicians and causes such as affirmative action and abortion rights while refusing to promote right-wing figures, meat consumption, and fossil fuels. The Wall Street Journal's editorial board wrote that Gemini's "apparently ingrained woke biases" were "fueling a backlash toward AI on the political right, which is joining the left in calling for more regulation."
Indian Ministry of Electronics and Information Technology junior minister Rajeev Chandrasekhar alleged that Google had violated the country's Information Technology Rules by refusing to summarize an article by the right-wing news website OpIndia, and for saying that some experts described Prime Minister Narendra Modi's policies as fascist. In France, Google was fined €250 million by the competition regulator Autorité de la concurrence under the Directive on Copyright in the Digital Single Market, in part due to its cited failure to inform local news publishers of when their content was used for Gemini's training. The U.S. state-owned broadcaster Voice of America accused Gemini of "parroting" Chinese propaganda.
During the 2024 Summer Olympics in July, Google aired a commercial for Gemini entitled "Dear Sydney" depicting a father asking the chatbot to generate a fan letter to the star athlete Sydney McLaughlin-Levrone for his young daughter. Similar to Apple's recent "Crush!" commercial for the seventh-generation iPad Pro, the advertisement drew heavy backlash online, with criticism for replacing authentic human expression and creativity with a computer; The Washington Post columnist Alexandra Petri lambasted the commercial as "missing the point". As a result, Google withdrew the commercial from NBC's rotation.

References
Further reading
External links

Official website 
Gemini on Google Play 
White paper
Generative AI at Google AI
Basic Books is a book publisher founded in 1950 and located in New York City, now an imprint of Hachette Book Group. It publishes books in the fields of psychology, philosophy, economics, science, politics, sociology, current affairs, and history.

History
Basic Books originated as a small Greenwich Village-based book club marketed to psychoanalysts. Arthur Rosenthal took over the book club in 1950, and under his ownership it soon began producing original books, mostly in the behavioral sciences. Early successes included Ernest Jones's The Life and Work of Sigmund Freud, as well as works by Claude Lévi-Strauss, Jean Piaget and Erik Erikson. Irving Kristol joined Basic Books in 1960, and helped Basic to expand into the social sciences. Harper & Row purchased the company in 1969.
In 1997, HarperCollins announced that it would merge Basic Books into its trade publishing program, effectively closing the imprint and ending its publishing of serious academic books.  That same year, Basic was purchased by the newly created Perseus Books Group. Perseus's publishing business was acquired by Hachette Book Group in 2016. In 2018, Seal Press became an imprint of Basic.

Authors
Basic's list of authors includes:

References
External links
Basic Books website
Perseus Group website (archived February 25, 2017)
Samuel G. Freedman on the formation of Perseus Books
In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.
In numerical analysis and approximation theory, basis functions are also called blending functions, because of their use in interpolation: In this application, a mixture of the basis functions provides an interpolating function (with the "blend" depending on the evaluation of the basis functions at the data points).

Examples
Monomial basis for Cω
The monomial basis for the vector space of analytic functions is given by 

  
    
      
        {
        
          x
          
            n
          
        
        ∣
        n
        ∈
        
          N
        
        }
        .
      
    
    {\displaystyle \{x^{n}\mid n\in \mathbb {N} \}.}
  

This basis is used in Taylor series, amongst others.

Monomial basis for polynomials
The monomial basis also forms a basis for the vector space of polynomials. After all, every polynomial can be written as 
  
    
      
        
          a
          
            0
          
        
        +
        
          a
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          a
          
            2
          
        
        
          x
          
            2
          
        
        +
        ⋯
        +
        
          a
          
            n
          
        
        
          x
          
            n
          
        
      
    
    {\displaystyle a_{0}+a_{1}x^{1}+a_{2}x^{2}+\cdots +a_{n}x^{n}}
  
 for some 
  
    
      
        n
        ∈
        
          N
        
      
    
    {\displaystyle n\in \mathbb {N} }
  
, which is a linear combination of monomials.

Fourier basis for L2[0,1]
Sines and cosines form an (orthonormal) Schauder basis for square-integrable functions on a bounded domain. As a particular example, the collection

  
    
      
        {
        
          
            2
          
        
        sin
        ⁡
        (
        2
        π
        n
        x
        )
        ∣
        n
        ∈
        
          N
        
        }
        ∪
        {
        
          
            2
          
        
        cos
        ⁡
        (
        2
        π
        n
        x
        )
        ∣
        n
        ∈
        
          N
        
        }
        ∪
        {
        1
        }
      
    
    {\displaystyle \{{\sqrt {2}}\sin(2\pi nx)\mid n\in \mathbb {N} \}\cup \{{\sqrt {2}}\cos(2\pi nx)\mid n\in \mathbb {N} \}\cup \{1\}}
  

forms a basis for L2[0,1].

See also
References
Itô, Kiyosi (1993). Encyclopedic Dictionary of Mathematics (2nd ed.). MIT Press. p. 1141. ISBN 0-262-59020-4.
In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.

Introduction
In the setting of supervised learning, a function of 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
  
 is to be learned, where 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is thought of as a space of inputs and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
  
 on 
  
    
      
        X
        ×
        Y
      
    
    {\displaystyle X\times Y}
  
. In reality, the learner never knows the true distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
  
 over instances. Instead, the learner usually has access to a training set of examples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
      
    
    {\displaystyle (x_{1},y_{1}),\ldots ,(x_{n},y_{n})}
  
. In this setting, the loss function is given as 
  
    
      
        V
        :
        Y
        ×
        Y
        →
        
          R
        
      
    
    {\displaystyle V:Y\times Y\to \mathbb {R} }
  
, such that 
  
    
      
        V
        (
        f
        (
        x
        )
        ,
        y
        )
      
    
    {\displaystyle V(f(x),y)}
  
 measures the difference between the predicted value 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 and the true value 
  
    
      
        y
      
    
    {\displaystyle y}
  
. The ideal goal is to select a function 
  
    
      
        f
        ∈
        
          
            H
          
        
      
    
    {\displaystyle f\in {\mathcal {H}}}
  
, where 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  
 is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.

Statistical view of online learning
In statistical learning models, the training sample 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},y_{i})}
  
 are assumed to have been drawn from the true distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
  
 and the objective is to minimize the expected "risk"

  
    
      
        I
        [
        f
        ]
        =
        
          E
        
        [
        V
        (
        f
        (
        x
        )
        ,
        y
        )
        ]
        =
        ∫
        V
        (
        f
        (
        x
        )
        ,
        y
        )
        
        d
        p
        (
        x
        ,
        y
        )
         
        .
      
    
    {\displaystyle I[f]=\mathbb {E} [V(f(x),y)]=\int V(f(x),y)\,dp(x,y)\ .}
  

A common paradigm in this situation is to estimate a function 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
  
 through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.
A purely online model in this category would learn based on just the new input 
  
    
      
        (
        
          x
          
            t
            +
            1
          
        
        ,
        
          y
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle (x_{t+1},y_{t+1})}
  
, the current best predictor 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
  
 and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where 
  
    
      
        
          f
          
            t
            +
            1
          
        
      
    
    {\displaystyle f_{t+1}}
  
 is permitted to depend on 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
  
 and all previous data points 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            t
          
        
        ,
        
          y
          
            t
          
        
        )
      
    
    {\displaystyle (x_{1},y_{1}),\ldots ,(x_{t},y_{t})}
  
. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.
A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of 
  
    
      
        b
        ≥
        1
      
    
    {\displaystyle b\geq 1}
  
 data points at a time, this can be considered as pseudo-online learning for 
  
    
      
        b
      
    
    {\displaystyle b}
  
 much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.

Example: linear least squares
The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.

Batch learning
Consider the setting of supervised learning with 
  
    
      
        f
      
    
    {\displaystyle f}
  
 being a linear function to be learned:

  
    
      
        f
        (
        
          x
          
            j
          
        
        )
        =
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        =
        w
        ⋅
        
          x
          
            j
          
        
      
    
    {\displaystyle f(x_{j})=\langle w,x_{j}\rangle =w\cdot x_{j}}
  

where 
  
    
      
        
          x
          
            j
          
        
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x_{j}\in \mathbb {R} ^{d}}
  
 is a vector of inputs (data points) and 
  
    
      
        w
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle w\in \mathbb {R} ^{d}}
  
 is a linear filter vector.
The goal is to compute the filter vector 
  
    
      
        w
      
    
    {\displaystyle w}
  
.
To this end, a square loss function 

  
    
      
        V
        (
        f
        (
        
          x
          
            j
          
        
        )
        ,
        
          y
          
            j
          
        
        )
        =
        (
        f
        (
        
          x
          
            j
          
        
        )
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
        =
        (
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\langle w,x_{j}\rangle -y_{j})^{2}}
  

is used to compute the vector 
  
    
      
        w
      
    
    {\displaystyle w}
  
 that minimizes the empirical loss

  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
        =
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        V
        (
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        ,
        
          y
          
            j
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        (
        
          x
          
            j
          
          
            
              T
            
          
        
        w
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle I_{n}[w]=\sum _{j=1}^{n}V(\langle w,x_{j}\rangle ,y_{j})=\sum _{j=1}^{n}(x_{j}^{\mathsf {T}}w-y_{j})^{2}}
  
 
where

  
    
      
        
          y
          
            j
          
        
        ∈
        
          R
        
        .
      
    
    {\displaystyle y_{j}\in \mathbb {R} .}
  

Let 
  
    
      
        X
      
    
    {\displaystyle X}
  
 be the 
  
    
      
        i
        ×
        d
      
    
    {\displaystyle i\times d}
  
 data matrix and 
  
    
      
        y
        ∈
        
          
            R
          
          
            i
          
        
      
    
    {\displaystyle y\in \mathbb {R} ^{i}}
  
 is the column vector of target values after the arrival of the first 
  
    
      
        i
      
    
    {\displaystyle i}
  
 data points.
Assuming that the covariance matrix 
  
    
      
        
          Σ
          
            i
          
        
        =
        
          X
          
            
              T
            
          
        
        X
      
    
    {\displaystyle \Sigma _{i}=X^{\mathsf {T}}X}
  
 is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution 
  
    
      
        
          f
          
            ∗
          
        
        (
        x
        )
        =
        ⟨
        
          w
          
            ∗
          
        
        ,
        x
        ⟩
      
    
    {\displaystyle f^{*}(x)=\langle w^{*},x\rangle }
  
 to the linear least squares problem is given by

  
    
      
        
          w
          
            ∗
          
        
        =
        (
        
          X
          
            
              T
            
          
        
        X
        
          )
          
            −
            1
          
        
        
          X
          
            
              T
            
          
        
        y
        =
        
          Σ
          
            i
          
          
            −
            1
          
        
        
          ∑
          
            j
            =
            1
          
          
            i
          
        
        
          x
          
            j
          
        
        
          y
          
            j
          
        
        .
      
    
    {\displaystyle w^{*}=(X^{\mathsf {T}}X)^{-1}X^{\mathsf {T}}y=\Sigma _{i}^{-1}\sum _{j=1}^{i}x_{j}y_{j}.}
  

Now, calculating the covariance matrix 
  
    
      
        
          Σ
          
            i
          
        
        =
        
          ∑
          
            j
            =
            1
          
          
            i
          
        
        
          x
          
            j
          
        
        
          x
          
            j
          
          
            
              T
            
          
        
      
    
    {\displaystyle \Sigma _{i}=\sum _{j=1}^{i}x_{j}x_{j}^{\mathsf {T}}}
  
 takes time 
  
    
      
        O
        (
        i
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(id^{2})}
  
, inverting the 
  
    
      
        d
        ×
        d
      
    
    {\displaystyle d\times d}
  
 matrix takes time 
  
    
      
        O
        (
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(d^{3})}
  
, while the rest of the multiplication takes time 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  
, giving a total time of 
  
    
      
        O
        (
        i
        
          d
          
            2
          
        
        +
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(id^{2}+d^{3})}
  
. When there are 
  
    
      
        n
      
    
    {\displaystyle n}
  
 total points in the dataset, to recompute the solution after the arrival of every datapoint 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  
, the naive approach will have a total complexity 
  
    
      
        O
        (
        
          n
          
            2
          
        
        
          d
          
            2
          
        
        +
        n
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{2}d^{2}+nd^{3})}
  
. Note that when storing the matrix 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
  
, then updating it at each step needs only adding 
  
    
      
        
          x
          
            i
            +
            1
          
        
        
          x
          
            i
            +
            1
          
          
            
              T
            
          
        
      
    
    {\displaystyle x_{i+1}x_{i+1}^{\mathsf {T}}}
  
, which takes 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  
 time, reducing the total time to 
  
    
      
        O
        (
        n
        
          d
          
            2
          
        
        +
        n
        
          d
          
            3
          
        
        )
        =
        O
        (
        n
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}
  
, but with an additional storage space of 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  
 to store 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
  
.

Online learning: recursive least squares
The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising 
  
    
      
        
          
            w
            
              0
            
          
          =
          0
          ∈
          
            
              R
            
            
              d
            
          
        
      
    
    {\displaystyle \textstyle w_{0}=0\in \mathbb {R} ^{d}}
  
 and 
  
    
      
        
          
            Γ
            
              0
            
          
          =
          I
          ∈
          
            
              R
            
            
              d
              ×
              d
            
          
        
      
    
    {\displaystyle \textstyle \Gamma _{0}=I\in \mathbb {R} ^{d\times d}}
  
, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:

  
    
      
        
          Γ
          
            i
          
        
        =
        
          Γ
          
            i
            −
            1
          
        
        −
        
          
            
              
                Γ
                
                  i
                  −
                  1
                
              
              
                x
                
                  i
                
              
              
                x
                
                  i
                
                
                  
                    T
                  
                
              
              
                Γ
                
                  i
                  −
                  1
                
              
            
            
              1
              +
              
                x
                
                  i
                
                
                  
                    T
                  
                
              
              
                Γ
                
                  i
                  −
                  1
                
              
              
                x
                
                  i
                
              
            
          
        
      
    
    {\displaystyle \Gamma _{i}=\Gamma _{i-1}-{\frac {\Gamma _{i-1}x_{i}x_{i}^{\mathsf {T}}\Gamma _{i-1}}{1+x_{i}^{\mathsf {T}}\Gamma _{i-1}x_{i}}}}
  

  
    
      
        
          w
          
            i
          
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          Γ
          
            i
          
        
        
          x
          
            i
          
        
        
          (
          
            
              x
              
                i
              
              
                
                  T
                
              
            
            
              w
              
                i
                −
                1
              
            
            −
            
              y
              
                i
              
            
          
          )
        
      
    
    {\displaystyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)}
  

The above iteration algorithm can be proved using induction on 
  
    
      
        i
      
    
    {\displaystyle i}
  
. The proof also shows that 
  
    
      
        
          Γ
          
            i
          
        
        =
        
          Σ
          
            i
          
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{i}=\Sigma _{i}^{-1}}
  
. One can look at RLS also in the context of adaptive filters (see RLS).
The complexity for 
  
    
      
        n
      
    
    {\displaystyle n}
  
 steps of this algorithm is 
  
    
      
        O
        (
        n
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(nd^{2})}
  
, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step 
  
    
      
        i
      
    
    {\displaystyle i}
  
 here are to store the matrix 
  
    
      
        
          Γ
          
            i
          
        
      
    
    {\displaystyle \Gamma _{i}}
  
, which is constant at 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  
. For the case when 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
  
 is not invertible, consider the regularised version of the problem loss function 
  
    
      
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          
            (
            
              
                x
                
                  j
                
                
                  
                    T
                  
                
              
              w
              −
              
                y
                
                  j
                
              
            
            )
          
          
            2
          
        
        +
        λ
        
          
            ‖
            w
            ‖
          
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \sum _{j=1}^{n}\left(x_{j}^{\mathsf {T}}w-y_{j}\right)^{2}+\lambda \left\|w\right\|_{2}^{2}}
  
. Then, it's easy to show that the same algorithm works with 
  
    
      
        
          Γ
          
            0
          
        
        =
        (
        I
        +
        λ
        I
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{0}=(I+\lambda I)^{-1}}
  
, and the iterations proceed to give 
  
    
      
        
          Γ
          
            i
          
        
        =
        (
        
          Σ
          
            i
          
        
        +
        λ
        I
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{i}=(\Sigma _{i}+\lambda I)^{-1}}
  
.

Stochastic gradient descent
When this 

  
    
      
        
          w
          
            i
          
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          Γ
          
            i
          
        
        
          x
          
            i
          
        
        
          (
          
            
              x
              
                i
              
              
                
                  T
                
              
            
            
              w
              
                i
                −
                1
              
            
            −
            
              y
              
                i
              
            
          
          )
        
      
    
    {\displaystyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)}
  
 
is replaced by

  
    
      
        
          w
          
            i
          
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          γ
          
            i
          
        
        
          x
          
            i
          
        
        
          (
          
            
              x
              
                i
              
              
                
                  T
                
              
            
            
              w
              
                i
                −
                1
              
            
            −
            
              y
              
                i
              
            
          
          )
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          γ
          
            i
          
        
        ∇
        V
        (
        ⟨
        
          w
          
            i
            −
            1
          
        
        ,
        
          x
          
            i
          
        
        ⟩
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle w_{i}=w_{i-1}-\gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{i}\rangle ,y_{i})}
  
 
or 
  
    
      
        
          Γ
          
            i
          
        
        ∈
        
          
            R
          
          
            d
            ×
            d
          
        
      
    
    {\displaystyle \Gamma _{i}\in \mathbb {R} ^{d\times d}}
  
 by 
  
    
      
        
          γ
          
            i
          
        
        ∈
        
          R
        
      
    
    {\displaystyle \gamma _{i}\in \mathbb {R} }
  
, this becomes the stochastic gradient descent algorithm. In this case, the complexity for 
  
    
      
        n
      
    
    {\displaystyle n}
  
 steps of this algorithm reduces to 
  
    
      
        O
        (
        n
        d
        )
      
    
    {\displaystyle O(nd)}
  
. The storage requirements at every step 
  
    
      
        i
      
    
    {\displaystyle i}
  
 are constant at 
  
    
      
        O
        (
        d
        )
      
    
    {\displaystyle O(d)}
  
.
However, the stepsize 
  
    
      
        
          γ
          
            i
          
        
      
    
    {\displaystyle \gamma _{i}}
  
 needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size 
  
    
      
        
          γ
          
            i
          
        
        ≈
        
          
            1
            
              i
            
          
        
        ,
      
    
    {\displaystyle \gamma _{i}\approx {\frac {1}{\sqrt {i}}},}
  
 one can prove the convergence of the average iterate 
  
    
      
        
          
            
              w
              ¯
            
          
          
            n
          
        
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            i
          
        
      
    
    {\textstyle {\overline {w}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}w_{i}}
  
. This setting is a special case of stochastic optimization, a well known problem in optimization.

Incremental stochastic gradient descent
In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is called incremental gradient method and corresponds to an iteration

  
    
      
        
          w
          
            i
          
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          γ
          
            i
          
        
        ∇
        V
        (
        ⟨
        
          w
          
            i
            −
            1
          
        
        ,
        
          x
          
            
              t
              
                i
              
            
          
        
        ⟩
        ,
        
          y
          
            
              t
              
                i
              
            
          
        
        )
      
    
    {\displaystyle w_{i}=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{t_{i}}\rangle ,y_{t_{i}})}
  
 
The main difference with the stochastic gradient method is that here a sequence 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  
 is chosen to decide which training point is visited in the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk. Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.

Kernel methods
Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method. This discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction  that if 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
 is the data matrix and 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  
 is the output after 
  
    
      
        i
      
    
    {\displaystyle i}
  
 steps of the SGD algorithm, then,

  
    
      
        
          w
          
            i
          
        
        =
        
          X
          
            i
          
          
            
              T
            
          
        
        
          c
          
            i
          
        
      
    
    {\displaystyle w_{i}=X_{i}^{\mathsf {T}}c_{i}}
  
 
where 
  
    
      
        
          c
          
            i
          
        
        =
        (
        (
        
          c
          
            i
          
        
        
          )
          
            1
          
        
        ,
        (
        
          c
          
            i
          
        
        
          )
          
            2
          
        
        ,
        .
        .
        .
        ,
        (
        
          c
          
            i
          
        
        
          )
          
            i
          
        
        )
        ∈
        
          
            R
          
          
            i
          
        
      
    
    {\displaystyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\in \mathbb {R} ^{i}}
  
 and the sequence 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
  
 satisfies the recursion:

  
    
      
        
          c
          
            0
          
        
        =
        0
      
    
    {\displaystyle c_{0}=0}
  

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            j
          
        
        =
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ,
        j
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        i
        −
        1
      
    
    {\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}
  
 and

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            i
          
        
        =
        
          γ
          
            i
          
        
        
          
            (
          
        
        
          y
          
            i
          
        
        −
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ⟨
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        ⟩
        
          
            )
          
        
      
    
    {\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x_{i}\rangle {\Big )}}
  

Notice that here 
  
    
      
        ⟨
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        ⟩
      
    
    {\displaystyle \langle x_{j},x_{i}\rangle }
  
 is just the standard Kernel on 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  
, and the predictor is of the form 

  
    
      
        
          f
          
            i
          
        
        (
        x
        )
        =
        ⟨
        
          w
          
            i
            −
            1
          
        
        ,
        x
        ⟩
        =
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ⟨
        
          x
          
            j
          
        
        ,
        x
        ⟩
        .
      
    
    {\displaystyle f_{i}(x)=\langle w_{i-1},x\rangle =\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x\rangle .}
  

Now, if a general kernel 
  
    
      
        K
      
    
    {\displaystyle K}
  
 is introduced instead and let the predictor be 

  
    
      
        
          f
          
            i
          
        
        (
        x
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        K
        (
        
          x
          
            j
          
        
        ,
        x
        )
      
    
    {\displaystyle f_{i}(x)=\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}
  

then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            i
          
        
        =
        
          γ
          
            i
          
        
        
          
            (
          
        
        
          y
          
            i
          
        
        −
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        K
        (
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        )
        
          
            )
          
        
      
    
    {\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\Big )}}
  

The above expression requires storing all the data for updating 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
  
. The total time complexity for the recursion when evaluating for the 
  
    
      
        n
      
    
    {\displaystyle n}
  
-th datapoint is 
  
    
      
        O
        (
        
          n
          
            2
          
        
        d
        k
        )
      
    
    {\displaystyle O(n^{2}dk)}
  
, where 
  
    
      
        k
      
    
    {\displaystyle k}
  
 is the cost of evaluating the kernel on a single pair of points. Thus, the use of the kernel has allowed the movement from a finite dimensional parameter space 
  
    
      
        
          
            w
            
              i
            
          
          ∈
          
            
              R
            
            
              d
            
          
        
      
    
    {\displaystyle \textstyle w_{i}\in \mathbb {R} ^{d}}
  
 to a possibly infinite dimensional feature represented by a kernel 
  
    
      
        K
      
    
    {\displaystyle K}
  
 by instead performing the recursion on the space of parameters 
  
    
      
        
          
            c
            
              i
            
          
          ∈
          
            
              R
            
            
              i
            
          
        
      
    
    {\displaystyle \textstyle c_{i}\in \mathbb {R} ^{i}}
  
, whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.

Online convex optimization
Online convex optimization (OCO)  is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:
For 
  
    
      
        t
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        T
      
    
    {\displaystyle t=1,2,...,T}
  

Learner receives input 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  

Learner outputs 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
  
 from a fixed convex set 
  
    
      
        S
      
    
    {\displaystyle S}
  

Nature sends back a convex loss function 
  
    
      
        
          v
          
            t
          
        
        :
        S
        →
        
          R
        
      
    
    {\displaystyle v_{t}:S\rightarrow \mathbb {R} }
  
.
Learner suffers loss 
  
    
      
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle v_{t}(w_{t})}
  
 and updates its model
The goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point 
  
    
      
        u
        ∈
        S
      
    
    {\displaystyle u\in S}
  
 in hindsight. As an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  
, and nature sends back the convex loss function 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        (
        ⟨
        w
        ,
        
          x
          
            t
          
        
        ⟩
        −
        
          y
          
            t
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle v_{t}(w)=(\langle w,x_{t}\rangle -y_{t})^{2}}
  
. Note here that 
  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
  
 is implicitly sent with 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
  
.
Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.
Some simple online convex optimisation algorithms are:

Follow the leader (FTL)
The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and round 
  
    
      
        t
      
    
    {\displaystyle t}
  
 is simply given by:

  
    
      
        
          w
          
            t
          
        
        =
        
          
            
              a
              r
              g
              
              m
              i
              n
            
          
          
            w
            ∈
            S
          
        
        ⁡
        
          ∑
          
            i
            =
            1
          
          
            t
            −
            1
          
        
        
          v
          
            i
          
        
        (
        w
        )
      
    
    {\displaystyle w_{t}=\mathop {\operatorname {arg\,min} } _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)}
  

This method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        
          
            ‖
            
              w
              −
              
                x
                
                  t
                
              
            
            ‖
          
          
            2
          
          
            2
          
        
      
    
    {\displaystyle v_{t}(w)=\left\|w-x_{t}\right\|_{2}^{2}}
  
), one can show a regret bound that grows as 
  
    
      
        log
        ⁡
        (
        T
        )
      
    
    {\displaystyle \log(T)}
  
. However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.

Follow the regularised leader (FTRL)
This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function 
  
    
      
        R
        :
        S
        →
        
          R
        
      
    
    {\displaystyle R:S\to \mathbb {R} }
  
 is chosen and learning performed in round t as follows:

  
    
      
        
          w
          
            t
          
        
        =
        
          
            
              a
              r
              g
              
              m
              i
              n
            
          
          
            w
            ∈
            S
          
        
        ⁡
        
          ∑
          
            i
            =
            1
          
          
            t
            −
            1
          
        
        
          v
          
            i
          
        
        (
        w
        )
        +
        R
        (
        w
        )
      
    
    {\displaystyle w_{t}=\mathop {\operatorname {arg\,min} } _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)+R(w)}
  

As a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        ⟨
        w
        ,
        
          z
          
            t
          
        
        ⟩
      
    
    {\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }
  
. Also, let 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  
. Suppose the regularisation function 
  
    
      
        R
        (
        w
        )
        =
        
          
            1
            
              2
              η
            
          
        
        
          
            ‖
            w
            ‖
          
          
            2
          
          
            2
          
        
      
    
    {\textstyle R(w)={\frac {1}{2\eta }}\left\|w\right\|_{2}^{2}}
  
 is chosen for some positive number 
  
    
      
        η
      
    
    {\displaystyle \eta }
  
. Then, one can show that the regret minimising iteration becomes 

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        −
        η
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          z
          
            i
          
        
        =
        
          w
          
            t
          
        
        −
        η
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=-\eta \sum _{i=1}^{t}z_{i}=w_{t}-\eta z_{t}}
  

Note that this can be rewritten as 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        η
        ∇
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle w_{t+1}=w_{t}-\eta \nabla v_{t}(w_{t})}
  
, which looks exactly like online gradient descent.
If S is instead some convex subspace of 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  
, S would need to be projected onto, leading to the modified update rule

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          Π
          
            S
          
        
        (
        −
        η
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          z
          
            i
          
        
        )
        =
        
          Π
          
            S
          
        
        (
        η
        
          θ
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle w_{t+1}=\Pi _{S}(-\eta \sum _{i=1}^{t}z_{i})=\Pi _{S}(\eta \theta _{t+1})}
  

This algorithm is known as lazy projection, as the vector 
  
    
      
        
          θ
          
            t
            +
            1
          
        
      
    
    {\displaystyle \theta _{t+1}}
  
 accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
  
, and thus the average regret goes to 0 as desired.

Online subgradient descent (OSD)
The above proved a regret bound for linear loss functions 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        ⟨
        w
        ,
        
          z
          
            t
          
        
        ⟩
      
    
    {\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }
  
. To generalise the algorithm to any convex loss function, the subgradient 
  
    
      
        ∂
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle \partial v_{t}(w_{t})}
  
 of 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
  
 is used as a linear approximation to 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
  
 near 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
  
, leading to the online subgradient descent algorithm:
Initialise parameter 
  
    
      
        η
        ,
        
          w
          
            1
          
        
        =
        0
      
    
    {\displaystyle \eta ,w_{1}=0}
  

For 
  
    
      
        t
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        T
      
    
    {\displaystyle t=1,2,...,T}
  

Predict using 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
  
, receive 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
  
 from nature.
Choose 
  
    
      
        
          z
          
            t
          
        
        ∈
        ∂
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle z_{t}\in \partial v_{t}(w_{t})}
  

If 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  
, update as 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        η
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=w_{t}-\eta z_{t}}
  

If 
  
    
      
        S
        ⊂
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S\subset \mathbb {R} ^{d}}
  
, project cumulative gradients onto 
  
    
      
        S
      
    
    {\displaystyle S}
  
 i.e. 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          Π
          
            S
          
        
        (
        η
        
          θ
          
            t
            +
            1
          
        
        )
        ,
        
          θ
          
            t
            +
            1
          
        
        =
        
          θ
          
            t
          
        
        +
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=\Pi _{S}(\eta \theta _{t+1}),\theta _{t+1}=\theta _{t}+z_{t}}
  

One can use the OSD algorithm to derive 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
  
 regret bounds for the online version of SVM's for classification, which use the hinge loss
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        max
        {
        0
        ,
        1
        −
        
          y
          
            t
          
        
        (
        w
        ⋅
        
          x
          
            t
          
        
        )
        }
      
    
    {\displaystyle v_{t}(w)=\max\{0,1-y_{t}(w\cdot x_{t})\}}

Other algorithms
Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent. The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm. For the Euclidean regularisation, one can show a regret bound of 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
  
, which can be improved further to a 
  
    
      
        O
        (
        log
        ⁡
        T
        )
      
    
    {\displaystyle O(\log T)}
  
 for strongly convex and exp-concave loss functions.

Continual learning
Continual learning means constantly improving the learned model by processing continuous streams of information. Continual learning capabilities are essential for software systems and autonomous agents interacting in an ever changing real world. However, continual learning is a challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting.

Interpretations of online learning
The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions 
  
    
      
        
          f
          
            1
          
        
        ,
        
          f
          
            2
          
        
        ,
        …
        ,
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{1},f_{2},\ldots ,f_{n}}
  
. The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by

  
    
      
        
          w
          
            t
          
        
        =
        
          w
          
            t
            −
            1
          
        
        −
        
          γ
          
            t
          
        
        ∇
        V
        (
        ⟨
        
          w
          
            t
            −
            1
          
        
        ,
        
          x
          
            t
          
        
        ⟩
        ,
        
          y
          
            t
          
        
        )
      
    
    {\displaystyle w_{t}=w_{t-1}-\gamma _{t}\nabla V(\langle w_{t-1},x_{t}\rangle ,y_{t})}
  

The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
  
 defined above. Indeed, in the case of an infinite stream of data, since the examples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        (
        
          x
          
            2
          
        
        ,
        
          y
          
            2
          
        
        )
        ,
        …
      
    
    {\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\ldots }
  
 are assumed to be drawn i.i.d. from the distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
  
, the sequence of gradients of 
  
    
      
        V
        (
        ⋅
        ,
        ⋅
        )
      
    
    {\displaystyle V(\cdot ,\cdot )}
  
 in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
  
 and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation 
  
    
      
        I
        [
        
          w
          
            t
          
        
        ]
        −
        I
        [
        
          w
          
            ∗
          
        
        ]
      
    
    {\displaystyle I[w_{t}]-I[w^{\ast }]}
  
, where 
  
    
      
        
          w
          
            ∗
          
        
      
    
    {\displaystyle w^{\ast }}
  
 is the minimizer of 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
  
. This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.
The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method. In this case, one instead looks at the empirical risk:

  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        ,
        
          y
          
            i
          
        
        )
         
        .
      
    
    {\displaystyle I_{n}[w]={\frac {1}{n}}\sum _{i=1}^{n}V(\langle w,x_{i}\rangle ,y_{i})\ .}
  

Since the gradients of 
  
    
      
        V
        (
        ⋅
        ,
        ⋅
        )
      
    
    {\displaystyle V(\cdot ,\cdot )}
  
 in the incremental gradient descent iterations are also stochastic estimates of the gradient of 
  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
      
    
    {\displaystyle I_{n}[w]}
  
, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations 
  
    
      
        
          I
          
            n
          
        
        [
        
          w
          
            t
          
        
        ]
        −
        
          I
          
            n
          
        
        [
        
          w
          
            n
          
          
            ∗
          
        
        ]
      
    
    {\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\ast }]}
  
, where 
  
    
      
        
          w
          
            n
          
          
            ∗
          
        
      
    
    {\displaystyle w_{n}^{\ast }}
  
 is the minimizer of 
  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
      
    
    {\displaystyle I_{n}[w]}
  
.

Implementations
Vowpal Wabbit: Open-source fast out-of-core online learning system which is notable for supporting a number of machine learning reductions, importance weighting and a selection of different loss functions and optimisation algorithms. It uses the hashing trick for bounding the size of the set of features independent of the amount of training data.
scikit-learn: Provides out-of-core implementations of algorithms for
Classification: Perceptron, SGD classifier, Naive bayes classifier.
Regression: SGD Regressor, Passive Aggressive regressor.
Clustering: Mini-batch k-means.
Feature extraction: Mini-batch dictionary learning, Incremental PCA.

See also
Learning paradigms

Incremental learning
Lazy learning
Offline learning, the opposite model
Reinforcement learning
Multi-armed bandit
Supervised learning
General algorithms

Online algorithm
Online optimization
Streaming algorithm
Stochastic gradient descent
Learning models

Adaptive Resonance Theory
Hierarchical temporal memory
k-nearest neighbor algorithm
Learning vector quantization
Perceptron

References
External links
6.883: Online Methods in Machine Learning: Theory and Applications. Alexander Rakhlin. MIT
Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.
While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks.

Internal covariate shift
Each layer of a neural network has inputs with a corresponding distribution, which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data. The effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift. Although a clear-cut precise definition seems to be missing, the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training.
Batch normalization was initially proposed to mitigate internal covariate shift. During the training stage of networks, as the parameters of the preceding layers change, the distribution of inputs to the current layer changes accordingly, such that the current layer needs to constantly readjust to new distributions. This problem is especially severe for deep networks, because small changes in shallower hidden layers will be amplified as they propagate within the network, resulting in significant shift in deeper hidden layers. Therefore, the method of batch normalization is proposed to reduce these unwanted shifts to speed up training and to produce more reliable models.
Besides reducing internal covariate shift, batch normalization is believed to introduce many other benefits. With this additional operation, the network can use higher learning rate without vanishing or exploding gradients. Furthermore, batch normalization seems to have a regularizing effect such that the network improves its generalization properties, and it is thus unnecessary to use dropout to mitigate overfitting. It has also been observed that the network becomes more robust to different initialization schemes and learning rates while using batch normalization.

Procedures
Transformation
In a neural network, batch normalization is achieved through a normalization step that fixes the means and variances of each layer's inputs. Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process.
Let us use B to denote a mini-batch of size m of the entire training set. The empirical mean and variance of B could thus be denoted as

  
    
      
        
          μ
          
            B
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle \mu _{B}={\frac {1}{m}}\sum _{i=1}^{m}x_{i}}
  
 and 
  
    
      
        
          σ
          
            B
          
          
            2
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        (
        
          x
          
            i
          
        
        −
        
          μ
          
            B
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle \sigma _{B}^{2}={\frac {1}{m}}\sum _{i=1}^{m}(x_{i}-\mu _{B})^{2}}
  
.
For a layer of the network with d-dimensional input, 
  
    
      
        x
        =
        (
        
          x
          
            (
            1
            )
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            (
            d
            )
          
        
        )
      
    
    {\displaystyle x=(x^{(1)},...,x^{(d)})}
  
, each dimension of its input is then normalized (i.e. re-centered and re-scaled) separately,

  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            i
          
          
            (
            k
            )
          
        
        =
        
          
            
              
                x
                
                  i
                
                
                  (
                  k
                  )
                
              
              −
              
                μ
                
                  B
                
                
                  (
                  k
                  )
                
              
            
            
              
                
                  (
                  
                    σ
                    
                      B
                    
                    
                      (
                      k
                      )
                    
                  
                  )
                
                
                  2
                
              
              +
              ϵ
            
          
        
      
    
    {\displaystyle {\hat {x}}_{i}^{(k)}={\frac {x_{i}^{(k)}-\mu _{B}^{(k)}}{\sqrt {\left(\sigma _{B}^{(k)}\right)^{2}+\epsilon }}}}
  
, where 
  
    
      
        k
        ∈
        [
        1
        ,
        d
        ]
      
    
    {\displaystyle k\in [1,d]}
  
 and  
  
    
      
        i
        ∈
        [
        1
        ,
        m
        ]
      
    
    {\displaystyle i\in [1,m]}
  
; 
  
    
      
        
          μ
          
            B
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle \mu _{B}^{(k)}}
  
 and 
  
    
      
        
          σ
          
            B
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle \sigma _{B}^{(k)}}
  
 are the per-dimension mean and standard deviation, respectively.

  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  
 is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation 
  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle {\hat {x}}^{(k)}}
  
have zero mean and unit variance, if 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  
 is not taken into account. To restore the representation power of the network, a transformation step then follows as

  
    
      
        
          y
          
            i
          
          
            (
            k
            )
          
        
        =
        
          γ
          
            (
            k
            )
          
        
        
          
            
              
                x
                ^
              
            
          
          
            i
          
          
            (
            k
            )
          
        
        +
        
          β
          
            (
            k
            )
          
        
      
    
    {\displaystyle y_{i}^{(k)}=\gamma ^{(k)}{\hat {x}}_{i}^{(k)}+\beta ^{(k)}}
  
,
where the parameters 
  
    
      
        
          γ
          
            (
            k
            )
          
        
      
    
    {\displaystyle \gamma ^{(k)}}
  
 and 
  
    
      
        
          β
          
            (
            k
            )
          
        
      
    
    {\displaystyle \beta ^{(k)}}
  
 are subsequently learned in the optimization process.
Formally, the operation that implements batch normalization is a transform 
  
    
      
        B
        
          N
          
            
              γ
              
                (
                k
                )
              
            
            ,
            
              β
              
                (
                k
                )
              
            
          
        
        :
        
          x
          
            1...
            m
          
          
            (
            k
            )
          
        
        →
        
          y
          
            1...
            m
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle BN_{\gamma ^{(k)},\beta ^{(k)}}:x_{1...m}^{(k)}\rightarrow y_{1...m}^{(k)}}
  
 called the Batch Normalizing transform. The output of the BN transform 
  
    
      
        
          y
          
            (
            k
            )
          
        
        =
        B
        
          N
          
            
              γ
              
                (
                k
                )
              
            
            ,
            
              β
              
                (
                k
                )
              
            
          
        
        (
        
          x
          
            (
            k
            )
          
        
        )
      
    
    {\displaystyle y^{(k)}=BN_{\gamma ^{(k)},\beta ^{(k)}}(x^{(k)})}
  
 is then passed to other network layers, while the normalized output  
  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            i
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle {\hat {x}}_{i}^{(k)}}
  
 remains internal to the current layer.

Backpropagation
The described BN transform is a differentiable operation, and the gradient of the loss l  with respect to the different parameters can be computed directly with the chain rule.
Specifically, 
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial y_{i}^{(k)}}}}
  
 depends on the choice of activation function, and the gradient against other parameters could be expressed as a function of 
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial y_{i}^{(k)}}}}
  
:

  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                
                  
                    
                      x
                      ^
                    
                  
                
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        =
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        
          γ
          
            (
            k
            )
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial {\hat {x}}_{i}^{(k)}}}={\frac {\partial l}{\partial y_{i}^{(k)}}}\gamma ^{(k)}}
  
,

  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                γ
                
                  (
                  k
                  )
                
              
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        
          
            
              
                x
                ^
              
            
          
          
            i
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial \gamma ^{(k)}}}=\sum _{i=1}^{m}{\frac {\partial l}{\partial y_{i}^{(k)}}}{\hat {x}}_{i}^{(k)}}
  
, 
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                β
                
                  (
                  k
                  )
                
              
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial \beta ^{(k)}}}=\sum _{i=1}^{m}{\frac {\partial l}{\partial y_{i}^{(k)}}}}
  
,
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                σ
                
                  B
                
                
                  (
                  k
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        (
        
          x
          
            i
          
          
            (
            k
            )
          
        
        −
        
          μ
          
            B
          
          
            (
            k
            )
          
        
        )
        
          (
          
            −
            
              
                
                  γ
                  
                    (
                    k
                    )
                  
                
                2
              
            
            (
            
              σ
              
                B
              
              
                (
                k
                
                  )
                  
                    2
                  
                
              
            
            +
            ϵ
            
              )
              
                −
                3
                
                  /
                
                2
              
            
          
          )
        
      
    
    {\displaystyle {\frac {\partial l}{\partial \sigma _{B}^{(k)^{2}}}}=\sum _{i=1}^{m}{\frac {\partial l}{\partial y_{i}^{(k)}}}(x_{i}^{(k)}-\mu _{B}^{(k)})\left(-{\frac {\gamma ^{(k)}}{2}}(\sigma _{B}^{(k)^{2}}+\epsilon )^{-3/2}\right)}
  
, 
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                μ
                
                  B
                
                
                  (
                  k
                  )
                
              
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            
              ∂
              l
            
            
              ∂
              
                y
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        
          
            
              −
              
                γ
                
                  (
                  k
                  )
                
              
            
            
              
                σ
                
                  B
                
                
                  (
                  k
                  
                    )
                    
                      2
                    
                  
                
              
              +
              ϵ
            
          
        
        +
        
          
            
              ∂
              l
            
            
              ∂
              
                σ
                
                  B
                
                
                  (
                  k
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        (
        −
        2
        )
        ⋅
        (
        
          x
          
            i
          
          
            (
            k
            )
          
        
        −
        
          μ
          
            B
          
          
            (
            k
            )
          
        
        )
      
    
    {\displaystyle {\frac {\partial l}{\partial \mu _{B}^{(k)}}}=\sum _{i=1}^{m}{\frac {\partial l}{\partial y_{i}^{(k)}}}{\frac {-\gamma ^{(k)}}{\sqrt {\sigma _{B}^{(k)^{2}}+\epsilon }}}+{\frac {\partial l}{\partial \sigma _{B}^{(k)^{2}}}}{\frac {1}{m}}\sum _{i=1}^{m}(-2)\cdot (x_{i}^{(k)}-\mu _{B}^{(k)})}
  
,
and 
  
    
      
        
          
            
              ∂
              l
            
            
              ∂
              
                x
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        =
        
          
            
              ∂
              l
            
            
              ∂
              
                
                  
                    
                      x
                      ^
                    
                  
                
                
                  i
                
                
                  (
                  k
                  )
                
              
            
          
        
        
          
            1
            
              
                σ
                
                  B
                
                
                  (
                  k
                  
                    )
                    
                      2
                    
                  
                
              
              +
              ϵ
            
          
        
        +
        
          
            
              ∂
              l
            
            
              ∂
              
                σ
                
                  B
                
                
                  (
                  k
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        
          
            
              2
              (
              
                x
                
                  i
                
                
                  (
                  k
                  )
                
              
              −
              
                μ
                
                  B
                
                
                  (
                  k
                  )
                
              
              )
            
            m
          
        
        +
        
          
            
              ∂
              l
            
            
              ∂
              
                μ
                
                  B
                
                
                  (
                  k
                  )
                
              
            
          
        
        
          
            1
            m
          
        
      
    
    {\displaystyle {\frac {\partial l}{\partial x_{i}^{(k)}}}={\frac {\partial l}{\partial {\hat {x}}_{i}^{(k)}}}{\frac {1}{\sqrt {\sigma _{B}^{(k)^{2}}+\epsilon }}}+{\frac {\partial l}{\partial \sigma _{B}^{(k)^{2}}}}{\frac {2(x_{i}^{(k)}-\mu _{B}^{(k)})}{m}}+{\frac {\partial l}{\partial \mu _{B}^{(k)}}}{\frac {1}{m}}}
  
.

Inference
During the training stage, the normalization steps depend on the mini-batches to ensure efficient and reliable training. However, in the inference stage, this dependence is not useful any more. Instead, the normalization step in this stage is computed with the population statistics such that the output could depend on the input in a deterministic manner. The population mean, 
  
    
      
        E
        [
        
          x
          
            (
            k
            )
          
        
        ]
      
    
    {\displaystyle E[x^{(k)}]}
  
, and variance, 
  
    
      
        Var
        ⁡
        [
        
          x
          
            (
            k
            )
          
        
        ]
      
    
    {\displaystyle \operatorname {Var} [x^{(k)}]}
  
, are computed as:

  
    
      
        E
        [
        
          x
          
            (
            k
            )
          
        
        ]
        =
        
          E
          
            B
          
        
        [
        
          μ
          
            B
          
          
            (
            k
            )
          
        
        ]
      
    
    {\displaystyle E[x^{(k)}]=E_{B}[\mu _{B}^{(k)}]}
  
, and 
  
    
      
        Var
        ⁡
        [
        
          x
          
            (
            k
            )
          
        
        ]
        =
        
          
            m
            
              m
              −
              1
            
          
        
        
          E
          
            B
          
        
        [
        
          
            (
            
              σ
              
                B
              
              
                (
                k
                )
              
            
            )
          
          
            2
          
        
        ]
      
    
    {\displaystyle \operatorname {Var} [x^{(k)}]={\frac {m}{m-1}}E_{B}[\left(\sigma _{B}^{(k)}\right)^{2}]}
  
.
The population statistics thus is a complete representation of the mini-batches.
The BN transform in the inference step thus becomes

  
    
      
        
          y
          
            (
            k
            )
          
        
        =
        B
        
          N
          
            
              γ
              
                (
                k
                )
              
            
            ,
            
              β
              
                (
                k
                )
              
            
          
          
            inf
          
        
        (
        
          x
          
            (
            k
            )
          
        
        )
        =
        
          γ
          
            (
            k
            )
          
        
        
          
            
              
                x
                
                  (
                  k
                  )
                
              
              −
              E
              [
              
                x
                
                  (
                  k
                  )
                
              
              ]
            
            
              Var
              ⁡
              [
              
                x
                
                  (
                  k
                  )
                
              
              ]
              +
              ϵ
            
          
        
        +
        
          β
          
            (
            k
            )
          
        
      
    
    {\displaystyle y^{(k)}=BN_{\gamma ^{(k)},\beta ^{(k)}}^{\text{inf}}(x^{(k)})=\gamma ^{(k)}{\frac {x^{(k)}-E[x^{(k)}]}{\sqrt {\operatorname {Var} [x^{(k)}]+\epsilon }}}+\beta ^{(k)}}
  
,
where 
  
    
      
        
          y
          
            (
            k
            )
          
        
      
    
    {\displaystyle y^{(k)}}
  
 is passed on to future layers instead of 
  
    
      
        
          x
          
            (
            k
            )
          
        
      
    
    {\displaystyle x^{(k)}}
  
. Since the parameters are fixed in this transformation, the batch normalization procedure is essentially applying a linear transform to the activation.

Theory
Although batch normalization has become popular due to its strong empirical performance, the working mechanism of the method is not yet well-understood. The explanation made in the original paper was that batch norm works by reducing internal covariate shift, but this has been challenged by more recent work. One experiment trained a VGG-16 network under 3 different training regimes: standard (no batch norm), batch norm, and batch norm with noise added to each layer during training. In the third model, the noise has non-zero mean and non-unit variance, i.e. it explicitly introduces covariate shift. Despite this, it showed similar accuracy to the second model, and both performed better than the first, suggesting that covariate shift is not the reason that batch norm improves performance.
Using batch normalization causes the items in a batch to no longer be iid, which can lead to difficulties in training due to lower quality gradient estimation.

Smoothness
One alternative explanation, is that the improvement with batch normalization is instead due to it producing a smoother parameter space and smoother gradients, as formalized by a smaller Lipschitz constant. 
Consider two identical networks, one contains batch normalization layers and the other does not, the behaviors of these two networks are then compared. Denote the loss functions as 
  
    
      
        
          
            
              L
              ^
            
          
        
      
    
    {\displaystyle {\hat {L}}}
  
 and 
  
    
      
        L
      
    
    {\displaystyle L}
  
, respectively. Let the input to both networks be 
  
    
      
        x
      
    
    {\displaystyle x}
  
, and the output be 
  
    
      
        y
      
    
    {\displaystyle y}
  
, for which 
  
    
      
        y
        =
        W
        x
      
    
    {\displaystyle y=Wx}
  
, where 
  
    
      
        W
      
    
    {\displaystyle W}
  
 is the layer weights. For the second network, 
  
    
      
        y
      
    
    {\displaystyle y}
  
 additionally goes through a batch normalization layer. Denote the normalized activation as 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  
, which has zero mean and unit variance. Let the transformed activation be 
  
    
      
        z
        =
        γ
        
          
            
              y
              ^
            
          
        
        +
        β
      
    
    {\displaystyle z=\gamma {\hat {y}}+\beta }
  
, and suppose 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 and 
  
    
      
        β
      
    
    {\displaystyle \beta }
  
 are constants. Finally, denote the standard deviation over a mini-batch 
  
    
      
        
          
            
              
                y
                
                  j
                
              
              ^
            
          
        
        ∈
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle {\hat {y_{j}}}\in \mathbb {R} ^{m}}
  
 as 
  
    
      
        
          σ
          
            j
          
        
      
    
    {\displaystyle \sigma _{j}}
  
.
First, it can be shown that the gradient magnitude of a batch normalized network, 
  
    
      
        
          |
        
        
          |
        
        
          ▽
          
            
              y
              
                i
              
            
          
        
        
          
            
              L
              ^
            
          
        
        
          |
        
        
          |
        
      
    
    {\displaystyle ||\triangledown _{y_{i}}{\hat {L}}||}
  
, is bounded, with the bound expressed as

  
    
      
        
          |
        
        
          |
        
        
          ▽
          
            
              y
              
                i
              
            
          
        
        
          
            
              L
              ^
            
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
        ≤
        
          
            
              γ
              
                2
              
            
            
              σ
              
                j
              
              
                2
              
            
          
        
        
          
            (
          
        
        
          |
        
        
          |
        
        
          ▽
          
            
              y
              
                i
              
            
          
        
        L
        
          |
        
        
          
            |
          
          
            2
          
        
        −
        
          
            1
            m
          
        
        ⟨
        1
        ,
        
          ▽
          
            
              y
              
                i
              
            
          
        
        L
        
          ⟩
          
            2
          
        
        −
        
          
            1
            m
          
        
        ⟨
        
          ▽
          
            
              y
              
                i
              
            
          
        
        L
        ,
        
          
            
              
                y
                ^
              
            
          
          
            j
          
        
        
          ⟩
          
            2
          
        
        
          
            )
          
        
      
    
    {\displaystyle ||\triangledown _{y_{i}}{\hat {L}}||^{2}\leq {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}{\Bigg (}||\triangledown _{y_{i}}L||^{2}-{\frac {1}{m}}\langle 1,\triangledown _{y_{i}}L\rangle ^{2}-{\frac {1}{m}}\langle \triangledown _{y_{i}}L,{\hat {y}}_{j}\rangle ^{2}{\bigg )}}
  
.
Since the gradient magnitude represents the Lipschitzness of the loss, this relationship indicates that a batch normalized network could achieve greater Lipschitzness comparatively. Notice that the bound gets tighter when the gradient 
  
    
      
        
          ▽
          
            
              y
              
                i
              
            
          
        
        
          
            
              L
              ^
            
          
        
      
    
    {\displaystyle \triangledown _{y_{i}}{\hat {L}}}
  
 correlates with the activation 
  
    
      
        
          
            
              
                y
                
                  i
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {y_{i}}}}
  
, which is a common phenomena. The scaling of 
  
    
      
        
          
            
              γ
              
                2
              
            
            
              σ
              
                j
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}}
  
 is also significant, since the variance is often large.
Secondly, the quadratic form of the loss Hessian with respect to activation in the gradient direction can be bounded as

  
    
      
        (
        
          ▽
          
            
              y
              
                j
              
            
          
        
        
          
            
              L
              ^
            
          
        
        
          )
          
            T
          
        
        
          
            
              ∂
              
                
                  
                    L
                    ^
                  
                
              
            
            
              ∂
              
                y
                
                  j
                
              
              ∂
              
                y
                
                  j
                
              
            
          
        
        (
        
          ▽
          
            
              y
              
                j
              
            
          
        
        
          
            
              L
              ^
            
          
        
        )
        ≤
        
          
            
              γ
              
                2
              
            
            
              σ
              
                2
              
            
          
        
        
          
            (
          
        
        
          
            
              ∂
              
                
                  
                    L
                    ^
                  
                
              
            
            
              ∂
              
                y
                
                  j
                
              
            
          
        
        
          
            
              )
            
          
          
            T
          
        
        
          
            (
          
        
        
          
            
              ∂
              L
            
            
              ∂
              
                y
                
                  j
                
              
              ∂
              
                y
                
                  j
                
              
            
          
        
        
          
            )
          
        
        
          
            (
          
        
        
          
            
              ∂
              
                
                  
                    L
                    ^
                  
                
              
            
            
              ∂
              
                y
                
                  j
                
              
            
          
        
        
          
            )
          
        
        −
        
          
            γ
            
              m
              
                σ
                
                  2
                
              
            
          
        
        ⟨
        
          ▽
          
            
              y
              
                j
              
            
          
        
        L
        ,
        
          
            
              
                y
                
                  j
                
              
              ^
            
          
        
        ⟩
        
          
            |
          
        
        
          
            |
          
        
        
          
            
              ∂
              
                
                  
                    L
                    ^
                  
                
              
            
            
              ∂
              
                y
                
                  j
                
              
            
          
        
        
          
            |
          
        
        
          
            
              |
            
          
          
            2
          
        
      
    
    {\displaystyle (\triangledown _{y_{j}}{\hat {L}})^{T}{\frac {\partial {\hat {L}}}{\partial y_{j}\partial y_{j}}}(\triangledown _{y_{j}}{\hat {L}})\leq {\frac {\gamma ^{2}}{\sigma ^{2}}}{\bigg (}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg )}^{T}{\bigg (}{\frac {\partial L}{\partial y_{j}\partial y_{j}}}{\bigg )}{\bigg (}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg )}-{\frac {\gamma }{m\sigma ^{2}}}\langle \triangledown _{y_{j}}L,{\hat {y_{j}}}\rangle {\bigg |}{\bigg |}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg |}{\bigg |}^{2}}
  
.
The scaling of 
  
    
      
        
          
            
              γ
              
                2
              
            
            
              σ
              
                j
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}}
  
 indicates that the loss Hessian is resilient to the mini-batch variance, whereas the second term on the right hand side suggests that it becomes smoother when the Hessian and the inner product are non-negative. If the loss is locally convex, then the Hessian is positive semi-definite, while the inner product is positive if 
  
    
      
        
          
            
              
                g
                
                  j
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {g_{j}}}}
  
 is in the direction towards the minimum of the loss. It could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer.
It then follows to translate the bounds related to the loss with respect to the normalized activation to a bound on the loss with respect to the network weights:

  
    
      
        
          
            
              
                g
                
                  j
                
              
              ^
            
          
        
        ≤
        
          
            
              γ
              
                2
              
            
            
              σ
              
                j
              
              
                2
              
            
          
        
        (
        
          g
          
            j
          
          
            2
          
        
        −
        m
        
          μ
          
            
              g
              
                j
              
            
          
          
            2
          
        
        −
        
          λ
          
            2
          
        
        ⟨
        
          ▽
          
            
              y
              
                j
              
            
          
        
        L
        ,
        
          
            
              
                y
                ^
              
            
          
          
            j
          
        
        
          ⟩
          
            2
          
        
        )
      
    
    {\displaystyle {\hat {g_{j}}}\leq {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}(g_{j}^{2}-m\mu _{g_{j}}^{2}-\lambda ^{2}\langle \triangledown _{y_{j}}L,{\hat {y}}_{j}\rangle ^{2})}
  
, where 
  
    
      
        
          g
          
            j
          
        
        =
        m
        a
        
          x
          
            
              |
            
            
              |
            
            X
            
              |
            
            
              |
            
            ≤
            λ
          
        
        
          |
        
        
          |
        
        
          ▽
          
            W
          
        
        L
        
          |
        
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle g_{j}=max_{||X||\leq \lambda }||\triangledown _{W}L||^{2}}
  
 and 
  
    
      
        
          
            
              
                g
                ^
              
            
          
          
            j
          
        
        =
        m
        a
        
          x
          
            
              |
            
            
              |
            
            X
            
              |
            
            
              |
            
            ≤
            λ
          
        
        
          |
        
        
          |
        
        
          ▽
          
            W
          
        
        
          
            
              L
              ^
            
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle {\hat {g}}_{j}=max_{||X||\leq \lambda }||\triangledown _{W}{\hat {L}}||^{2}}
  
.
In addition to the smoother landscape, it is further shown that batch normalization could result in a better initialization with the following inequality:

  
    
      
        
          |
        
        
          |
        
        
          W
          
            0
          
        
        −
        
          
            
              
                W
                ^
              
            
          
          
            ∗
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
        ≤
        
          |
        
        
          |
        
        
          W
          
            0
          
        
        −
        
          W
          
            ∗
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
        −
        
          
            1
            
              
                |
              
              
                |
              
              
                W
                
                  ∗
                
              
              
                |
              
              
                
                  |
                
                
                  2
                
              
            
          
        
        (
        
          |
        
        
          |
        
        
          W
          
            ∗
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
        −
        ⟨
        
          W
          
            ∗
          
        
        ,
        
          W
          
            0
          
        
        ⟩
        
          )
          
            2
          
        
      
    
    {\displaystyle ||W_{0}-{\hat {W}}^{*}||^{2}\leq ||W_{0}-W^{*}||^{2}-{\frac {1}{||W^{*}||^{2}}}(||W^{*}||^{2}-\langle W^{*},W_{0}\rangle )^{2}}
  
, where 
  
    
      
        
          W
          
            ∗
          
        
      
    
    {\displaystyle W^{*}}
  
 and 
  
    
      
        
          
            
              
                W
                ^
              
            
          
          
            ∗
          
        
      
    
    {\displaystyle {\hat {W}}^{*}}
  
 are the local optimal weights for the two networks, respectively.
Some scholars argue that the above analysis cannot fully capture the performance of batch normalization, because the proof only concerns the largest eigenvalue, or equivalently, one direction in the landscape at all points. It is suggested that the complete eigenspectrum needs to be taken into account to make a conclusive analysis.

Measure
Since it is hypothesized that batch normalization layers could reduce internal covariate shift, an experiment is set up to measure quantitatively how much covariate shift is reduced. First, the notion of internal covariate shift needs to be defined mathematically. Specifically, to quantify the adjustment that a layer's parameters make in response to updates in previous layers, the correlation between the gradients of the loss before and after all previous layers are updated is measured, since gradients could capture the shifts from the first-order training method. If the shift introduced by the changes in previous layers is small, then the correlation between the gradients would be close to 1.
The correlation between the gradients are computed for four models: a standard VGG network, a VGG network with batch normalization layers, a 25-layer deep linear network (DLN) trained with full-batch gradient descent, and a DLN network with batch normalization layers. Interestingly, it is shown that the standard VGG and DLN models both have higher correlations of gradients compared with their counterparts, indicating that the additional batch normalization layers are not reducing internal covariate shift.

Vanishing/exploding gradients
Even though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems, a deep batchnorm network in fact suffers from gradient explosion at initialization time, no matter what it uses for nonlinearity. Thus the optimization landscape is very far from smooth for a randomly initialized, deep batchnorm network.
More precisely, if the network has 
  
    
      
        L
      
    
    {\displaystyle L}
  
 layers, then the gradient of the first layer weights has norm 
  
    
      
        >
        c
        
          λ
          
            L
          
        
      
    
    {\displaystyle >c\lambda ^{L}}
  
 for some 
  
    
      
        λ
        >
        1
        ,
        c
        >
        0
      
    
    {\displaystyle \lambda >1,c>0}
  
 depending only on the nonlinearity.
For any fixed nonlinearity, 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 decreases as the batch size increases. For example, for ReLU, 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 decreases to 
  
    
      
        π
        
          /
        
        (
        π
        −
        1
        )
        ≈
        1.467
      
    
    {\displaystyle \pi /(\pi -1)\approx 1.467}
  
 as the batch size tends to infinity.
Practically, this means deep batchnorm networks are untrainable.
This is only relieved by skip connections in the fashion of residual networks.
This gradient explosion on the surface contradicts the smoothness property explained in the previous section, but in fact they are consistent. The previous section studies the effect of inserting a single batchnorm in a network, while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks.

Decoupling
Another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training.
By interpreting batch norm as a reparametrization of weight space, it can be shown that the length and the direction of the weights are separated and can thus be trained separately. For a particular neural network unit with input 
  
    
      
        x
      
    
    {\displaystyle x}
  
 and weight vector 
  
    
      
        w
      
    
    {\displaystyle w}
  
, denote its output as 
  
    
      
        f
        (
        w
        )
        =
        
          E
          
            x
          
        
        [
        ϕ
        (
        
          x
          
            T
          
        
        w
        )
        ]
      
    
    {\displaystyle f(w)=E_{x}[\phi (x^{T}w)]}
  
, where 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
 is the activation function, and denote 
  
    
      
        S
        =
        E
        [
        x
        
          x
          
            T
          
        
        ]
      
    
    {\displaystyle S=E[xx^{T}]}
  
. Assume that 
  
    
      
        E
        [
        x
        ]
        =
        0
      
    
    {\displaystyle E[x]=0}
  
, and that the spectrum of the matrix 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is bounded as 
  
    
      
        0
        <
        μ
        =
        
          λ
          
            m
            i
            n
          
        
        (
        S
        )
      
    
    {\displaystyle 0<\mu =\lambda _{min}(S)}
  
, 
  
    
      
        L
        =
        
          λ
          
            m
            a
            x
          
        
        (
        S
        )
        <
        ∞
      
    
    {\displaystyle L=\lambda _{max}(S)<\infty }
  
, such that 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is symmetric positive definite. Adding batch normalization to this unit thus results in

  
    
      
        
          f
          
            B
            N
          
        
        (
        w
        ,
        γ
        ,
        β
        )
        =
        
          E
          
            x
          
        
        [
        ϕ
        (
        B
        N
        (
        
          x
          
            T
          
        
        w
        )
        )
        ]
        =
        
          E
          
            x
          
        
        
          
            [
          
        
        ϕ
        
          
            (
          
        
        γ
        (
        
          
            
              
                x
                
                  T
                
              
              w
              −
              
                E
                
                  x
                
              
              [
              
                x
                
                  T
                
              
              w
              ]
            
            
              v
              a
              
                r
                
                  x
                
              
              [
              
                x
                
                  T
                
              
              w
              
                ]
                
                  1
                  
                    /
                  
                  2
                
              
            
          
        
        )
        +
        β
        
          
            )
          
        
        
          
            ]
          
        
      
    
    {\displaystyle f_{BN}(w,\gamma ,\beta )=E_{x}[\phi (BN(x^{T}w))]=E_{x}{\bigg [}\phi {\bigg (}\gamma ({\frac {x^{T}w-E_{x}[x^{T}w]}{var_{x}[x^{T}w]^{1/2}}})+\beta {\bigg )}{\bigg ]}}
  
, by definition.
The variance term can be simplified such that 
  
    
      
        v
        a
        
          r
          
            x
          
        
        [
        
          x
          
            T
          
        
        w
        ]
        =
        
          w
          
            T
          
        
        S
        w
      
    
    {\displaystyle var_{x}[x^{T}w]=w^{T}Sw}
  
. Assume that 
  
    
      
        x
      
    
    {\displaystyle x}
  
 has zero mean and 
  
    
      
        β
      
    
    {\displaystyle \beta }
  
 can be omitted, then it follows that

  
    
      
        
          f
          
            B
            N
          
        
        (
        w
        ,
        γ
        )
        =
        
          E
          
            x
          
        
        
          
            [
          
        
        ϕ
        
          
            (
          
        
        γ
        
          
            
              
                x
                
                  T
                
              
              w
            
            
              (
              
                w
                
                  T
                
              
              S
              w
              
                )
                
                  1
                  
                    /
                  
                  2
                
              
            
          
        
        
          
            )
          
        
        
          
            ]
          
        
      
    
    {\displaystyle f_{BN}(w,\gamma )=E_{x}{\bigg [}\phi {\bigg (}\gamma {\frac {x^{T}w}{(w^{T}Sw)^{1/2}}}{\bigg )}{\bigg ]}}
  
, where 
  
    
      
        (
        
          w
          
            T
          
        
        S
        w
        
          )
          
            
              1
              2
            
          
        
      
    
    {\displaystyle (w^{T}Sw)^{\frac {1}{2}}}
  
 is the induced norm of 
  
    
      
        S
      
    
    {\displaystyle S}
  
, 
  
    
      
        
          |
        
        
          |
        
        w
        
          |
        
        
          
            |
          
          
            s
          
        
      
    
    {\displaystyle ||w||_{s}}
  
.
Hence, it could be concluded that 
  
    
      
        
          f
          
            B
            N
          
        
        (
        w
        ,
        γ
        )
        =
        
          E
          
            x
          
        
        [
        ϕ
        (
        
          x
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
        ]
      
    
    {\displaystyle f_{BN}(w,\gamma )=E_{x}[\phi (x^{T}{\tilde {w}})]}
  
, where 
  
    
      
        
          
            
              w
              ~
            
          
        
        =
        γ
        
          
            w
            
              
                |
              
              
                |
              
              w
              
                |
              
              
                
                  |
                
                
                  s
                
              
            
          
        
      
    
    {\displaystyle {\tilde {w}}=\gamma {\frac {w}{||w||_{s}}}}
  
, and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 and 
  
    
      
        w
      
    
    {\displaystyle w}
  
 accounts for its length and direction separately. This property could then be used to prove the faster convergence of problems with batch normalization.

Linear convergence
Least-square problem
With the reparametrization interpretation, it could then be proved that applying batch normalization to the ordinary least squares problem achieves a linear convergence rate in gradient descent, which is faster than the regular gradient descent with only sub-linear convergence.
Denote the objective of minimizing an ordinary least squares problem as

  
    
      
        m
        i
        
          n
          
            
              
                
                  w
                  ~
                
              
            
            ∈
            
              R
              
                d
              
            
          
        
        
          f
          
            O
            L
            S
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        =
        m
        i
        
          n
          
            
              
                
                  w
                  ~
                
              
            
            ∈
            
              R
              
                d
              
            
          
        
        (
        
          E
          
            x
            ,
            y
          
        
        [
        (
        y
        −
        
          x
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        
          )
          
            2
          
        
        ]
        )
        =
        m
        i
        
          n
          
            
              
                
                  w
                  ~
                
              
            
            ∈
            
              R
              
                d
              
            
          
        
        (
        2
        
          u
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        +
        
          
            
              
                w
                ~
              
            
          
          
            T
          
        
        S
        
          
            
              w
              ~
            
          
        
        )
      
    
    {\displaystyle min_{{\tilde {w}}\in R^{d}}f_{OLS}({\tilde {w}})=min_{{\tilde {w}}\in R^{d}}(E_{x,y}[(y-x^{T}{\tilde {w}})^{2}])=min_{{\tilde {w}}\in R^{d}}(2u^{T}{\tilde {w}}+{\tilde {w}}^{T}S{\tilde {w}})}
  
, where 
  
    
      
        u
        =
        E
        [
        −
        y
        x
        ]
      
    
    {\displaystyle u=E[-yx]}
  
 and 
  
    
      
        S
        =
        E
        [
        x
        
          x
          
            T
          
        
        ]
      
    
    {\displaystyle S=E[xx^{T}]}
  
.
Since 
  
    
      
        
          
            
              w
              ~
            
          
        
        =
        γ
        
          
            w
            
              
                |
              
              
                |
              
              w
              
                |
              
              
                
                  |
                
                
                  s
                
              
            
          
        
      
    
    {\displaystyle {\tilde {w}}=\gamma {\frac {w}{||w||_{s}}}}
  
, the objective thus becomes

  
    
      
        m
        i
        
          n
          
            w
            ∈
            
              R
              
                d
              
            
            ∖
            {
            0
            }
            ,
            γ
            ∈
            R
          
        
        
          f
          
            O
            L
            S
          
        
        (
        w
        ,
        γ
        )
        =
        m
        i
        
          n
          
            w
            ∈
            
              R
              
                d
              
            
            ∖
            {
            0
            }
            ,
            γ
            ∈
            R
          
        
        
          
            (
          
        
        2
        γ
        
          
            
              
                u
                
                  T
                
              
              w
            
            
              
                |
              
              
                |
              
              w
              
                |
              
              
                
                  |
                
                
                  S
                
              
              +
              
                γ
                
                  2
                
              
            
          
        
        
          
            )
          
        
      
    
    {\displaystyle min_{w\in R^{d}\backslash \{0\},\gamma \in R}f_{OLS}(w,\gamma )=min_{w\in R^{d}\backslash \{0\},\gamma \in R}{\bigg (}2\gamma {\frac {u^{T}w}{||w||_{S}+\gamma ^{2}}}{\bigg )}}
  
, where 0 is excluded to avoid 0 in the denominator.
Since the objective is convex with respect to 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
, its optimal value could be calculated by setting the partial derivative of the objective against 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 to 0. The objective could be further simplified to be

  
    
      
        m
        i
        
          n
          
            w
            ∈
            
              R
              
                d
              
            
            ∖
            {
            0
            }
          
        
        ρ
        (
        w
        )
        =
        m
        i
        
          n
          
            w
            ∈
            
              R
              
                d
              
            
            ∖
            {
            0
            }
          
        
        
          
            (
          
        
        −
        
          
            
              
                w
                
                  T
                
              
              u
              
                u
                
                  T
                
              
              w
            
            
              
                w
                
                  T
                
              
              S
              w
            
          
        
        
          
            )
          
        
      
    
    {\displaystyle min_{w\in R^{d}\backslash \{0\}}\rho (w)=min_{w\in R^{d}\backslash \{0\}}{\bigg (}-{\frac {w^{T}uu^{T}w}{w^{T}Sw}}{\bigg )}}
  
.
Note that this objective is a form of the generalized Rayleigh quotient

  
    
      
        
          
            
              ρ
              ~
            
          
        
        (
        w
        )
        =
        
          
            
              
                w
                
                  T
                
              
              B
              w
            
            
              
                w
                
                  T
                
              
              A
              w
            
          
        
      
    
    {\displaystyle {\tilde {\rho }}(w)={\frac {w^{T}Bw}{w^{T}Aw}}}
  
, where 
  
    
      
        B
        ∈
        
          R
          
            d
            ×
            d
          
        
      
    
    {\displaystyle B\in R^{d\times d}}
  
 is a symmetric matrix and 
  
    
      
        A
        ∈
        
          R
          
            d
            ×
            d
          
        
      
    
    {\displaystyle A\in R^{d\times d}}
  
 is a symmetric positive definite matrix.
It is proven that the gradient descent convergence rate of the generalized Rayleigh quotient is

  
    
      
        
          
            
              
                λ
                
                  1
                
              
              −
              ρ
              (
              
                w
                
                  t
                  +
                  1
                
              
              )
            
            
              ρ
              (
              
                w
                
                  t
                  +
                  1
                
              
              −
              
                λ
                
                  2
                
              
              )
            
          
        
        ≤
        
          
            (
          
        
        1
        −
        
          
            
              
                λ
                
                  1
                
              
              −
              
                λ
                
                  2
                
              
            
            
              
                λ
                
                  1
                
              
              −
              
                λ
                
                  m
                  i
                  n
                
              
            
          
        
        
          
            
              )
            
          
          
            2
            t
          
        
        
          
            
              
                λ
                
                  1
                
              
              −
              ρ
              (
              
                w
                
                  t
                
              
              )
            
            
              ρ
              (
              
                w
                
                  t
                
              
              )
              −
              
                λ
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{1}-\rho (w_{t+1})}{\rho (w_{t+1}-\lambda _{2})}}\leq {\bigg (}1-{\frac {\lambda _{1}-\lambda _{2}}{\lambda _{1}-\lambda _{min}}}{\bigg )}^{2t}{\frac {\lambda _{1}-\rho (w_{t})}{\rho (w_{t})-\lambda _{2}}}}
  
, where 
  
    
      
        
          λ
          
            1
          
        
      
    
    {\displaystyle \lambda _{1}}
  
 is the largest eigenvalue of 
  
    
      
        B
      
    
    {\displaystyle B}
  
, 
  
    
      
        
          λ
          
            2
          
        
      
    
    {\displaystyle \lambda _{2}}
  
 is the second largest eigenvalue of 
  
    
      
        B
      
    
    {\displaystyle B}
  
, and 
  
    
      
        
          λ
          
            m
            i
            n
          
        
      
    
    {\displaystyle \lambda _{min}}
  
 is the smallest eigenvalue of 
  
    
      
        B
      
    
    {\displaystyle B}
  
.
In our case, 
  
    
      
        B
        =
        u
        
          u
          
            T
          
        
      
    
    {\displaystyle B=uu^{T}}
  
is a rank one matrix, and the convergence result can be simplified accordingly. Specifically, consider gradient descent steps of the form 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        
          η
          
            t
          
        
        ▽
        ρ
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle w_{t+1}=w_{t}-\eta _{t}\triangledown \rho (w_{t})}
  
 with step size 
  
    
      
        
          η
          
            t
          
        
        =
        
          
            
              
                w
                
                  t
                
                
                  T
                
              
              S
              
                w
                
                  t
                
              
            
            
              2
              L
              
                |
              
              ρ
              (
              
                w
                
                  t
                
              
              )
              
                |
              
            
          
        
      
    
    {\displaystyle \eta _{t}={\frac {w_{t}^{T}Sw_{t}}{2L|\rho (w_{t})|}}}
  
, and starting from 
  
    
      
        ρ
        (
        
          w
          
            0
          
        
        )
        ≠
        0
      
    
    {\displaystyle \rho (w_{0})\neq 0}
  
, then

  
    
      
        ρ
        (
        
          w
          
            t
          
        
        )
        −
        ρ
        (
        
          w
          
            ∗
          
        
        )
        ≤
        
          
            (
          
        
        1
        −
        
          
            μ
            L
          
        
        
          
            
              )
            
          
          
            2
            t
          
        
        (
        ρ
        (
        
          w
          
            0
          
        
        )
        −
        ρ
        (
        
          w
          
            ∗
          
        
        )
        )
      
    
    {\displaystyle \rho (w_{t})-\rho (w^{*})\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2t}(\rho (w_{0})-\rho (w^{*}))}
  
.

Learning halfspace problem
The problem of learning halfspaces refers to the training of the Perceptron, which is the simplest form of neural network. The optimization problem in this case is

  
    
      
        m
        i
        
          n
          
            
              
                
                  w
                  ~
                
              
            
            ∈
            
              R
              
                d
              
            
          
        
        
          f
          
            L
            H
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        =
        
          E
          
            y
            ,
            x
          
        
        [
        ϕ
        (
        
          z
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
        ]
      
    
    {\displaystyle min_{{\tilde {w}}\in R^{d}}f_{LH}({\tilde {w}})=E_{y,x}[\phi (z^{T}{\tilde {w}})]}
  
, where 
  
    
      
        z
        =
        −
        y
        x
      
    
    {\displaystyle z=-yx}
  
 and 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
 is an arbitrary loss function.
Suppose that 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
 is infinitely differentiable and has a bounded derivative. Assume that the objective function 
  
    
      
        
          f
          
            L
            H
          
        
      
    
    {\displaystyle f_{LH}}
  
 is 
  
    
      
        ζ
      
    
    {\displaystyle \zeta }
  
-smooth, and that a solution 
  
    
      
        
          α
          
            ∗
          
        
        =
        a
        r
        g
        m
        i
        
          n
          
            α
          
        
        
          |
        
        
          |
        
        ▽
        f
        (
        α
        w
        )
        
          |
        
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle \alpha ^{*}=argmin_{\alpha }||\triangledown f(\alpha w)||^{2}}
  
 exists and is bounded such that 
  
    
      
        −
        ∞
        <
        
          α
          
            ∗
          
        
        <
        ∞
      
    
    {\displaystyle -\infty <\alpha ^{*}<\infty }
  
. Also assume 
  
    
      
        z
      
    
    {\displaystyle z}
  
 is a multivariate normal random variable. With the Gaussian assumption, it can be shown that all critical points lie on the same line, for any choice of loss function 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
. Specifically, the gradient of 
  
    
      
        
          f
          
            L
            H
          
        
      
    
    {\displaystyle f_{LH}}
  
 could be represented as

  
    
      
        
          ▽
          
            
              
                w
                ~
              
            
          
        
        
          f
          
            L
            H
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        =
        
          c
          
            1
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        u
        +
        
          c
          
            2
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        S
        
          
            
              w
              ~
            
          
        
      
    
    {\displaystyle \triangledown _{\tilde {w}}f_{LH}({\tilde {w}})=c_{1}({\tilde {w}})u+c_{2}({\tilde {w}})S{\tilde {w}}}
  
, where  
  
    
      
        
          c
          
            1
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        =
        
          E
          
            z
          
        
        [
        
          ϕ
          
            (
            1
            )
          
        
        (
        
          z
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
        ]
        −
        
          E
          
            z
          
        
        [
        
          ϕ
          
            (
            2
            )
          
        
        (
        
          z
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
        ]
        (
        
          u
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
      
    
    {\displaystyle c_{1}({\tilde {w}})=E_{z}[\phi ^{(1)}(z^{T}{\tilde {w}})]-E_{z}[\phi ^{(2)}(z^{T}{\tilde {w}})](u^{T}{\tilde {w}})}
  
, 
  
    
      
        
          c
          
            2
          
        
        (
        
          
            
              w
              ~
            
          
        
        )
        =
        
          E
          
            z
          
        
        [
        
          ϕ
          
            (
            2
            )
          
        
        (
        
          z
          
            T
          
        
        
          
            
              w
              ~
            
          
        
        )
        ]
      
    
    {\displaystyle c_{2}({\tilde {w}})=E_{z}[\phi ^{(2)}(z^{T}{\tilde {w}})]}
  
, and 
  
    
      
        
          ϕ
          
            (
            i
            )
          
        
      
    
    {\displaystyle \phi ^{(i)}}
  
 is the 
  
    
      
        i
      
    
    {\displaystyle i}
  
-th derivative of 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
.
By setting the gradient to 0, it thus follows that the bounded critical points 
  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            ∗
          
        
      
    
    {\displaystyle {\tilde {w}}_{*}}
  
 can be expressed as 
  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            ∗
          
        
        =
        
          g
          
            ∗
          
        
        
          S
          
            −
            1
          
        
        u
      
    
    {\displaystyle {\tilde {w}}_{*}=g_{*}S^{-1}u}
  
, where 
  
    
      
        
          g
          
            ∗
          
        
      
    
    {\displaystyle g_{*}}
  
 depends on 
  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            ∗
          
        
      
    
    {\displaystyle {\tilde {w}}_{*}}
  
 and 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
. Combining this global property with length-direction decoupling, it could thus be proved that this optimization problem converges linearly.
First, a variation of gradient descent with batch normalization, Gradient Descent in Normalized Parameterization (GDNP), is designed for the objective function 
  
    
      
        m
        i
        
          n
          
            w
            ∈
            
              R
              
                d
              
            
            ∖
            {
            0
            }
            ,
            γ
            ∈
            R
          
        
        
          f
          
            L
            H
          
        
        (
        w
        ,
        γ
        )
      
    
    {\displaystyle min_{w\in R^{d}\backslash \{0\},\gamma \in R}f_{LH}(w,\gamma )}
  
, such that the direction and length of the weights are updated separately. Denote the stopping criterion of GDNP as

  
    
      
        h
        (
        
          w
          
            t
          
        
        ,
        
          γ
          
            t
          
        
        )
        =
        
          E
          
            z
          
        
        [
        
          ϕ
          ′
        
        (
        
          z
          
            T
          
        
        
          
            
              
                w
                ~
              
            
          
          
            t
          
        
        )
        ]
        (
        
          u
          
            T
          
        
        
          w
          
            t
          
        
        )
        −
        
          E
          
            z
          
        
        [
        
          ϕ
          ″
        
        (
        
          z
          
            T
          
        
        
          
            
              
                w
                ~
              
            
          
          
            t
          
        
        )
        ]
        (
        
          u
          
            T
          
        
        
          w
          
            t
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle h(w_{t},\gamma _{t})=E_{z}[\phi '(z^{T}{\tilde {w}}_{t})](u^{T}w_{t})-E_{z}[\phi ''(z^{T}{\tilde {w}}_{t})](u^{T}w_{t})^{2}}
  
.
Let the step size be

  
    
      
        
          s
          
            t
          
        
        =
        s
        (
        
          w
          
            t
          
        
        ,
        
          γ
          
            t
          
        
        )
        =
        −
        
          
            
              
                |
              
              
                |
              
              
                w
                
                  t
                
              
              
                |
              
              
                
                  |
                
                
                  S
                
                
                  3
                
              
            
            
              L
              
                g
                
                  t
                
              
              h
              (
              
                w
                
                  t
                
              
              ,
              
                γ
                
                  t
                
              
              )
            
          
        
      
    
    {\displaystyle s_{t}=s(w_{t},\gamma _{t})=-{\frac {||w_{t}||_{S}^{3}}{Lg_{t}h(w_{t},\gamma _{t})}}}
  
.
For each step, if 
  
    
      
        h
        (
        
          w
          
            t
          
        
        ,
        
          γ
          
            t
          
        
        )
        ≠
        0
      
    
    {\displaystyle h(w_{t},\gamma _{t})\neq 0}
  
, then update the direction as

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        
          s
          
            t
          
        
        
          ▽
          
            w
          
        
        f
        (
        
          w
          
            t
          
        
        ,
        
          γ
          
            t
          
        
        )
      
    
    {\displaystyle w_{t+1}=w_{t}-s_{t}\triangledown _{w}f(w_{t},\gamma _{t})}
  
.
Then update the length according to

  
    
      
        
          γ
          
            t
          
        
        =
        B
        i
        s
        e
        c
        t
        i
        o
        n
        (
        
          T
          
            s
          
        
        ,
        f
        ,
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle \gamma _{t}=Bisection(T_{s},f,w_{t})}
  
, where 
  
    
      
        B
        i
        s
        e
        c
        t
        i
        o
        n
        (
        )
      
    
    {\displaystyle Bisection()}
  
 is the classical bisection algorithm, and 
  
    
      
        
          T
          
            s
          
        
      
    
    {\displaystyle T_{s}}
  
 is the total iterations ran in the bisection step.
Denote the total number of iterations as 
  
    
      
        
          T
          
            d
          
        
      
    
    {\displaystyle T_{d}}
  
, then the final output of GDNP is

  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            
              T
              
                d
              
            
          
        
        =
        
          γ
          
            
              T
              
                d
              
            
          
        
        
          
            
              w
              
                
                  T
                  
                    d
                  
                
              
            
            
              
                |
              
              
                |
              
              
                w
                
                  
                    T
                    
                      d
                    
                  
                
              
              
                |
              
              
                
                  |
                
                
                  S
                
              
            
          
        
      
    
    {\displaystyle {\tilde {w}}_{T_{d}}=\gamma _{T_{d}}{\frac {w_{T_{d}}}{||w_{T_{d}}||_{S}}}}
  
.
The GDNP algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis.
It can be shown that in GDNP, the partial derivative of 
  
    
      
        
          f
          
            L
            H
          
        
      
    
    {\displaystyle f_{LH}}
  
against the length component converges to zero at a linear rate, such that

  
    
      
        (
        
          ∂
          
            γ
          
        
        
          f
          
            L
            H
          
        
        (
        
          w
          
            t
          
        
        ,
        
          a
          
            t
          
          
            (
            
              T
              
                s
              
            
            )
          
        
        
          )
          
            2
          
        
        ≤
        
          
            
              
                2
                
                  −
                  
                    T
                    
                      s
                    
                  
                
              
              ζ
              
                |
              
              
                b
                
                  t
                
                
                  (
                  0
                  )
                
              
              −
              
                a
                
                  t
                
                
                  (
                  0
                  )
                
              
              
                |
              
            
            
              μ
              
                2
              
            
          
        
      
    
    {\displaystyle (\partial _{\gamma }f_{LH}(w_{t},a_{t}^{(T_{s})})^{2}\leq {\frac {2^{-T_{s}}\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\mu ^{2}}}}
  
, where 
  
    
      
        
          a
          
            t
          
          
            (
            0
            )
          
        
      
    
    {\displaystyle a_{t}^{(0)}}
  
 and 
  
    
      
        
          b
          
            t
          
          
            0
          
        
      
    
    {\displaystyle b_{t}^{0}}
  
 are the two starting points of the bisection algorithm on the left and on the right, correspondingly.
Further, for each iteration, the norm of the gradient of 
  
    
      
        
          f
          
            L
            H
          
        
      
    
    {\displaystyle f_{LH}}
  
 with respect to 
  
    
      
        w
      
    
    {\displaystyle w}
  
 converges linearly, such that

  
    
      
        
          |
        
        
          |
        
        
          w
          
            t
          
        
        
          |
        
        
          
            |
          
          
            S
          
          
            2
          
        
        
          |
        
        
          |
        
        ▽
        
          f
          
            L
            H
          
        
        (
        
          w
          
            t
          
        
        ,
        
          g
          
            t
          
        
        )
        
          |
        
        
          
            |
          
          
            
              S
              
                −
                1
              
            
          
          
            2
          
        
        ≤
        
          
            (
          
        
        1
        −
        
          
            μ
            L
          
        
        
          
            
              )
            
          
          
            2
            t
          
        
        
          Φ
          
            2
          
        
        
          γ
          
            t
          
          
            2
          
        
        (
        ρ
        (
        
          w
          
            0
          
        
        )
        −
        
          ρ
          
            ∗
          
        
        )
      
    
    {\displaystyle ||w_{t}||_{S}^{2}||\triangledown f_{LH}(w_{t},g_{t})||_{S^{-1}}^{2}\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2t}\Phi ^{2}\gamma _{t}^{2}(\rho (w_{0})-\rho ^{*})}
  
.
Combining these two inequalities, a bound could thus be obtained for the gradient with respect to 
  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            
              T
              
                d
              
            
          
        
      
    
    {\displaystyle {\tilde {w}}_{T_{d}}}
  
:

  
    
      
        
          |
        
        
          |
        
        
          ▽
          
            
              
                w
                ~
              
            
          
        
        f
        (
        
          
            
              
                w
                ~
              
            
          
          
            
              T
              
                d
              
            
          
        
        )
        
          |
        
        
          
            |
          
          
            2
          
        
        ≤
        
          
            (
          
        
        1
        −
        
          
            μ
            L
          
        
        
          
            
              )
            
          
          
            2
            
              T
              
                d
              
            
          
        
        
          Φ
          
            2
          
        
        (
        ρ
        (
        
          w
          
            0
          
        
        )
        −
        
          ρ
          
            ∗
          
        
        )
        +
        
          
            
              
                2
                
                  −
                  
                    T
                    
                      s
                    
                  
                
              
              ζ
              
                |
              
              
                b
                
                  t
                
                
                  (
                  0
                  )
                
              
              −
              
                a
                
                  t
                
                
                  (
                  0
                  )
                
              
              
                |
              
            
            
              μ
              
                2
              
            
          
        
      
    
    {\displaystyle ||\triangledown _{\tilde {w}}f({\tilde {w}}_{T_{d}})||^{2}\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2T_{d}}\Phi ^{2}(\rho (w_{0})-\rho ^{*})+{\frac {2^{-T_{s}}\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\mu ^{2}}}}
  
, such that the algorithm is guaranteed to converge linearly.
Although the proof stands on the assumption of Gaussian input, it is also shown in experiments that GDNP could accelerate optimization without this constraint.

Neural networks
Consider a multilayer perceptron (MLP) with one hidden layer and 
  
    
      
        m
      
    
    {\displaystyle m}
  
 hidden units with mapping from input 
  
    
      
        x
        ∈
        
          R
          
            d
          
        
      
    
    {\displaystyle x\in R^{d}}
  
 to a scalar output described as

  
    
      
        
          F
          
            x
          
        
        (
        
          
            
              W
              ~
            
          
        
        ,
        Θ
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          θ
          
            i
          
        
        ϕ
        (
        
          x
          
            T
          
        
        
          
            
              
                w
                ~
              
            
          
          
            (
            i
            )
          
        
        )
      
    
    {\displaystyle F_{x}({\tilde {W}},\Theta )=\sum _{i=1}^{m}\theta _{i}\phi (x^{T}{\tilde {w}}^{(i)})}
  
, where 
  
    
      
        
          
            
              
                w
                ~
              
            
          
          
            (
            i
            )
          
        
      
    
    {\displaystyle {\tilde {w}}^{(i)}}
  
 and 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  
 are the input and output weights of unit 
  
    
      
        i
      
    
    {\displaystyle i}
  
 correspondingly, and 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
 is the activation function and is assumed to be a tanh function.
The input and output weights could then be optimized with

  
    
      
        m
        i
        
          n
          
            
              
                
                  W
                  ~
                
              
            
            ,
            Θ
          
        
        (
        
          f
          
            N
            N
          
        
        (
        
          
            
              W
              ~
            
          
        
        ,
        Θ
        )
        =
        
          E
          
            y
            ,
            x
          
        
        [
        l
        (
        −
        y
        
          F
          
            x
          
        
        (
        
          
            
              W
              ~
            
          
        
        ,
        Θ
        )
        )
        ]
        )
      
    
    {\displaystyle min_{{\tilde {W}},\Theta }(f_{NN}({\tilde {W}},\Theta )=E_{y,x}[l(-yF_{x}({\tilde {W}},\Theta ))])}
  
, where 
  
    
      
        l
      
    
    {\displaystyle l}
  
 is a loss function, 
  
    
      
        
          
            
              W
              ~
            
          
        
        =
        {
        
          
            
              
                w
                ~
              
            
          
          
            (
            1
            )
          
        
        ,
        .
        .
        .
        ,
        
          
            
              
                w
                ~
              
            
          
          
            (
            m
            )
          
        
        }
      
    
    {\displaystyle {\tilde {W}}=\{{\tilde {w}}^{(1)},...,{\tilde {w}}^{(m)}\}}
  
, and 
  
    
      
        Θ
        =
        {
        
          θ
          
            (
            1
            )
          
        
        ,
        .
        .
        .
        ,
        
          θ
          
            (
            m
            )
          
        
        }
      
    
    {\displaystyle \Theta =\{\theta ^{(1)},...,\theta ^{(m)}\}}
  
.
Consider fixed 
  
    
      
        Θ
      
    
    {\displaystyle \Theta }
  
 and optimizing only 
  
    
      
        
          
            
              W
              ~
            
          
        
      
    
    {\displaystyle {\tilde {W}}}
  
, it can be shown that the critical points of 
  
    
      
        
          f
          
            N
            N
          
        
        (
        
          
            
              W
              ~
            
          
        
        )
      
    
    {\displaystyle f_{NN}({\tilde {W}})}
  
 of a particular hidden unit 
  
    
      
        i
      
    
    {\displaystyle i}
  
, 
  
    
      
        
          
            
              
                w
                ^
              
            
          
          
            (
            i
            )
          
        
      
    
    {\displaystyle {\hat {w}}^{(i)}}
  
, all align along one line depending on incoming information into the hidden layer, such that

  
    
      
        
          
            
              
                w
                ^
              
            
          
          
            (
            i
            )
          
        
        =
        
          
            
              
                c
                ^
              
            
          
          
            (
            i
            )
          
        
        
          S
          
            −
            1
          
        
        u
      
    
    {\displaystyle {\hat {w}}^{(i)}={\hat {c}}^{(i)}S^{-1}u}
  
, where 
  
    
      
        
          
            
              
                c
                ^
              
            
          
          
            (
            i
            )
          
        
        ∈
        R
      
    
    {\displaystyle {\hat {c}}^{(i)}\in R}
  
 is a scalar, 
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        ,
        m
      
    
    {\displaystyle i=1,...,m}
  
.
This result could be proved by setting the gradient of 
  
    
      
        
          f
          
            N
            N
          
        
      
    
    {\displaystyle f_{NN}}
  
 to zero and solving the system of equations.
Apply the GDNP algorithm to this optimization problem by alternating optimization over the different hidden units. Specifically, for each hidden unit, run GDNP to find the optimal 
  
    
      
        W
      
    
    {\displaystyle W}
  
 and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
. With the same choice of stopping criterion and stepsize, it follows that

  
    
      
        
          |
        
        
          |
        
        
          ▽
          
            
              
                
                  
                    w
                    ~
                  
                
              
              
                (
                i
                )
              
            
          
        
        f
        (
        
          
            
              
                w
                ~
              
            
          
          
            t
          
          
            (
            i
            )
          
        
        )
        
          |
        
        
          
            |
          
          
            
              S
              
                −
                1
              
            
          
          
            2
          
        
        ≤
        
          
            (
          
        
        1
        −
        
          
            μ
            L
          
        
        
          
            
              )
            
          
          
            2
            t
          
        
        C
        (
        ρ
        (
        
          w
          
            0
          
        
        )
        −
        
          ρ
          
            ∗
          
        
        )
        +
        
          
            
              
                2
                
                  −
                  
                    T
                    
                      s
                    
                    
                      (
                      i
                      )
                    
                  
                
              
              ζ
              
                |
              
              
                b
                
                  t
                
                
                  (
                  0
                  )
                
              
              −
              
                a
                
                  t
                
                
                  (
                  0
                  )
                
              
              
                |
              
            
            
              μ
              
                2
              
            
          
        
      
    
    {\displaystyle ||\triangledown _{{\tilde {w}}^{(i)}}f({\tilde {w}}_{t}^{(i)})||_{S^{-1}}^{2}\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2t}C(\rho (w_{0})-\rho ^{*})+{\frac {2^{-T_{s}^{(i)}}\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\mu ^{2}}}}
  
.
Since the parameters of each hidden unit converge linearly, the whole optimization problem has a linear rate of convergence.

References
Further reading
Ioffe, Sergey; Szegedy, Christian (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", ICML'15: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, July 2015 Pages 448–456
Simonyan, Karen; Zisserman, Andrew (2014). "Very Deep Convolutional Networks for Large-Scale Image Recognition". arXiv:1409.1556 [cs.CV].
Bayesian inference ( BAY-zee-ən or  BAY-zhən) is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Fundamentally, Bayesian inference uses prior knowledge, in the form of a prior distribution in order to estimate posterior probabilities. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability".

Introduction to Bayes' rule
Formal explanation
Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a "likelihood function" derived from a statistical model for the observed data. Bayesian inference computes the posterior probability according to Bayes' theorem:

  
    
      
        P
        (
        H
        ∣
        E
        )
        =
        
          
            
              P
              (
              E
              ∣
              H
              )
              ⋅
              P
              (
              H
              )
            
            
              P
              (
              E
              )
            
          
        
        ,
      
    
    {\displaystyle P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)}},}
  

where

  
    
      
        H
      
    
    {\displaystyle H}
  
 stands for any hypothesis whose probability may be affected by data (called evidence below). Often there are competing hypotheses, and the task is to determine which is the most probable.

  
    
      
        P
        (
        H
        )
      
    
    {\displaystyle P(H)}
  
, the prior probability, is the estimate of the probability of the hypothesis 
  
    
      
        H
      
    
    {\displaystyle H}
  
 before the data 
  
    
      
        E
      
    
    {\displaystyle E}
  
, the current evidence, is observed.

  
    
      
        E
      
    
    {\displaystyle E}
  
, the evidence, corresponds to new data that were not used in computing the prior probability.

  
    
      
        P
        (
        H
        ∣
        E
        )
      
    
    {\displaystyle P(H\mid E)}
  
, the posterior probability, is the probability of 
  
    
      
        H
      
    
    {\displaystyle H}
  
 given 
  
    
      
        E
      
    
    {\displaystyle E}
  
, i.e., after 
  
    
      
        E
      
    
    {\displaystyle E}
  
 is observed.  This is what we want to know: the probability of a hypothesis given the observed evidence.

  
    
      
        P
        (
        E
        ∣
        H
        )
      
    
    {\displaystyle P(E\mid H)}
  
 is the probability of observing 
  
    
      
        E
      
    
    {\displaystyle E}
  
 given 
  
    
      
        H
      
    
    {\displaystyle H}
  
 and is called the likelihood. As a function of 
  
    
      
        E
      
    
    {\displaystyle E}
  
 with 
  
    
      
        H
      
    
    {\displaystyle H}
  
 fixed, it indicates the compatibility of the evidence with the given hypothesis. The likelihood function is a function of the evidence, 
  
    
      
        E
      
    
    {\displaystyle E}
  
, while the posterior probability is a function of the hypothesis, 
  
    
      
        H
      
    
    {\displaystyle H}
  
.

  
    
      
        P
        (
        E
        )
      
    
    {\displaystyle P(E)}
  
 is sometimes termed the marginal likelihood or "model evidence". This factor is the same for all possible hypotheses being considered (as is evident from the fact that the hypothesis 
  
    
      
        H
      
    
    {\displaystyle H}
  
 does not appear anywhere in the symbol, unlike for all the other factors) and hence does not factor into determining the relative probabilities of different hypotheses.

  
    
      
        P
        (
        E
        )
        >
        0
      
    
    {\displaystyle P(E)>0}
  
 (Else one has 
  
    
      
        0
        
          /
        
        0
      
    
    {\displaystyle 0/0}
  
.)
For different values of 
  
    
      
        H
      
    
    {\displaystyle H}
  
, only the factors 
  
    
      
        P
        (
        H
        )
      
    
    {\displaystyle P(H)}
  
 and 
  
    
      
        P
        (
        E
        ∣
        H
        )
      
    
    {\displaystyle P(E\mid H)}
  
, both in the numerator, affect the value of 
  
    
      
        P
        (
        H
        ∣
        E
        )
      
    
    {\displaystyle P(H\mid E)}
  
 –  the posterior probability of a hypothesis is proportional to its prior probability (its inherent likeliness) and the newly acquired likelihood (its compatibility with the new observed evidence).
In cases where 
  
    
      
        ¬
        H
      
    
    {\displaystyle \neg H}
  
 ("not 
  
    
      
        H
      
    
    {\displaystyle H}
  
"), the logical negation of 
  
    
      
        H
      
    
    {\displaystyle H}
  
, is a valid likelihood, Bayes' rule can be rewritten as follows:

  
    
      
        
          
            
              
                P
                (
                H
                ∣
                E
                )
              
              
                
                =
                
                  
                    
                      P
                      (
                      E
                      ∣
                      H
                      )
                      P
                      (
                      H
                      )
                    
                    
                      P
                      (
                      E
                      )
                    
                  
                
              
            
            
              
            
            
              
              
                
                =
                
                  
                    
                      P
                      (
                      E
                      ∣
                      H
                      )
                      P
                      (
                      H
                      )
                    
                    
                      P
                      (
                      E
                      ∣
                      H
                      )
                      P
                      (
                      H
                      )
                      +
                      P
                      (
                      E
                      ∣
                      ¬
                      H
                      )
                      P
                      (
                      ¬
                      H
                      )
                    
                  
                
              
            
            
              
            
            
              
              
                
                =
                
                  
                    1
                    
                      1
                      +
                      
                        (
                        
                          
                            
                              1
                              
                                P
                                (
                                H
                                )
                              
                            
                          
                          −
                          1
                        
                        )
                      
                      
                        
                          
                            P
                            (
                            E
                            ∣
                            ¬
                            H
                            )
                          
                          
                            P
                            (
                            E
                            ∣
                            H
                            )
                          
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}P(H\mid E)&={\frac {P(E\mid H)P(H)}{P(E)}}\\\\&={\frac {P(E\mid H)P(H)}{P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}}\\\\&={\frac {1}{1+\left({\frac {1}{P(H)}}-1\right){\frac {P(E\mid \neg H)}{P(E\mid H)}}}}\\\end{aligned}}}
  

because

  
    
      
        P
        (
        E
        )
        =
        P
        (
        E
        ∣
        H
        )
        P
        (
        H
        )
        +
        P
        (
        E
        ∣
        ¬
        H
        )
        P
        (
        ¬
        H
        )
      
    
    {\displaystyle P(E)=P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}
  

and

  
    
      
        P
        (
        H
        )
        +
        P
        (
        ¬
        H
        )
        =
        1.
      
    
    {\displaystyle P(H)+P(\neg H)=1.}
  
  This focuses attention on the term 
  
    
      
        
          (
          
            
              
                
                  1
                  
                    P
                    (
                    H
                    )
                  
                
              
            
            −
            1
          
          )
        
        
          
            
              
                P
                (
                E
                ∣
                ¬
                H
                )
              
              
                P
                (
                E
                ∣
                H
                )
              
            
          
        
        .
      
    
    {\displaystyle \left({\tfrac {1}{P(H)}}-1\right){\tfrac {P(E\mid \neg H)}{P(E\mid H)}}.}
  
  If that term is approximately 1, then the probability of the hypothesis given the evidence, 
  
    
      
        P
        (
        H
        ∣
        E
        )
      
    
    {\displaystyle P(H\mid E)}
  
, is about 
  
    
      
        
          
            
              1
              2
            
          
        
      
    
    {\displaystyle {\tfrac {1}{2}}}
  
, about 50% likely - equally likely or not likely.  If that term is very small, close to zero, then the probability of the hypothesis, given the evidence, 
  
    
      
        P
        (
        H
        ∣
        E
        )
      
    
    {\displaystyle P(H\mid E)}
  
 is close to 1 or the conditional hypothesis is quite likely.  If that term is very large, much larger than 1, then the hypothesis, given the evidence, is quite unlikely.  If the hypothesis (without consideration of evidence) is unlikely, then 
  
    
      
        P
        (
        H
        )
      
    
    {\displaystyle P(H)}
  
 is small (but not necessarily astronomically small) and 
  
    
      
        
          
            
              1
              
                P
                (
                H
                )
              
            
          
        
      
    
    {\displaystyle {\tfrac {1}{P(H)}}}
  
 is much larger than 1 and this term can be approximated as 
  
    
      
        
          
            
              
                P
                (
                E
                ∣
                ¬
                H
                )
              
              
                P
                (
                E
                ∣
                H
                )
                ⋅
                P
                (
                H
                )
              
            
          
        
      
    
    {\displaystyle {\tfrac {P(E\mid \neg H)}{P(E\mid H)\cdot P(H)}}}
  
 and relevant probabilities can be compared directly to each other.
One quick and easy way to remember the equation would be to use rule of multiplication:

  
    
      
        P
        (
        E
        ∩
        H
        )
        =
        P
        (
        E
        ∣
        H
        )
        P
        (
        H
        )
        =
        P
        (
        H
        ∣
        E
        )
        P
        (
        E
        )
        .
      
    
    {\displaystyle P(E\cap H)=P(E\mid H)P(H)=P(H\mid E)P(E).}

Alternatives to Bayesian updating
Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational.
Ian Hacking noted that traditional "Dutch book" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote: "And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."
Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "probability kinematics") following the publication of Richard C. Jeffrey's rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability. The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.

Inference over exclusive and exhaustive possibilities
If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.

General formulation
Suppose a process is generating independent and identically distributed events 
  
    
      
        
          E
          
            n
          
        
        ,
         
        n
        =
        1
        ,
        2
        ,
        3
        ,
        …
      
    
    {\displaystyle E_{n},\ n=1,2,3,\ldots }
  
, but the probability distribution is unknown. Let the event space 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  
 represent the current state of belief for this process. Each model is represented by event 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{m}}
  
. The conditional probabilities 
  
    
      
        P
        (
        
          E
          
            n
          
        
        ∣
        
          M
          
            m
          
        
        )
      
    
    {\displaystyle P(E_{n}\mid M_{m})}
  
 are specified to define the models. 
  
    
      
        P
        (
        
          M
          
            m
          
        
        )
      
    
    {\displaystyle P(M_{m})}
  
 is the degree of belief in 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{m}}
  
. Before the first inference step, 
  
    
      
        {
        P
        (
        
          M
          
            m
          
        
        )
        }
      
    
    {\displaystyle \{P(M_{m})\}}
  
 is a set of initial prior probabilities. These must sum to 1, but are otherwise arbitrary.
Suppose that the process is observed to generate 
  
    
      
        E
        ∈
        {
        
          E
          
            n
          
        
        }
      
    
    {\displaystyle E\in \{E_{n}\}}
  
. For each 
  
    
      
        M
        ∈
        {
        
          M
          
            m
          
        
        }
      
    
    {\displaystyle M\in \{M_{m}\}}
  
, the prior 
  
    
      
        P
        (
        M
        )
      
    
    {\displaystyle P(M)}
  
 is updated to the posterior 
  
    
      
        P
        (
        M
        ∣
        E
        )
      
    
    {\displaystyle P(M\mid E)}
  
. From Bayes' theorem:

  
    
      
        P
        (
        M
        ∣
        E
        )
        =
        
          
            
              P
              (
              E
              ∣
              M
              )
            
            
              
                ∑
                
                  m
                
              
              
                P
                (
                E
                ∣
                
                  M
                  
                    m
                  
                
                )
                P
                (
                
                  M
                  
                    m
                  
                
                )
              
            
          
        
        ⋅
        P
        (
        M
        )
        .
      
    
    {\displaystyle P(M\mid E)={\frac {P(E\mid M)}{\sum _{m}{P(E\mid M_{m})P(M_{m})}}}\cdot P(M).}
  

Upon observation of further evidence, this procedure may be repeated.

Multiple observations
For a sequence of independent and identically distributed observations 
  
    
      
        
          E
        
        =
        (
        
          e
          
            1
          
        
        ,
        …
        ,
        
          e
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}
  
, it can be shown by induction that repeated application of the above is equivalent to

  
    
      
        P
        (
        M
        ∣
        
          E
        
        )
        =
        
          
            
              P
              (
              
                E
              
              ∣
              M
              )
            
            
              
                ∑
                
                  m
                
              
              
                P
                (
                
                  E
                
                ∣
                
                  M
                  
                    m
                  
                
                )
                P
                (
                
                  M
                  
                    m
                  
                
                )
              
            
          
        
        ⋅
        P
        (
        M
        )
        ,
      
    
    {\displaystyle P(M\mid \mathbf {E} )={\frac {P(\mathbf {E} \mid M)}{\sum _{m}{P(\mathbf {E} \mid M_{m})P(M_{m})}}}\cdot P(M),}
  

where

  
    
      
        P
        (
        
          E
        
        ∣
        M
        )
        =
        
          ∏
          
            k
          
        
        
          P
          (
          
            e
            
              k
            
          
          ∣
          M
          )
        
        .
      
    
    {\displaystyle P(\mathbf {E} \mid M)=\prod _{k}{P(e_{k}\mid M)}.}

Parametric formulation: motivating the formal description
By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is, however, equally applicable to discrete distributions.
Let the vector 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  
 span the parameter space. Let the initial prior distribution over 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  
 be 
  
    
      
        p
        (
        
          θ
        
        ∣
        
          α
        
        )
      
    
    {\displaystyle p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})}
  
, where 
  
    
      
        
          α
        
      
    
    {\displaystyle {\boldsymbol {\alpha }}}
  
 is a set of parameters to the prior itself, or hyperparameters. Let 
  
    
      
        
          E
        
        =
        (
        
          e
          
            1
          
        
        ,
        …
        ,
        
          e
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}
  
 be a sequence of independent and identically distributed event observations, where all 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
  
 are distributed as 
  
    
      
        p
        (
        e
        ∣
        
          θ
        
        )
      
    
    {\displaystyle p(e\mid {\boldsymbol {\theta }})}
  
 for some 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  
. Bayes' theorem is applied to find the posterior distribution over 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  
:

  
    
      
        
          
            
              
                p
                (
                
                  θ
                
                ∣
                
                  E
                
                ,
                
                  α
                
                )
              
              
                
                =
                
                  
                    
                      p
                      (
                      
                        E
                      
                      ∣
                      
                        θ
                      
                      ,
                      
                        α
                      
                      )
                    
                    
                      p
                      (
                      
                        E
                      
                      ∣
                      
                        α
                      
                      )
                    
                  
                
                ⋅
                p
                (
                
                  θ
                
                ∣
                
                  α
                
                )
              
            
            
              
              
                
                =
                
                  
                    
                      p
                      (
                      
                        E
                      
                      ∣
                      
                        θ
                      
                      ,
                      
                        α
                      
                      )
                    
                    
                      ∫
                      p
                      (
                      
                        E
                      
                      ∣
                      
                        θ
                      
                      ,
                      
                        α
                      
                      )
                      p
                      (
                      
                        θ
                      
                      ∣
                      
                        α
                      
                      )
                      
                      d
                      
                        θ
                      
                    
                  
                
                ⋅
                p
                (
                
                  θ
                
                ∣
                
                  α
                
                )
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}p({\boldsymbol {\theta }}\mid \mathbf {E} ,{\boldsymbol {\alpha }})&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{p(\mathbf {E} \mid {\boldsymbol {\alpha }})}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\\&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{\int p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\,d{\boldsymbol {\theta }}}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }}),\end{aligned}}}
  

where

  
    
      
        p
        (
        
          E
        
        ∣
        
          θ
        
        ,
        
          α
        
        )
        =
        
          ∏
          
            k
          
        
        p
        (
        
          e
          
            k
          
        
        ∣
        
          θ
        
        )
        .
      
    
    {\displaystyle p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})=\prod _{k}p(e_{k}\mid {\boldsymbol {\theta }}).}

Formal description of Bayesian inference
Definitions
x
      
    
    {\displaystyle x}
  
, a data point in general.  This may in fact be a vector of values.

  
    
      
        θ
      
    
    {\displaystyle \theta }
  
, the parameter of the data point's distribution, i.e., 
  
    
      
        x
        ∼
        p
        (
        x
        ∣
        θ
        )
      
    
    {\displaystyle x\sim p(x\mid \theta )}
  
.  This may be a vector of parameters.

  
    
      
        α
      
    
    {\displaystyle \alpha }
  
, the hyperparameter of the parameter distribution, i.e., 
  
    
      
        θ
        ∼
        p
        (
        θ
        ∣
        α
        )
      
    
    {\displaystyle \theta \sim p(\theta \mid \alpha )}
  
.  This may be a vector of hyperparameters.

  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  
 is the sample, a set of 
  
    
      
        n
      
    
    {\displaystyle n}
  
 observed data points, i.e., 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\ldots ,x_{n}}
  
.

  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\tilde {x}}}
  
, a new data point whose distribution is to be predicted.

Bayesian inference
The prior distribution is the distribution of the parameter(s) before any data is observed, i.e. 
  
    
      
        p
        (
        θ
        ∣
        α
        )
      
    
    {\displaystyle p(\theta \mid \alpha )}
  
 . The prior distribution might not be easily determined; in such a case, one possibility may be to use the Jeffreys prior to obtain a prior distribution before updating it with newer observations.
The sampling distribution is the distribution of the observed data conditional on its parameters, i.e. 
  
    
      
        p
        (
        
          X
        
        ∣
        θ
        )
      
    
    {\displaystyle p(\mathbf {X} \mid \theta )}
  
.  This is also termed the likelihood, especially when viewed as a function of the parameter(s), sometimes written 
  
    
      
        L
        ⁡
        (
        θ
        ∣
        
          X
        
        )
        =
        p
        (
        
          X
        
        ∣
        θ
        )
      
    
    {\displaystyle \operatorname {L} (\theta \mid \mathbf {X} )=p(\mathbf {X} \mid \theta )}
  
.
The marginal likelihood (sometimes also termed the evidence) is the distribution of the observed data marginalized over the parameter(s), i.e. 
  
    
      
        p
        (
        
          X
        
        ∣
        α
        )
        =
        ∫
        p
        (
        
          X
        
        ∣
        θ
        )
        p
        (
        θ
        ∣
        α
        )
        d
        θ
        .
      
    
    {\displaystyle p(\mathbf {X} \mid \alpha )=\int p(\mathbf {X} \mid \theta )p(\theta \mid \alpha )d\theta .}
  
 It quantifies the agreement between data and expert opinion, in a geometric sense that can be made precise. If the marginal likelihood is 0 then there is no agreement between the data and expert opinion and Bayes' rule cannot be applied.
The posterior distribution is the distribution of the parameter(s) after taking into account the observed data.  This is determined by Bayes' rule, which forms the heart of Bayesian inference: 
  
    
      
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
        =
        
          
            
              p
              (
              θ
              ,
              
                X
              
              ,
              α
              )
            
            
              p
              (
              
                X
              
              ,
              α
              )
            
          
        
        =
        
          
            
              p
              (
              
                X
              
              ∣
              θ
              ,
              α
              )
              p
              (
              θ
              ,
              α
              )
            
            
              p
              (
              
                X
              
              ∣
              α
              )
              p
              (
              α
              )
            
          
        
        =
        
          
            
              p
              (
              
                X
              
              ∣
              θ
              ,
              α
              )
              p
              (
              θ
              ∣
              α
              )
            
            
              p
              (
              
                X
              
              ∣
              α
              )
            
          
        
        ∝
        p
        (
        
          X
        
        ∣
        θ
        ,
        α
        )
        p
        (
        θ
        ∣
        α
        )
        .
      
    
    {\displaystyle p(\theta \mid \mathbf {X} ,\alpha )={\frac {p(\theta ,\mathbf {X} ,\alpha )}{p(\mathbf {X} ,\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta ,\alpha )}{p(\mathbf {X} \mid \alpha )p(\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha )}{p(\mathbf {X} \mid \alpha )}}\propto p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha ).}
  
 This is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence".
In practice, for almost all complex Bayesian models used in machine learning, the posterior distribution 
  
    
      
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
      
    
    {\displaystyle p(\theta \mid \mathbf {X} ,\alpha )}
  
 is not obtained in a closed form distribution, mainly because the parameter space for 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
 can be very high, or the Bayesian model retains certain hierarchical structure formulated from the observations 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  
 and parameter 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
. In such situations, we need to resort to approximation techniques.
General case: Let 
  
    
      
        
          P
          
            Y
          
          
            x
          
        
      
    
    {\displaystyle P_{Y}^{x}}
  
 be the conditional distribution of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 given 
  
    
      
        X
        =
        x
      
    
    {\displaystyle X=x}
  
 and let 
  
    
      
        
          P
          
            X
          
        
      
    
    {\displaystyle P_{X}}
  
 be the distribution of 
  
    
      
        X
      
    
    {\displaystyle X}
  
. The joint distribution is then 
  
    
      
        
          P
          
            X
            ,
            Y
          
        
        (
        d
        x
        ,
        d
        y
        )
        =
        
          P
          
            Y
          
          
            x
          
        
        (
        d
        y
        )
        
          P
          
            X
          
        
        (
        d
        x
        )
      
    
    {\displaystyle P_{X,Y}(dx,dy)=P_{Y}^{x}(dy)P_{X}(dx)}
  
. The conditional distribution 
  
    
      
        
          P
          
            X
          
          
            y
          
        
      
    
    {\displaystyle P_{X}^{y}}
  
 of 
  
    
      
        X
      
    
    {\displaystyle X}
  
  given 
  
    
      
        Y
        =
        y
      
    
    {\displaystyle Y=y}
  
 is then determined by

  
    
      
        
          P
          
            X
          
          
            y
          
        
        (
        A
        )
        =
        E
        (
        
          1
          
            A
          
        
        (
        X
        )
        
          |
        
        Y
        =
        y
        )
      
    
    {\displaystyle P_{X}^{y}(A)=E(1_{A}(X)|Y=y)}
  
Existence and uniqueness of the needed conditional expectation is a consequence of the Radon–Nikodym theorem. This was formulated by Kolmogorov in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing "I wish to call attention to  ... and especially the theory of conditional probabilities and conditional expectations ..." in the Preface. The Bayes theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions. Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line. Modern Markov chain Monte Carlo methods have boosted the importance of Bayes' theorem including cases with improper priors.

Bayesian prediction
The posterior predictive distribution is the distribution of a new data point, marginalized over the posterior: 
  
    
      
        p
        (
        
          
            
              x
              ~
            
          
        
        ∣
        
          X
        
        ,
        α
        )
        =
        ∫
        p
        (
        
          
            
              x
              ~
            
          
        
        ∣
        θ
        )
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
        d
        θ
      
    
    {\displaystyle p({\tilde {x}}\mid \mathbf {X} ,\alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )d\theta }
  

The prior predictive distribution is the distribution of a new data point, marginalized over the prior: 
  
    
      
        p
        (
        
          
            
              x
              ~
            
          
        
        ∣
        α
        )
        =
        ∫
        p
        (
        
          
            
              x
              ~
            
          
        
        ∣
        θ
        )
        p
        (
        θ
        ∣
        α
        )
        d
        θ
      
    
    {\displaystyle p({\tilde {x}}\mid \alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \alpha )d\theta }
  

Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned.  Only this way is the entire posterior distribution of the parameter(s) used.  By comparison, prediction in frequentist statistics often involves finding an optimum point estimate of the parameter(s)—e.g., by maximum likelihood or maximum a posteriori estimation (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution.
In some instances, frequentist statistics can work around this problem. For example, confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a Student's t-distribution.  This correctly estimates the variance, due to the facts that (1) the average of normally distributed random variables is also normally distributed, and (2) the predictive distribution of a normally distributed data point with unknown mean and variance, using conjugate or uninformative priors, has a Student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least to an arbitrary level of precision when numerical methods are used.
Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood). In fact, if the prior distribution is a conjugate prior, such that the prior and posterior distributions come from the same family, it can be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the conjugate prior article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.

Mathematical properties
Interpretation of factor
P
              (
              E
              ∣
              M
              )
            
            
              P
              (
              E
              )
            
          
        
        >
        1
        ⇒
        P
        (
        E
        ∣
        M
        )
        >
        P
        (
        E
        )
      
    
    {\textstyle {\frac {P(E\mid M)}{P(E)}}>1\Rightarrow P(E\mid M)>P(E)}
  
. That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, 
  
    
      
        
          
            
              P
              (
              E
              ∣
              M
              )
            
            
              P
              (
              E
              )
            
          
        
        =
        1
        ⇒
        P
        (
        E
        ∣
        M
        )
        =
        P
        (
        E
        )
      
    
    {\textstyle {\frac {P(E\mid M)}{P(E)}}=1\Rightarrow P(E\mid M)=P(E)}
  
. That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.

Cromwell's rule
If 
  
    
      
        P
        (
        M
        )
        =
        0
      
    
    {\displaystyle P(M)=0}
  
 then 
  
    
      
        P
        (
        M
        ∣
        E
        )
        =
        0
      
    
    {\displaystyle P(M\mid E)=0}
  
. If 
  
    
      
        P
        (
        M
        )
        =
        1
      
    
    {\displaystyle P(M)=1}
  
 and 
  
    
      
        P
        (
        E
        )
        >
        0
      
    
    {\displaystyle P(E)>0}
  
, then 
  
    
      
        P
        (
        M
        
          |
        
        E
        )
        =
        1
      
    
    {\displaystyle P(M|E)=1}
  
. This can be interpreted to mean that hard convictions are insensitive to counter-evidence.
The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event "not 
  
    
      
        M
      
    
    {\displaystyle M}
  
" in place of "
  
    
      
        M
      
    
    {\displaystyle M}
  
", yielding "if 
  
    
      
        1
        −
        P
        (
        M
        )
        =
        0
      
    
    {\displaystyle 1-P(M)=0}
  
, then 
  
    
      
        1
        −
        P
        (
        M
        ∣
        E
        )
        =
        0
      
    
    {\displaystyle 1-P(M\mid E)=0}
  
", from which the result immediately follows.

Asymptotic behaviour of posterior
Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials. For sufficiently nice prior probabilities, the Bernstein-von Mises theorem gives that in the limit of infinite trials, the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L. Doob in 1948, namely if the random variable in consideration has a finite probability space. The more general results were obtained later by the statistician David A. Freedman who published in two seminal research papers in 1963  and 1965  when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable probability space (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the Bernstein-von Mises theorem is not applicable. In this case there is almost surely no asymptotic convergence. Later in the 1980s and 1990s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces. To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.

Conjugate priors
In parameterized form, the prior distribution is often assumed to come from a family of distributions called conjugate priors. The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in closed form.

Estimates of parameters and predictions
It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution.
For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a robust estimator.
If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.

  
    
      
        
          
            
              θ
              ~
            
          
        
        =
        E
        ⁡
        [
        θ
        ]
        =
        ∫
        θ
        
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
        
        d
        θ
      
    
    {\displaystyle {\tilde {\theta }}=\operatorname {E} [\theta ]=\int \theta \,p(\theta \mid \mathbf {X} ,\alpha )\,d\theta }
  

Taking a value with the greatest probability defines maximum a posteriori (MAP) estimates:

  
    
      
        {
        
          θ
          
            MAP
          
        
        }
        ⊂
        arg
        ⁡
        
          max
          
            θ
          
        
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
        .
      
    
    {\displaystyle \{\theta _{\text{MAP}}\}\subset \arg \max _{\theta }p(\theta \mid \mathbf {X} ,\alpha ).}
  

There are examples where no maximum is attained, in which case the set of MAP estimates is empty.
There are other methods of estimation that minimize the posterior risk (expected-posterior loss) with respect to a loss function, and these are of interest to statistical decision theory using the sampling distribution ("frequentist statistics").
The posterior predictive distribution of a new observation 
  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\tilde {x}}}
  
 (that is independent of previous observations) is determined by

  
    
      
        p
        (
        
          
            
              x
              ~
            
          
        
        
          |
        
        
          X
        
        ,
        α
        )
        =
        ∫
        p
        (
        
          
            
              x
              ~
            
          
        
        ,
        θ
        ∣
        
          X
        
        ,
        α
        )
        
        d
        θ
        =
        ∫
        p
        (
        
          
            
              x
              ~
            
          
        
        ∣
        θ
        )
        p
        (
        θ
        ∣
        
          X
        
        ,
        α
        )
        
        d
        θ
        .
      
    
    {\displaystyle p({\tilde {x}}|\mathbf {X} ,\alpha )=\int p({\tilde {x}},\theta \mid \mathbf {X} ,\alpha )\,d\theta =\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )\,d\theta .}

Examples
Probability of a hypothesis
Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?
Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  
 correspond to bowl #1, and 
  
    
      
        
          H
          
            2
          
        
      
    
    {\displaystyle H_{2}}
  
 to bowl #2.
It is given that the bowls are identical from Fred's point of view, thus 
  
    
      
        P
        (
        
          H
          
            1
          
        
        )
        =
        P
        (
        
          H
          
            2
          
        
        )
      
    
    {\displaystyle P(H_{1})=P(H_{2})}
  
, and the two must add up to 1, so both are equal to 0.5.
The event 
  
    
      
        E
      
    
    {\displaystyle E}
  
 is the observation of a plain cookie. From the contents of the bowls, we know that 
  
    
      
        P
        (
        E
        ∣
        
          H
          
            1
          
        
        )
        =
        30
        
          /
        
        40
        =
        0.75
      
    
    {\displaystyle P(E\mid H_{1})=30/40=0.75}
  
 and 
  
    
      
        P
        (
        E
        ∣
        
          H
          
            2
          
        
        )
        =
        20
        
          /
        
        40
        =
        0.5.
      
    
    {\displaystyle P(E\mid H_{2})=20/40=0.5.}
  
 Bayes' formula then yields

  
    
      
        
          
            
              
                P
                (
                
                  H
                  
                    1
                  
                
                ∣
                E
                )
              
              
                
                =
                
                  
                    
                      P
                      (
                      E
                      ∣
                      
                        H
                        
                          1
                        
                      
                      )
                      
                      P
                      (
                      
                        H
                        
                          1
                        
                      
                      )
                    
                    
                      P
                      (
                      E
                      ∣
                      
                        H
                        
                          1
                        
                      
                      )
                      
                      P
                      (
                      
                        H
                        
                          1
                        
                      
                      )
                      
                      +
                      
                      P
                      (
                      E
                      ∣
                      
                        H
                        
                          2
                        
                      
                      )
                      
                      P
                      (
                      
                        H
                        
                          2
                        
                      
                      )
                    
                  
                
              
            
            
              
            
            
              
                 
              
              
                
                =
                
                  
                    
                      0.75
                      ×
                      0.5
                    
                    
                      0.75
                      ×
                      0.5
                      +
                      0.5
                      ×
                      0.5
                    
                  
                
              
            
            
              
            
            
              
                 
              
              
                
                =
                0.6
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}P(H_{1}\mid E)&={\frac {P(E\mid H_{1})\,P(H_{1})}{P(E\mid H_{1})\,P(H_{1})\;+\;P(E\mid H_{2})\,P(H_{2})}}\\\\\ &={\frac {0.75\times 0.5}{0.75\times 0.5+0.5\times 0.5}}\\\\\ &=0.6\end{aligned}}}
  

Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, 
  
    
      
        P
        (
        
          H
          
            1
          
        
        )
      
    
    {\displaystyle P(H_{1})}
  
, which was 0.5. After observing the cookie, we must revise the probability to 
  
    
      
        P
        (
        
          H
          
            1
          
        
        ∣
        E
        )
      
    
    {\displaystyle P(H_{1}\mid E)}
  
, which is 0.6.

Making a prediction
An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?
The degree of belief in the continuous variable 
  
    
      
        C
      
    
    {\displaystyle C}
  
 (century) is to be calculated, with the discrete set of events 
  
    
      
        {
        G
        D
        ,
        G
        
          
            
              D
              ¯
            
          
        
        ,
        
          
            
              G
              ¯
            
          
        
        D
        ,
        
          
            
              G
              ¯
            
          
        
        
          
            
              D
              ¯
            
          
        
        }
      
    
    {\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}
  
 as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,

  
    
      
        P
        (
        E
        =
        G
        D
        ∣
        C
        =
        c
        )
        =
        (
        0.01
        +
        
          
            
              0.81
              −
              0.01
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
        (
        0.5
        −
        
          
            
              0.5
              −
              0.05
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
      
    
    {\displaystyle P(E=GD\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}
  

  
    
      
        P
        (
        E
        =
        G
        
          
            
              D
              ¯
            
          
        
        ∣
        C
        =
        c
        )
        =
        (
        0.01
        +
        
          
            
              0.81
              −
              0.01
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
        (
        0.5
        +
        
          
            
              0.5
              −
              0.05
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
      
    
    {\displaystyle P(E=G{\bar {D}}\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}
  

  
    
      
        P
        (
        E
        =
        
          
            
              G
              ¯
            
          
        
        D
        ∣
        C
        =
        c
        )
        =
        (
        (
        1
        −
        0.01
        )
        −
        
          
            
              0.81
              −
              0.01
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
        (
        0.5
        −
        
          
            
              0.5
              −
              0.05
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
      
    
    {\displaystyle P(E={\bar {G}}D\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}
  

  
    
      
        P
        (
        E
        =
        
          
            
              G
              ¯
            
          
        
        
          
            
              D
              ¯
            
          
        
        ∣
        C
        =
        c
        )
        =
        (
        (
        1
        −
        0.01
        )
        −
        
          
            
              0.81
              −
              0.01
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
        (
        0.5
        +
        
          
            
              0.5
              −
              0.05
            
            
              16
              −
              11
            
          
        
        (
        c
        −
        11
        )
        )
      
    
    {\displaystyle P(E={\bar {G}}{\bar {D}}\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}
  

Assume a uniform prior of 
  
    
      
        
          f
          
            C
          
        
        (
        c
        )
        =
        0.2
      
    
    {\textstyle f_{C}(c)=0.2}
  
, and that trials are independent and identically distributed. When a new fragment of type 
  
    
      
        e
      
    
    {\displaystyle e}
  
 is discovered, Bayes' theorem is applied to update the degree of belief for each 
  
    
      
        c
      
    
    {\displaystyle c}
  
:

  
    
      
        
          f
          
            C
          
        
        (
        c
        ∣
        E
        =
        e
        )
        =
        
          
            
              P
              (
              E
              =
              e
              ∣
              C
              =
              c
              )
            
            
              P
              (
              E
              =
              e
              )
            
          
        
        
          f
          
            C
          
        
        (
        c
        )
        =
        
          
            
              P
              (
              E
              =
              e
              ∣
              C
              =
              c
              )
            
            
              
                ∫
                
                  11
                
                
                  16
                
              
              
                P
                (
                E
                =
                e
                ∣
                C
                =
                c
                )
                
                  f
                  
                    C
                  
                
                (
                c
                )
                d
                c
              
            
          
        
        
          f
          
            C
          
        
        (
        c
        )
      
    
    {\displaystyle f_{C}(c\mid E=e)={\frac {P(E=e\mid C=c)}{P(E=e)}}f_{C}(c)={\frac {P(E=e\mid C=c)}{\int _{11}^{16}{P(E=e\mid C=c)f_{C}(c)dc}}}f_{C}(c)}
  

A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or 
  
    
      
        c
        =
        15.2
      
    
    {\displaystyle c=15.2}
  
. By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. The Bernstein-von Mises theorem asserts here the asymptotic convergence to the "true" distribution because the probability space corresponding to the discrete set of events 
  
    
      
        {
        G
        D
        ,
        G
        
          
            
              D
              ¯
            
          
        
        ,
        
          
            
              G
              ¯
            
          
        
        D
        ,
        
          
            
              G
              ¯
            
          
        
        
          
            
              D
              ¯
            
          
        
        }
      
    
    {\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}
  
 is finite (see above section on asymptotic behaviour of the posterior).

In frequentist statistics and decision theory
A decision-theoretic justification of the use of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.
Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation, hypothesis testing, and computing confidence intervals. For example:

"Under some conditions, all admissible procedures are either Bayes procedures or limits of Bayes procedures (in various senses). These remarkable results, at least in their original form, are due essentially to Wald. They are useful because the property of being Bayes is easier to analyze than admissibility."
"In decision theory, a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution."
"In the first chapters of this work, prior distributions with finite support and the corresponding Bayes procedures were used to establish some of the main theorems relating to the comparison of experiments. Bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics, including its asymptotic theory." "There are many problems where a glance at posterior distributions, for suitable priors, yields immediately interesting information. Also, this technique can hardly be avoided in sequential analysis."
"A useful fact is that any Bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible"
"An important area of investigation in the development of admissibility ideas has been that of conventional sampling-theory procedures, and many interesting results have been obtained."

Model selection
Bayesian methodology also plays a role in model selection where the aim is to select one model from a set of competing models that represents most closely the underlying process that generated the observed data. In Bayesian model comparison, the model with the highest posterior probability given the data is selected. The posterior probability of a model depends on the evidence, or marginal likelihood, which reflects the probability that the data is generated by the model, and on the prior belief of the model. When two competing models are a priori considered to be equiprobable, the ratio of their posterior probabilities corresponds to the Bayes factor. Since Bayesian model comparison is aimed on selecting the model with the highest posterior probability, this methodology is also referred to as the maximum a posteriori (MAP) selection rule  or the MAP probability rule.

Probabilistic programming
While conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving PPLs to handle the computational details for them.

Applications
Statistical data analysis
See the separate Wikipedia entry on Bayesian statistics, specifically the statistical modeling section in that page.

Computer applications
Bayesian inference has applications in artificial intelligence and expert systems.  Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s. There is also an ever-growing connection between Bayesian methods and simulation-based Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a graphical model structure may allow for efficient simulation algorithms like the Gibbs sampling and other Metropolis–Hastings algorithm schemes. Recently Bayesian inference has gained popularity among the phylogenetics community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.
As applied to statistical classification, Bayesian inference has been used to develop algorithms for identifying e-mail spam. Applications which make use of Bayesian inference for spam filtering include CRM114, DSPAM, Bogofilter, SpamAssassin, SpamBayes, Mozilla, XEAMS, and others. Spam classification is treated in more detail in the article on the naïve Bayes classifier.
Solomonoff's Inductive inference is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and Occam's Razor. Solomonoff's universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs (for a universal computer) that compute something starting with p. Given some p and any computable but unknown probability distribution from which x is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of x in optimal fashion.

Bioinformatics and healthcare applications
Bayesian inference has been applied in different Bioinformatics applications, including differential gene expression analysis. Bayesian inference is also used in a general cancer risk model, called CIRI (Continuous Individualized Risk Index), where serial measurements are incorporated to update a Bayesian model which is primarily built from prior knowledge.

In the courtroom
Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for "beyond a reasonable doubt". Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in odds form, as betting odds are more widely understood than probabilities. Alternatively, a logarithmic approach, replacing multiplication with addition, might be easier for a jury to handle.

If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population. For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.
The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence expert witness explained Bayes' theorem to the jury in R v Adams. The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task."
Gardner-Medwin argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt, but rather the probability of the evidence, given that the defendant is innocent (akin to a frequentist p-value). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:

A – the known facts and testimony could have arisen if the defendant is guilty.
B – the known facts and testimony could have arisen if the defendant is innocent.
C – the defendant is guilty.
Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also Lindley's paradox.

Bayesian epistemology
Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.
Karl Popper and David Miller have rejected the idea of Bayesian rationalism, i.e. using Bayes rule to make epistemological inferences: It is prone to the same vicious circle as any other justificationist epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification, rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.

Other
The scientific method is sometimes interpreted as an application of Bayesian inference. In this view, Bayes' rule guides (or should guide) the updating of probabilities about hypotheses conditional on new observations or experiments. The Bayesian inference has also been applied to treat stochastic scheduling problems with incomplete information by Cai et al. (2009).
Bayesian search theory is used to search for lost objects.
Bayesian inference in phylogeny
Bayesian tool for methylation analysis
Bayesian approaches to brain function  investigate the brain as a Bayesian mechanism.
Bayesian inference in ecological studies
Bayesian inference is used to estimate parameters in stochastic chemical kinetic models
Bayesian inference in econophysics for currency or stock market prediction
Bayesian inference in marketing
Bayesian inference in motor learning
Bayesian inference is used in probabilistic numerics to solve numerical problems

Bayes and Bayesian inference
The problem considered by Bayes in Proposition 9 of his essay, "An Essay Towards Solving a Problem in the Doctrine of Chances", is the posterior distribution for the parameter a (the success rate) of the binomial distribution.

History
The term Bayesian refers to Thomas Bayes (1701–1761), who proved that probabilistic limits could be placed on an unknown event.   However, it was Pierre-Simon Laplace (1749–1827) who introduced (as Principle VI) what is now called Bayes' theorem and used it to address problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called "inverse probability" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, "inverse probability" was largely supplanted  by a collection of methods that came to be called frequentist statistics.
In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to objective and subjective currents in Bayesian practice. In the objective or "non-informative" current, the statistical analysis depends on only the model assumed, the data analyzed, and the method assigning the prior, which differs from one objective Bayesian practitioner to another. In the subjective or "informative" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications. Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics. Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of machine learning.

See also
References
Citations
Sources
Further reading
For a full report on the history of Bayesian statistics and the debates with frequentists approaches, read Vallverdu, Jordi (2016). Bayesians Versus Frequentists A Philosophical Debate on Statistical Reasoning. New York: Springer. ISBN 978-3-662-48638-2.
Clayton, Aubrey (August 2021). Bernoulli's Fallacy: Statistical Illogic and the Crisis of Modern Science. Columbia University Press. ISBN 978-0-231-55335-3.

Elementary
The following books are listed in ascending order of probabilistic sophistication:

Stone, JV (2013), "Bayes' Rule: A Tutorial Introduction to Bayesian Analysis",   Download first  chapter here, Sebtel Press, England.
Dennis V. Lindley (2013). Understanding Uncertainty, Revised Edition (2nd ed.). John Wiley. ISBN 978-1-118-65012-7.
Colin Howson & Peter Urbach (2005). Scientific Reasoning: The Bayesian Approach (3rd ed.). Open Court Publishing Company. ISBN 978-0-8126-9578-6.
Berry, Donald A. (1996). Statistics: A Bayesian Perspective. Duxbury. ISBN 978-0-534-23476-8.
Morris H. DeGroot & Mark J. Schervish (2002). Probability and Statistics (third ed.). Addison-Wesley. ISBN 978-0-201-52488-8.
Bolstad, William M. (2007) Introduction to Bayesian Statistics: Second Edition, John Wiley ISBN 0-471-27020-2
Winkler, Robert L (2003). Introduction to Bayesian Inference and Decision (2nd ed.). Probabilistic. ISBN 978-0-9647938-4-2. Updated classic textbook. Bayesian theory clearly presented.
Lee, Peter M. Bayesian Statistics: An Introduction. Fourth Edition (2012), John Wiley ISBN 978-1-1183-3257-3
Carlin, Bradley P. & Louis, Thomas A. (2008). Bayesian Methods for Data Analysis, Third Edition. Boca Raton, FL: Chapman and Hall/CRC. ISBN 978-1-58488-697-6.
Gelman, Andrew; Carlin, John B.; Stern, Hal S.; Dunson, David B.; Vehtari, Aki; Rubin, Donald B. (2013). Bayesian Data Analysis, Third Edition. Chapman and Hall/CRC. ISBN 978-1-4398-4095-5.

Intermediate or advanced
Berger, James O (1985). Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics (Second ed.). Springer-Verlag. Bibcode:1985sdtb.book.....B. ISBN 978-0-387-96098-2.
Bernardo, José M.; Smith, Adrian F. M. (1994). Bayesian Theory. Wiley.
DeGroot, Morris H., Optimal Statistical Decisions. Wiley Classics Library. 2004. (Originally published (1970) by McGraw-Hill.) ISBN 0-471-68029-X.
Schervish, Mark J. (1995). Theory of statistics. Springer-Verlag. ISBN 978-0-387-94546-0.
Jaynes, E. T. (1998). Probability Theory: The Logic of Science.
O'Hagan, A. and Forster, J. (2003). Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference. Arnold, New York. ISBN 0-340-52922-9.
Robert, Christian P (2007). The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation (paperback ed.). Springer. ISBN 978-0-387-71598-8.
Pearl, Judea. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo, CA: Morgan Kaufmann.
Pierre Bessière et al. (2013). "Bayesian Programming". CRC Press. ISBN 9781439880326
Francisco J. Samaniego (2010). "A Comparison of the Bayesian and Frequentist Approaches to Estimation". Springer. New York, ISBN 978-1-4419-5940-9

External links
"Bayesian approach to statistical problems", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Bayesian Statistics from Scholarpedia.
Introduction to Bayesian probability from Queen Mary University of London
Mathematical Notes on Bayesian Statistics and Markov Chain Monte Carlo
Bayesian reading list Archived 2011-06-25 at the Wayback Machine, categorized and annotated by Tom Griffiths
A. Hajek and S. Hartmann: Bayesian Epistemology, in: J. Dancy et al. (eds.), A Companion to Epistemology. Oxford: Blackwell 2010, 93–106.
S. Hartmann and J. Sprenger: Bayesian Epistemology, in: S. Bernecker and D. Pritchard (eds.), Routledge Companion to Epistemology. London: Routledge 2010, 609–620.
Stanford Encyclopedia of Philosophy: "Inductive Logic"
Bayesian Confirmation Theory (PDF)
What is Bayesian Learning?
Data, Uncertainty and Inference — Informal introduction with many examples, ebook (PDF) freely available at causaScientia
A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

Graphical model
Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 
  
    
      
        m
      
    
    {\displaystyle m}
  
 parent nodes represent 
  
    
      
        m
      
    
    {\displaystyle m}
  
 Boolean variables, then the probability function could be represented by a table of 
  
    
      
        
          2
          
            m
          
        
      
    
    {\displaystyle 2^{m}}
  
 entries, one entry for each of the 
  
    
      
        
          2
          
            m
          
        
      
    
    {\displaystyle 2^{m}}
  
 possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.

Example
Let us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
The joint probability function is, by the chain rule of probability,

  
    
      
        Pr
        (
        G
        ,
        S
        ,
        R
        )
        =
        Pr
        (
        G
        ∣
        S
        ,
        R
        )
        Pr
        (
        S
        ∣
        R
        )
        Pr
        (
        R
        )
      
    
    {\displaystyle \Pr(G,S,R)=\Pr(G\mid S,R)\Pr(S\mid R)\Pr(R)}
  

where G = "Grass wet (true/false)", S = "Sprinkler turned on (true/false)", and R = "Raining (true/false)".
The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like "What is the probability that it is raining, given the grass is wet?" by using the conditional probability formula and summing over all nuisance variables:

  
    
      
        Pr
        (
        R
        =
        T
        ∣
        G
        =
        T
        )
        =
        
          
            
              Pr
              (
              G
              =
              T
              ,
              R
              =
              T
              )
            
            
              Pr
              (
              G
              =
              T
              )
            
          
        
        =
        
          
            
              
                ∑
                
                  x
                  ∈
                  {
                  T
                  ,
                  F
                  }
                
              
              Pr
              (
              G
              =
              T
              ,
              S
              =
              x
              ,
              R
              =
              T
              )
            
            
              
                ∑
                
                  x
                  ,
                  y
                  ∈
                  {
                  T
                  ,
                  F
                  }
                
              
              Pr
              (
              G
              =
              T
              ,
              S
              =
              x
              ,
              R
              =
              y
              )
            
          
        
      
    
    {\displaystyle \Pr(R=T\mid G=T)={\frac {\Pr(G=T,R=T)}{\Pr(G=T)}}={\frac {\sum _{x\in \{T,F\}}\Pr(G=T,S=x,R=T)}{\sum _{x,y\in \{T,F\}}\Pr(G=T,S=x,R=y)}}}
  

Using the expansion for the joint probability function 
  
    
      
        Pr
        (
        G
        ,
        S
        ,
        R
        )
      
    
    {\displaystyle \Pr(G,S,R)}
  
 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,

  
    
      
        
          
            
              
                Pr
                (
                G
                =
                T
                ,
                S
                =
                T
                ,
                R
                =
                T
                )
              
              
                
                =
                Pr
                (
                G
                =
                T
                ∣
                S
                =
                T
                ,
                R
                =
                T
                )
                Pr
                (
                S
                =
                T
                ∣
                R
                =
                T
                )
                Pr
                (
                R
                =
                T
                )
              
            
            
              
              
                
                =
                0.99
                ×
                0.01
                ×
                0.2
              
            
            
              
              
                
                =
                0.00198.
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(G=T,S=T,R=T)&=\Pr(G=T\mid S=T,R=T)\Pr(S=T\mid R=T)\Pr(R=T)\\&=0.99\times 0.01\times 0.2\\&=0.00198.\end{aligned}}}
  

Then the numerical results (subscripted by the associated variable values) are

  
    
      
        Pr
        (
        R
        =
        T
        ∣
        G
        =
        T
        )
        =
        
          
            
              
                0.00198
                
                  T
                  T
                  T
                
              
              +
              
                0.1584
                
                  T
                  F
                  T
                
              
            
            
              
                0.00198
                
                  T
                  T
                  T
                
              
              +
              
                0.288
                
                  T
                  T
                  F
                
              
              +
              
                0.1584
                
                  T
                  F
                  T
                
              
              +
              
                0.0
                
                  T
                  F
                  F
                
              
            
          
        
        =
        
          
            891
            2491
          
        
        ≈
        35.77
        %
        .
      
    
    {\displaystyle \Pr(R=T\mid G=T)={\frac {0.00198_{TTT}+0.1584_{TFT}}{0.00198_{TTT}+0.288_{TTF}+0.1584_{TFT}+0.0_{TFF}}}={\frac {891}{2491}}\approx 35.77\%.}
  

To answer an interventional question, such as "What is the probability that it would rain, given that we wet the grass?" the answer is governed by the post-intervention joint distribution function

  
    
      
        Pr
        (
        S
        ,
        R
        ∣
        
          do
        
        (
        G
        =
        T
        )
        )
        =
        Pr
        (
        S
        ∣
        R
        )
        Pr
        (
        R
        )
      
    
    {\displaystyle \Pr(S,R\mid {\text{do}}(G=T))=\Pr(S\mid R)\Pr(R)}
  

obtained by removing the factor 
  
    
      
        Pr
        (
        G
        ∣
        S
        ,
        R
        )
      
    
    {\displaystyle \Pr(G\mid S,R)}
  
 from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:

  
    
      
        Pr
        (
        R
        ∣
        
          do
        
        (
        G
        =
        T
        )
        )
        =
        Pr
        (
        R
        )
        .
      
    
    {\displaystyle \Pr(R\mid {\text{do}}(G=T))=\Pr(R).}
  

To predict the impact of turning the sprinkler on:

  
    
      
        Pr
        (
        R
        ,
        G
        ∣
        
          do
        
        (
        S
        =
        T
        )
        )
        =
        Pr
        (
        R
        )
        Pr
        (
        G
        ∣
        R
        ,
        S
        =
        T
        )
      
    
    {\displaystyle \Pr(R,G\mid {\text{do}}(S=T))=\Pr(R)\Pr(G\mid R,S=T)}
  

with the term 
  
    
      
        Pr
        (
        S
        =
        T
        ∣
        R
        )
      
    
    {\displaystyle \Pr(S=T\mid R)}
  
 removed, showing that the action affects the grass but not the rain.
These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 
  
    
      
        
          do
        
        (
        x
        )
      
    
    {\displaystyle {\text{do}}(x)}
  
 can still be predicted, however, whenever the back-door criterion is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then

  
    
      
        Pr
        (
        Y
        ,
        Z
        ∣
        
          do
        
        (
        x
        )
        )
        =
        
          
            
              Pr
              (
              Y
              ,
              Z
              ,
              X
              =
              x
              )
            
            
              Pr
              (
              X
              =
              x
              ∣
              Z
              )
            
          
        
        .
      
    
    {\displaystyle \Pr(Y,Z\mid {\text{do}}(x))={\frac {\Pr(Y,Z,X=x)}{\Pr(X=x\mid Z)}}.}
  

A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called "sufficient" or "admissible." For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not "identified". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of "do-calculus" and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.
Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 
  
    
      
        
          2
          
            10
          
        
        =
        1024
      
    
    {\displaystyle 2^{10}=1024}
  
 values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 
  
    
      
        10
        ⋅
        
          2
          
            3
          
        
        =
        80
      
    
    {\displaystyle 10\cdot 2^{3}=80}
  
 values.
One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.

Inference and learning
Bayesian networks perform three main inference tasks:

Inferring unobserved variables
Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.

Parameter learning
In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)
Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.
A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.

Structure learning
In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.
Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl and rests on the distinction between the three possible patterns allowed in a 3-node DAG:

The first 2 represent the same dependencies (
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Z
      
    
    {\displaystyle Z}
  
 are independent given 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Z
      
    
    {\displaystyle Z}
  
 are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Z
      
    
    {\displaystyle Z}
  
 have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.
An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Such method can handle problems with up to 100 variables.
In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.
Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.
Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.

Statistical introduction
Given data 
  
    
      
        x
        
        
      
    
    {\displaystyle x\,\!}
  
 and parameter 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
, a simple Bayesian analysis starts with a prior probability (prior) 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
  
 and likelihood 
  
    
      
        p
        (
        x
        ∣
        θ
        )
      
    
    {\displaystyle p(x\mid \theta )}
  
 to compute a posterior probability 
  
    
      
        p
        (
        θ
        ∣
        x
        )
        ∝
        p
        (
        x
        ∣
        θ
        )
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}
  
.
Often the prior on 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
 depends in turn on other parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 that are not mentioned in the likelihood. So, the prior 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
  
 must be replaced by a likelihood 
  
    
      
        p
        (
        θ
        ∣
        φ
        )
      
    
    {\displaystyle p(\theta \mid \varphi )}
  
, and a prior 
  
    
      
        p
        (
        φ
        )
      
    
    {\displaystyle p(\varphi )}
  
 on the newly introduced parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 is required, resulting in a posterior probability

  
    
      
        p
        (
        θ
        ,
        φ
        ∣
        x
        )
        ∝
        p
        (
        x
        ∣
        θ
        )
        p
        (
        θ
        ∣
        φ
        )
        p
        (
        φ
        )
        .
      
    
    {\displaystyle p(\theta ,\varphi \mid x)\propto p(x\mid \theta )p(\theta \mid \varphi )p(\varphi ).}
  

This is the simplest example of a hierarchical Bayes model.
The process may be repeated; for example, the parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
  
 may depend in turn on additional parameters 
  
    
      
        ψ
        
        
      
    
    {\displaystyle \psi \,\!}
  
, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.

Introductory examples
Given the measured quantities 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}\,\!}
  
each with normally distributed errors of known standard deviation 
  
    
      
        σ
        
        
      
    
    {\displaystyle \sigma \,\!}
  
,

  
    
      
        
          x
          
            i
          
        
        ∼
        N
        (
        
          θ
          
            i
          
        
        ,
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle x_{i}\sim N(\theta _{i},\sigma ^{2})}
  

Suppose we are interested in estimating the 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  
. An approach would be to estimate the 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  
 using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply

  
    
      
        
          θ
          
            i
          
        
        =
        
          x
          
            i
          
        
        .
      
    
    {\displaystyle \theta _{i}=x_{i}.}
  

However, if the quantities are related, so that for example the individual 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  
have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,

  
    
      
        
          x
          
            i
          
        
        ∼
        N
        (
        
          θ
          
            i
          
        
        ,
        
          σ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle x_{i}\sim N(\theta _{i},\sigma ^{2}),}
  

  
    
      
        
          θ
          
            i
          
        
        ∼
        N
        (
        φ
        ,
        
          τ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle \theta _{i}\sim N(\varphi ,\tau ^{2}),}
  

with improper priors 
  
    
      
        φ
        ∼
        
          flat
        
      
    
    {\displaystyle \varphi \sim {\text{flat}}}
  
, 
  
    
      
        τ
        ∼
        
          flat
        
        ∈
        (
        0
        ,
        ∞
        )
      
    
    {\displaystyle \tau \sim {\text{flat}}\in (0,\infty )}
  
. When 
  
    
      
        n
        ≥
        3
      
    
    {\displaystyle n\geq 3}
  
, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  
 will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.

Restrictions on priors
Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable 
  
    
      
        τ
        
        
      
    
    {\displaystyle \tau \,\!}
  
 in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.

Definitions and concepts
Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.

Factorization definition
X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:

  
    
      
        p
        (
        x
        )
        =
        
          ∏
          
            v
            ∈
            V
          
        
        p
        
          (
          
            
              x
              
                v
              
            
            
            
              
                |
              
            
            
            
              x
              
                pa
                ⁡
                (
                v
                )
              
            
          
          )
        
      
    
    {\displaystyle p(x)=\prod _{v\in V}p\left(x_{v}\,{\big |}\,x_{\operatorname {pa} (v)}\right)}
  

where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).
For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:

  
    
      
        P
        ⁡
        (
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        )
        =
        
          ∏
          
            v
            =
            1
          
          
            n
          
        
        P
        ⁡
        
          (
          
            
              X
              
                v
              
            
            =
            
              x
              
                v
              
            
            ∣
            
              X
              
                v
                +
                1
              
            
            =
            
              x
              
                v
                +
                1
              
            
            ,
            …
            ,
            
              X
              
                n
              
            
            =
            
              x
              
                n
              
            
          
          )
        
      
    
    {\displaystyle \operatorname {P} (X_{1}=x_{1},\ldots ,X_{n}=x_{n})=\prod _{v=1}^{n}\operatorname {P} \left(X_{v}=x_{v}\mid X_{v+1}=x_{v+1},\ldots ,X_{n}=x_{n}\right)}
  

Using the definition above, this can be written as:

  
    
      
        P
        ⁡
        (
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        )
        =
        
          ∏
          
            v
            =
            1
          
          
            n
          
        
        P
        ⁡
        (
        
          X
          
            v
          
        
        =
        
          x
          
            v
          
        
        ∣
        
          X
          
            j
          
        
        =
        
          x
          
            j
          
        
        
           for each 
        
        
          X
          
            j
          
        
        
        
           that is a parent of 
        
        
          X
          
            v
          
        
        
        )
      
    
    {\displaystyle \operatorname {P} (X_{1}=x_{1},\ldots ,X_{n}=x_{n})=\prod _{v=1}^{n}\operatorname {P} (X_{v}=x_{v}\mid X_{j}=x_{j}{\text{ for each }}X_{j}\,{\text{ that is a parent of }}X_{v}\,)}
  

The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.

Local Markov property
X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:

  
    
      
        
          X
          
            v
          
        
        ⊥
        
        
        
        ⊥
        
          X
          
            V
            
            ∖
            
            de
            ⁡
            (
            v
            )
          
        
        ∣
        
          X
          
            pa
            ⁡
            (
            v
            )
          
        
        
        
          for all 
        
        v
        ∈
        V
      
    
    {\displaystyle X_{v}\perp \!\!\!\perp X_{V\,\smallsetminus \,\operatorname {de} (v)}\mid X_{\operatorname {pa} (v)}\quad {\text{for all }}v\in V}
  

where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v.
This can be expressed in terms similar to the first definition, as

  
    
      
        
          
            
              
              
                P
                ⁡
                (
                
                  X
                  
                    v
                  
                
                =
                
                  x
                  
                    v
                  
                
                ∣
                
                  X
                  
                    i
                  
                
                =
                
                  x
                  
                    i
                  
                
                
                   for each 
                
                
                  X
                  
                    i
                  
                
                
                   that is not a descendant of 
                
                
                  X
                  
                    v
                  
                
                
                )
              
            
            
              
                =
                

                
              
              
                P
                (
                
                  X
                  
                    v
                  
                
                =
                
                  x
                  
                    v
                  
                
                ∣
                
                  X
                  
                    j
                  
                
                =
                
                  x
                  
                    j
                  
                
                
                   for each 
                
                
                  X
                  
                    j
                  
                
                
                   that is a parent of 
                
                
                  X
                  
                    v
                  
                
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&\operatorname {P} (X_{v}=x_{v}\mid X_{i}=x_{i}{\text{ for each }}X_{i}{\text{ that is not a descendant of }}X_{v}\,)\\[6pt]={}&P(X_{v}=x_{v}\mid X_{j}=x_{j}{\text{ for each }}X_{j}{\text{ that is a parent of }}X_{v}\,)\end{aligned}}}
  

The set of parents is a subset of the set of non-descendants because the graph is acyclic.

Marginal independence structure
In general, learning a Bayesian network from data is known to be NP-hard. This is due in part to the combinatorial explosion of enumerating DAGs as the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure: while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by a simple undirected graph with special properties such as equal intersection and independence numbers.

Developing Bayesian networks
Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.

Markov blanket
The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.

d-separation
This definition can be made more general by defining the "d"-separation of two nodes, where d stands for directional. We first define the "d"-separation of a trail and then we will define the "d"-separation of two nodes in terms of that.
Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:

P contains (but does not need to be entirely) a directed chain, 
  
    
      
        u
        ⋯
        ←
        m
        ←
        ⋯
        v
      
    
    {\displaystyle u\cdots \leftarrow m\leftarrow \cdots v}
  
 or 
  
    
      
        u
        ⋯
        →
        m
        →
        ⋯
        v
      
    
    {\displaystyle u\cdots \rightarrow m\rightarrow \cdots v}
  
, such that the middle node m is in Z,
P contains a fork, 
  
    
      
        u
        ⋯
        ←
        m
        →
        ⋯
        v
      
    
    {\displaystyle u\cdots \leftarrow m\rightarrow \cdots v}
  
, such that the middle node m is in Z, or
P contains an inverted fork (or collider), 
  
    
      
        u
        ⋯
        →
        m
        ←
        ⋯
        v
      
    
    {\displaystyle u\cdots \rightarrow m\leftarrow \cdots v}
  
, such that the middle node m is not in Z and no descendant of m is in Z.
The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.
X is a Bayesian network with respect to G if, for any two nodes u, v:

  
    
      
        
          X
          
            u
          
        
        ⊥
        
        
        
        ⊥
        
          X
          
            v
          
        
        ∣
        
          X
          
            Z
          
        
      
    
    {\displaystyle X_{u}\perp \!\!\!\perp X_{v}\mid X_{Z}}
  

where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)

Causal networks
Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:

  
    
      
        a
        →
        b
        →
        c
        
        
          and
        
        
        a
        ←
        b
        ←
        c
      
    
    {\displaystyle a\rightarrow b\rightarrow c\qquad {\text{and}}\qquad a\leftarrow b\leftarrow c}
  

are equivalent: that is they impose exactly the same conditional independence requirements.
A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x. Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.

Inference complexity and approximation algorithms
In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.
In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 
  
    
      
        1
        
          /
        
        p
        (
        n
        )
      
    
    {\displaystyle 1/p(n)}
  
 where 
  
    
      
        p
        (
        n
        )
      
    
    {\displaystyle p(n)}
  
 was any polynomial of the number of nodes in the network, 
  
    
      
        n
      
    
    {\displaystyle n}
  
.

Software
Notable software for Bayesian networks include:

Just another Gibbs sampler (JAGS) – Open-source alternative to WinBUGS. Uses Gibbs sampling.
OpenBUGS – Open-source development of WinBUGS.
SPSS Modeler – Commercial software that includes an implementation for Bayesian networks.
Stan (software) – Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo.
PyMC – A Python library implementing an embedded domain specific language to represent bayesian networks, and a variety of samplers (including NUTS)
WinBUGS – One of the first computational implementations of MCMC samplers. No longer maintained.

History
The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:

the often subjective nature of the input information
the reliance on Bayes' conditioning as the basis for updating information
the distinction between causal and evidential modes of reasoning
In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems and Neapolitan's Probabilistic Reasoning in Expert Systems summarized their properties and established them as a field of study.

See also
Notes
References
Further reading
Conrady S, Jouffe L (2015-07-01). Bayesian Networks and BayesiaLab – A practical introduction for researchers. Franklin, Tennessee: Bayesian USA. ISBN 978-0-9965333-0-0.
Charniak E (Winter 1991). "Bayesian networks without tears" (PDF). AI Magazine.
Kruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P (2013). Computational Intelligence A Methodological Introduction. London: Springer-Verlag. ISBN 978-1-4471-5012-1.
Borgelt C, Steinbrecher M, Kruse R (2009). Graphical Models – Representations for Learning, Reasoning and Data Mining (Second ed.). Chichester: Wiley. ISBN 978-0-470-74956-2.

External links
An Introduction to Bayesian Networks and their Contemporary Applications
On-line Tutorial on Bayesian nets and probability
Web-App to create Bayesian nets and run it with a Monte Carlo method
Continuous Time Bayesian Networks
Bayesian Networks: Explanation and Analogy
A live tutorial on learning Bayesian networks
A hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples.
Hierarchical Naive Bayes Model for handling sample uncertainty Archived 2007-09-28 at the Wayback Machine, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.
Bayesian optimization is a sequential design strategy for global optimization of black-box functions, that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions. With the rise of artificial intelligence innovation in the 21st century, Bayesian optimizations have found prominent use in machine learning problems, for optimizing hyperparameter values.

History
The term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.

Strategy
Bayesian optimization is typically used on problems of the form 
  
    
      
        
          max
          
            x
            ∈
            A
          
        
        f
        (
        x
        )
      
    
    {\textstyle \max _{x\in A}f(x)}
  
, where 
  
    
      
        A
      
    
    {\textstyle A}
  
 is a set of points, 
  
    
      
        x
      
    
    {\textstyle x}
  
, which rely upon less (or equal to) than 20 dimensions (
  
    
      
        
          
            R
          
          
            d
          
        
        ,
        d
        ≤
        20
      
    
    {\textstyle \mathbb {R} ^{d},d\leq 20}
  
), and whose membership can easily be evaluated. Bayesian optimization is particularly advantageous for problems where 
  
    
      
        f
        (
        x
        )
      
    
    {\textstyle f(x)}
  
 is difficult to evaluate due to its computational cost. The objective function, 
  
    
      
        f
      
    
    {\textstyle f}
  
, is continuous and takes the form of some unknown structure, referred to as a "black box". Upon its evaluation, only 
  
    
      
        f
        (
        x
        )
      
    
    {\textstyle f(x)}
  
 is observed and its derivatives are not evaluated.
Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.
There are several methods used to define the prior/posterior distribution over the objective function. The most common two methods use Gaussian processes in a method called kriging. Another less expensive method uses the Parzen-Tree Estimator to construct two distributions for 'high' and 'low' points, and then finds the location that maximizes the expected improvement.
Standard Bayesian optimization relies upon each 
  
    
      
        x
        ∈
        A
      
    
    {\displaystyle x\in A}
  
 being easy to evaluate, and problems that deviate from this assumption are known as exotic Bayesian optimization problems. Optimization problems can become exotic if it is known that there is noise, the evaluations are being done in parallel, the quality of evaluations relies upon a tradeoff between difficulty and accuracy, the presence of random environmental conditions, or if the evaluation involves derivatives.

Acquisition functions
Examples of acquisition functions include 

probability of improvement
expected improvement
Bayesian expected losses
upper confidence bounds (UCB) or lower confidence bounds
Thompson sampling
and hybrids of these. They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are expensive to evaluate.

Solution methods
The maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer. Acquisition functions are 
maximized using a numerical optimization technique, such as Newton's Method or quasi-Newton methods like the Broyden–Fletcher–Goldfarb–Shanno algorithm.

Applications
The approach has been applied to solve a wide range of problems, including learning to rank, computer graphics and visual design, robotics, sensor networks, automatic algorithm configuration, automatic machine learning toolboxes, reinforcement learning, planning, visual attention, architecture configuration in deep learning, static program analysis, experimental particle physics, quality-diversity optimization, chemistry, material design, and drug development.
Bayesian Optimization has been applied in the field of facial recognition. The performance of the Histogram of Oriented Gradients (HOG) algorithm, a popular feature extraction method, heavily relies on its parameter settings. Optimizing these parameters can be challenging but crucial for achieving high accuracy. A novel approach to optimize the HOG algorithm parameters and image size for facial recognition using a Tree-structured Parzen Estimator (TPE) based Bayesian optimization technique has been proposed. This optimized approach has the potential to be adapted for other computer vision applications and contributes to the ongoing development of hand-crafted parameter-based feature extraction algorithms in computer vision.

See also
Multi-armed bandit
Kriging
Thompson sampling
Global optimization
Bayesian experimental design
Probabilistic numerics
Pareto optimum
Active learning (machine learning)
Multi-objective optimization


== References ==
Behaviorism is a systematic approach to understand the behavior of humans and other animals. It assumes that behavior is either a reflex elicited by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events. The cognitive revolution of the late 20th century largely replaced behaviorism as an explanatory theory with cognitive psychology, which unlike behaviorism views internal mental states as explanations for observable behavior.
Behaviorism emerged in the early 1900s as a reaction to depth psychology and other traditional forms of psychology, which often had difficulty making predictions that could be tested experimentally. It was derived from earlier research in the late nineteenth century, such as when Edward Thorndike pioneered the law of effect, a procedure that involved the use of consequences to strengthen or weaken behavior.
With a 1924 publication, John B. Watson devised methodological behaviorism, which rejected introspective methods and sought to understand behavior by only measuring observable behaviors and events. It was not until 1945 that B. F. Skinner proposed that covert behavior—including cognition and emotions—are subject to the same controlling variables as observable behavior, which became the basis for his philosophy called radical behaviorism. While Watson and Ivan Pavlov investigated how (conditioned) neutral stimuli elicit reflexes in respondent conditioning, Skinner assessed the reinforcement histories of the discriminative (antecedent) stimuli that emits behavior; the process became known as operant conditioning.
The application of radical behaviorism—known as applied behavior analysis—is used in a variety of contexts, including, for example, applied animal behavior and organizational behavior management to treatment of mental disorders, such as autism and substance abuse. In addition, while behaviorism and cognitive schools of psychological thought do not agree theoretically, they have complemented each other in the cognitive-behavioral therapies, which have demonstrated utility in treating certain pathologies, including simple phobias, PTSD, and mood disorders.

Branches of behaviorism
The titles given to the various branches of behaviorism include:

Behavioral genetics: Proposed in 1869 by Francis Galton, a relative of Charles Darwin. Galton believed that inherited factors had a significant impact on individuals' behaviors, however did not believe nurturing was not important. Which was later discredited due to association with the eugenics movement - researchers did not want to associate with Nazi politics whether direct or indirect. doi:10.3724/sp.j.1041.2008.01073
Interbehaviorism: Proposed by Jacob Robert Kantor before B. F. Skinner's writings.
Methodological behaviorism: John B. Watson's behaviorism states that only public events (motor behaviors of an individual) can be objectively observed. Although it was still acknowledged that thoughts and feelings exist, they were not considered part of the science of behavior. It also laid the theoretical foundation for the early approach behavior modification in the 1970s and 1980s. Often compared to the views of B.F Skinner (radical behaviorism). Methodological behaviorism "representing the logical positivist-derived philosophy of science" which is common in science today, radical focuses on the "pragmatist perspective." JSTOR 27759016
Psychological behaviorism: As proposed by Arthur W. Staats, unlike the previous behaviorisms of Skinner, Hull, and Tolman, was based upon a program of human research involving various types of human behavior. Psychological behaviorism introduces new principles of human learning. Humans learn not only by animal learning principles but also by special human learning principles. Those principles involve humans' uniquely huge learning ability. Humans learn repertoires that enable them to learn other things. Human learning is thus cumulative. No other animal demonstrates that ability, making the human species unique.
Radical behaviorism: Skinner's philosophy is an extension of Watson's form of behaviorism by theorizing that processes within the organism—particularly, private events, such as thoughts and feelings—are also part of the science of behavior, and suggests that environmental variables control these internal events just as they control observable behaviors. Behavioral events may be observable but not all are, some are considered "private": they are accessible and noticed by only the person who is behaving. B.F. Skinner described behavior as the name for the part of the functioning of the organism that consists of its interacting or having commerce with its surrounding environment. In simple terms, how an individual interacts with its surrounding environment.[RB] Although private events cannot be directly seen by others, they are later determined through the species' overt behavior. Radical behaviorism forms the core philosophy behind behavior analysis. Willard Van Orman Quine used many of radical behaviorism's ideas in his study of knowledge and language.
Teleological behaviorism: Proposed by Howard Rachlin, post-Skinnerian, purposive, close to microeconomics. Focuses on objective observation as opposed to cognitive processes.
Theoretical behaviorism: Proposed by J. E. R. Staddon,  adds a concept of internal state to allow for the effects of context. According to theoretical behaviorism, a state is a set of equivalent histories, i.e., past histories in which members of the same stimulus class produce members of the same response class (i.e., B. F. Skinner's concept of the operant). Conditioned stimuli are thus seen to control neither stimulus nor response but state. Theoretical behaviorism is a logical extension of Skinner's class-based (generic) definition of the operant.
Two subtypes of theoretical behaviorism are:

Hullian and post-Hullian: theoretical, group data, not dynamic, physiological
Purposive: Tolman's behavioristic anticipation of cognitive psychology

Modern-day theory: radical behaviorism
B. F. Skinner proposed radical behaviorism as the conceptual underpinning of the experimental analysis of behavior. This viewpoint differs from other approaches to behavioral research in various ways, but, most notably here, it contrasts with methodological behaviorism in accepting feelings, states of mind and introspection as behaviors also subject to scientific investigation. Like methodological behaviorism, it rejects the reflex as a model of all behavior, and it defends the science of behavior as complementary to but independent of physiology. Radical behaviorism overlaps considerably with other western philosophical positions, such as American pragmatism.
Although John B. Watson mainly emphasized his position of methodological behaviorism throughout his career, Watson and Rosalie Rayner conducted the infamous Little Albert experiment (1920), a study in which Ivan Pavlov's theory to respondent conditioning was first applied to eliciting a fearful reflex of crying in a human infant, and this became the launching point for understanding covert behavior (or private events) in radical behaviorism. However, Skinner felt that aversive stimuli should only be experimented on with animals and spoke out against Watson for testing something so controversial on a human.
In 1959, Skinner observed the emotions of two pigeons by noting that they appeared angry because their feathers ruffled. The pigeons were placed together in an operant chamber, where they were aggressive as a consequence of previous reinforcement in the environment. Through stimulus control and subsequent discrimination training, whenever Skinner turned off the green light, the pigeons came to notice that the food reinforcer is discontinued following each peck and responded without aggression. Skinner concluded that humans also learn aggression and possess such emotions (as well as other private events) no differently than do nonhuman animals.

Experimental and conceptual innovations
As experimental behavioural psychology is related to behavioral neuroscience, we can date the first researches in the area were done in the beginning of 19th century.
Later, this essentially philosophical position gained strength from the success of Skinner's early experimental work with rats and pigeons, summarized in his books The Behavior of Organisms and Schedules of Reinforcement. Of particular importance was his concept of the operant response, of which the canonical example was the rat's lever-press. In contrast with the idea of a physiological or reflex response, an operant is a class of structurally distinct but functionally equivalent responses. For example, while a rat might press a lever with its left paw or its right paw or its tail, all of these responses operate on the world in the same way and have a common consequence. Operants are often thought of as species of responses, where the individuals differ but the class coheres in its function-shared consequences with operants and reproductive success with species. This is a clear distinction between Skinner's theory and S–R theory.
Skinner's empirical work expanded on earlier research on trial-and-error learning by researchers such as Thorndike and Guthrie with both conceptual reformulations—Thorndike's notion of a stimulus-response "association" or "connection" was abandoned; and methodological ones—the use of the "free operant", so-called because the animal was now permitted to respond at its own rate rather than in a series of trials determined by the experimenter procedures. With this method, Skinner carried out substantial experimental work on the effects of different schedules and rates of reinforcement on the rates of operant responses made by rats and pigeons. He achieved remarkable success in training animals to perform unexpected responses, to emit large numbers of responses, and to demonstrate many empirical regularities at the purely behavioral level. This lent some credibility to his conceptual analysis. It is largely his conceptual analysis that made his work much more rigorous than his peers, a point which can be seen clearly in his seminal work Are Theories of Learning Necessary? in which he criticizes what he viewed to be theoretical weaknesses then common in the study of psychology. An important descendant of the experimental analysis of behavior is the Society for Quantitative Analysis of Behavior.

Relation to language
As Skinner turned from experimental work to concentrate on the philosophical underpinnings of a science of behavior, his attention turned to human language with his 1957 book Verbal Behavior and other language-related publications; Verbal Behavior laid out a vocabulary and theory for functional analysis of verbal behavior, and was strongly criticized in a review by Noam Chomsky.
Skinner did not respond in detail but claimed that Chomsky failed to understand his ideas, and the disagreements between the two and the theories involved have been further discussed. Innateness theory, which has been heavily critiqued, is opposed to behaviorist theory which claims that language is a set of habits that can be acquired by means of conditioning. According to some, the behaviorist account is a process which would be too slow to explain a phenomenon as complicated as language learning. What was important for a behaviorist's analysis of human behavior was not language acquisition so much as the interaction between language and overt behavior. In an essay republished in his 1969 book Contingencies of Reinforcement, Skinner took the view that humans could construct linguistic stimuli that would then acquire control over their behavior in the same way that external stimuli could. The possibility of such "instructional control" over behavior meant that contingencies of reinforcement would not always produce the same effects on human behavior as they reliably do in other animals. The focus of a radical behaviorist analysis of human behavior therefore shifted to an attempt to understand the interaction between instructional control and contingency control, and also to understand the behavioral processes that determine what instructions are constructed and what control they acquire over behavior. Recently, a new line of behavioral research on language was started under the name of relational frame theory.

Education
B.F. Skinner's book Verbal Behavior (1957) does not quite emphasize on language development, but to understand human behavior. Additionally, his work serves in understanding social interactions in the child's early developmental stages focusing on the topic of caregiver-infant interaction. Skinner's functional analysis of verbal behavior terminology and theories is commonly used to understand the relationship between language development but was primarily designed to describe behaviors of interest and explain the cause of those behaviors. Noam Chomsky, an American linguistic professor, has criticized and questioned Skinner's theories about the possible suggestion of parental tutoring in language development. However, there is a lack of supporting evidence where Skinner makes the statement.
Understanding language is a complex topic, but can be understood through the use of two theories: Innateness and acquisition. Both theories offer a different perspective whether language is inherently "acquired" or "learned."

Operant conditioning
Operant conditioning was developed by B.F. Skinner in 1938 and is form of learning in which the frequency of a behavior is controlled by consequences to change behavior. In other words, behavior is controlled by historical consequential contingencies, particularly reinforcement—a stimulus that increases the probability of performing behaviors, and punishment—a stimulus that decreases such probability. The core tools of consequences are either positive (presenting stimuli following a response), or negative (withdrawn stimuli following a response).
The following descriptions explains the concepts of four common types of consequences in operant conditioning:

Positive reinforcement: Providing a stimulus that an individual enjoys, seeks, or craves, in order to reinforce desired behaviors. For example, when a person is teaching a dog to sit, they pair the command "sit" with a treat. The treat is the positive reinforcement to the behavior of sitting. The key to making positive reinforcement effect is to reward the behavior immediately.
Negative reinforcement: Increases the frequency of a behavior, but the behavior results from removing unpleasant or unwanted stimulus. For example, a child hates being nagged (negative) to clean his room (behavior) which increases the frequency of the child cleaning his room to prevent his mother from nagging. Another example would be putting on sunscreen (behavior) before going outside to prevent sunburn (negative).
Positive punishment: Providing a stimulus that an individual does not desire to decrease undesired behaviors. For example, if a child engages in an undesired behavior, then parents may spank (stimulus) the child to correct their behavior.
Negative punishment: Removing a stimulus that an individual desires in order to decrease undesired behaviors. An example of this would be grounding a child for failing a test. Grounding in this example is taking away the child's ability to play video games. As long as it is clear that the ability to play video games was taken away because they failed a test, this is negative punishment. The key here is the connection to the behavior and the result of the behavior.
A classical experiment in operant conditioning, for example, is the Skinner Box, "puzzle box" or operant conditioning chamber to test the effects of operant conditioning principles on rats, cats and other species. From this experiment, he discovered that the rats learned very effectively if they were rewarded frequently with food. Skinner also found that he could shape (create new behavior) the rats' behavior through the use of rewards, which could, in turn, be applied to human learning as well.
Skinner's model was based on the premise that reinforcement is used for the desired actions or responses while punishment was used to stop the responses of the undesired actions that are not. This theory proved that humans or animals will repeat any action that leads to a positive outcome, and avoid any action that leads to a negative outcome. The experiment with the pigeons showed that a positive outcome leads to learned behavior since the pigeon learned to peck the disc in return for the reward of food.
These historical consequential contingencies subsequently lead to (antecedent) stimulus control, but in contrast to respondent conditioning where antecedent stimuli elicit reflexive behavior, operant behavior is only emitted and therefore does not force its occurrence. It includes the following controlling stimuli:

Discriminative stimulus (Sd): An antecedent stimulus that increases the chance of the organism engaging in a behavior. One example of this occurred in Skinner's laboratory. Whenever the green light (Sd) appeared, it signaled the pigeon to perform the behavior of pecking because it learned in the past that each time it pecked, food was presented (the positive reinforcing stimulus).
Stimulus delta (S-delta): An antecedent stimulus that signals the organism not to perform a behavior since it was extinguished or punished in the past. One notable instance of this occurs when a person stops their car immediately after the traffic light turns red (S-delta). However, the person could decide to drive through the red light, but subsequently receive a speeding ticket (the positive punishing stimulus), so this behavior will potentially not reoccur following the presence of the S-delta.

Respondent conditioning
Although operant conditioning plays the largest role in discussions of behavioral mechanisms, respondent conditioning (also called Pavlovian or classical conditioning) is also an important behavior-analytic process that needs not refer to mental or other internal processes. Pavlov's experiments with dogs provide the most familiar example of the classical conditioning procedure. In the beginning, the dog was provided meat (unconditioned stimulus, UCS, naturally elicit a response that is not controlled) to eat, resulting in increased salivation (unconditioned response, UCR, which means that a response is naturally caused by UCS). Afterward, a bell ring was presented together with food to the dog. Although bell ring was a neutral stimulus (NS, meaning that the stimulus did not have any effect), dog would start to salivate when only hearing a bell ring after a number of pairings. Eventually, the neutral stimulus (bell ring) became conditioned. Therefore, salivation was elicited as a conditioned response (the response same as the unconditioned response), pairing up with meat—the conditioned stimulus)   Although Pavlov proposed some tentative physiological processes that might be involved in classical conditioning, these have not been confirmed. The idea of classical conditioning helped behaviorist John Watson discover the key mechanism behind how humans acquire the behaviors that they do, which was to find a natural reflex that produces the response being considered.
Watson's "Behaviourist Manifesto" has three aspects that deserve special recognition: one is that psychology should be purely objective, with any interpretation of conscious experience being removed, thus leading to psychology as the "science of behaviour"; the second one is that the goals of psychology should be to predict and control behaviour (as opposed to describe and explain conscious mental states); the third one is that there is no notable distinction between human and non-human behaviour. Following Darwin's theory of evolution, this would simply mean that human behaviour is just a more complex version in respect to behaviour displayed by other species.

In philosophy
Behaviorism is a psychological movement that can be contrasted with philosophy of mind. The basic premise of behaviorism is that the study of behavior should be a natural science, such as chemistry or physics.  Initially behaviorism rejected any reference to hypothetical inner states of organisms as causes for their behavior, but B.F. Skinner's radical behaviorism reintroduced reference to inner states and also advocated for the study of thoughts and feelings as behaviors subject to the same mechanisms as external behavior.  Behaviorism takes a functional view of behavior. According to Edmund Fantino and colleagues: "Behavior analysis has much to offer the study of phenomena normally dominated by cognitive and social psychologists. We hope that successful application of behavioral theory and methodology will not only shed light on central problems in judgment and choice but will also generate greater appreciation of the behavioral approach."
Behaviorist sentiments are not uncommon within philosophy of language and analytic philosophy. It is sometimes argued that Ludwig Wittgenstein defended a logical behaviorist position (e.g., the beetle in a box argument). In logical positivism (as held, e.g., by Rudolf Carnap and Carl Hempel),  the meaning of psychological statements are their verification conditions, which consist of performed overt behavior. W. V. O. Quine made use of a type of behaviorism, influenced by some of Skinner's ideas, in his own work on language. Quine's work in semantics differed substantially from the empiricist semantics of Carnap which he attempted to create an alternative to, couching his semantic theory in references to physical objects rather than sensations. Gilbert Ryle defended a distinct strain of philosophical behaviorism, sketched in his book The Concept of Mind.  Ryle's central claim was that instances of dualism frequently represented "category mistakes", and hence that they were really misunderstandings of the use of ordinary language. Daniel Dennett likewise acknowledges himself to be a type of behaviorist, though he offers extensive criticism of radical behaviorism and refutes Skinner's rejection of the value of intentional idioms and the possibility of free will.

This is Dennett's main point in "Skinner Skinned." Dennett argues that there is a crucial difference between explaining and explaining away... If our explanation of apparently rational behavior turns out to be extremely simple, we may want to say that the behavior was not really rational after all. But if the explanation is very complex and intricate, we may want to say not that the behavior is not rational, but that we now have a better understanding of what rationality consists in. (Compare: if we find out how a computer program solves problems in linear algebra, we don't say it's not really solving them, we just say we know how it does it. On the other hand, in cases like Weizenbaum's ELIZA program, the explanation of how the computer carries on a conversation is so simple that the right thing to say seems to be that the machine isn't really carrying on a conversation, it's just a trick.)

Law of effect and trace conditioning
Law of effect: Although Edward Thorndike's methodology mainly dealt with reinforcing observable behavior, it viewed cognitive antecedents as the causes of behavior, and was theoretically much more similar to the cognitive-behavior therapies than classical (methodological) or modern-day (radical) behaviorism. Nevertheless, Skinner's operant conditioning was heavily influenced by the Law of Effect's principle of reinforcement.
Trace conditioning: Akin to B.F. Skinner's radical behaviorism, it is a respondent conditioning technique based on Ivan Pavlov's concept of a "memory trace" in which the observer recalls the conditioned stimulus (CS), with the memory or recall being the unconditioned response (UR). There is also a time delay between the CS and unconditioned stimulus (US), causing the conditioned response (CR)—particularly the reflex—to be faded over time. According to Marchand, the hippocampus is a part of the cognitive processes during trace conditioning and other forms of classical conditioning in two ways: needing to overcome stimuli or due to mre activity from complex challenges. However, results may vary due to the nature of the task and the design of the experiment .

Molecular versus molar behaviorism
Skinner's view of behavior is most often characterized as a "molecular" view of behavior; that is, behavior can be decomposed into atomistic parts or molecules. This view is inconsistent with Skinner's complete description of behavior as delineated in other works, including his 1981 article "Selection by Consequences". Skinner proposed that a complete account of behavior requires understanding of selection history at three levels: biology (the natural selection or phylogeny of the animal); behavior (the reinforcement history or ontogeny of the behavioral repertoire of the animal); and for some species, culture (the cultural practices of the social group to which the animal belongs). This whole organism then interacts with its environment. Molecular behaviorists use notions from melioration theory, negative power function discounting or additive versions of negative power function discounting. According to Moore, the perseverance in a molecular examination of behavior may be sign of a desire for an in-depth understanding, maybe to identify any underlying mechanism or components that contribute to comples actions. This strategy might involve elements, procedure, or variables that contribute to behaviorism.
Molar behaviorists, such as Howard Rachlin, Richard Herrnstein, and William Baum, argue that behavior cannot be understood by focusing on events in the moment. That is, they argue that behavior is best understood as the ultimate product of an organism's history and that molecular behaviorists are committing a fallacy by inventing fictitious proximal causes for behavior. Molar behaviorists argue that standard molecular constructs, such as "associative strength", are better replaced by molar variables such as rate of reinforcement. Thus, a molar behaviorist would describe "loving someone" as a pattern of loving behavior over time; there is no isolated, proximal cause of loving behavior, only a history of behaviors (of which the current behavior might be an example) that can be summarized as "love".

Theoretical behaviorism
Skinner's radical behaviorism has been highly successful experimentally, revealing new phenomena with new methods, but Skinner's dismissal of theory limited its development. Theoretical behaviorism recognized that a historical system, an organism, has a state as well as sensitivity to stimuli and the ability to emit responses.  Indeed, Skinner himself acknowledged the possibility of what he called "latent" responses in humans, even though he neglected to extend this idea to rats and pigeons.  Latent responses constitute a repertoire, from which operant reinforcement can select.  Theoretical behaviorism links between the brain and the behavior that provides a real understanding of the behavior, rather than a mental presumption of how brain-behavior relates.  The theoretical concept of behaviorism are blended with knowledge of mental structure such as memory and expectancies associated with inflexable behaviorist stances that have traditionally forbidden the examination of the mental state. Because of its flexibility, theoretical behaviorism permits the cognitive process to have an impact on behavior.

Behavior analysis and culture
From its inception, behavior analysis has centered its examination on cultural occurrences (Skinner, 1953, 1961, 1971, 1974 ). Nevertheless, the methods used to tackle these occurrences have evolved. Initially, culture was perceived as a factor influencing behavior, later becoming a subject of study in itself. This shift prompted research into group practices and the potential for significant behavioral transformations on a larger scale. Following Glenn's (1986) influential work, "Metacontingencies in Walden Two,"   numerous research endeavors exploring behavior analysis in cultural contexts have centered around the concept of the metacontingency. Glenn (2003) posited that understanding the origins and development of cultures necessitates delving beyond evolutionary and behavioral principles governing species characteristics and individual learned behaviors requires analysis at a major level.

Behavior informatics and behavior computing
With the fast growth of big behavioral data and applications, behavior analysis is ubiquitous. Understanding behavior from the informatics and computing perspective becomes increasingly critical for in-depth understanding of what, why and how behaviors are formed, interact, evolve, change and affect business and decision. Behavior informatics and behavior computing deeply explore behavior intelligence and behavior insights from the informatics and computing perspectives.
Pavel et al. (2015) found that in the realm of healthcare and health psychology, substantial evidence supports the notion that personalized health interventions yield greater effectiveness compared to standardized approaches. Additionally, researchers found that recent progress in sensor and communication technology, coupled with data analysis and computational modeling, holds significant potential in revolutionizing interventions aimed at changing health behavior. Simultaneous advancements in sensor and communication technology, alongside the field of data science, have now made it possible to comprehensively measure behaviors occurring in real-life settings. These two elements, when combined with advancements in computational modeling, have laid the groundwork for the emerging discipline known as behavioral informatics. Behavioral informatics represents a scientific and engineering domain encompassing behavior tracking, evaluation, computational modeling, deduction, and intervention.

Criticisms and limitations
In the second half of the 20th century, behaviorism was largely eclipsed as a result of the cognitive revolution. This shift was due to radical behaviorism being highly criticized for not examining mental processes, and this led to the development of the cognitive therapy movement.
In the mid-20th century, three main influences arose that would inspire and shape cognitive psychology as a formal school of thought:

Noam Chomsky's 1959 critique of behaviorism, and empiricism more generally, initiated what would come to be known as the "cognitive revolution".
Developments in computer science would lead to parallels being drawn between human thought and the computational functionality of computers, opening entirely new areas of psychological thought. Allen Newell and Herbert Simon spent years developing the concept of artificial intelligence (AI) and later worked with cognitive psychologists regarding the implications of AI. The effective result was more of a framework conceptualization of mental functions with their counterparts in computers (memory, storage, retrieval, etc.).
Formal recognition of the field involved the establishment of research institutions such as George Mandler's Center for Human Information Processing in 1964. Mandler described the origins of cognitive psychology in a 2002 article in the Journal of the History of the Behavioral Sciences
In more recent years, several scholars have expressed reservations about the pragmatic tendencies of behaviorism.

Burgos (2003) highlights the potential peril of pragmatism, noting that within William James pragmatism—widely discussed in philosophy and science, including behaviorism and behavior analysis—there exists a tolerance for anything deemed useful, even if nonsensical. Additionally, Burgos (2007) contends that pragmatism engenders a relativism that contradicts the emphasis on science as the paramount path to knowledge.
Staddon (2018, as cited in Araiba, 2019) further argues that the proliferation of diversification in social science poses disadvantages by hindering healthy and open scientific communication and critique among specialized areas.
Rider (1991) shares a similar concern, highlighting reduced communication between the experimental analysis of behavior and applied behavior analysis. Contrarily, diversification is portrayed as an innate and uncontrollable consequence of the environment, a natural facet contributing to species' survival. It is viewed as an integral aspect of the evolution of behaviorism.
In the early years of cognitive psychology, behaviorist critics held that the empiricism it pursued was incompatible with the concept of internal mental states. Cognitive neuroscience, however, continues to gather evidence of direct correlations between physiological brain activity and putative mental states, endorsing the basis for cognitive psychology.

Limitations
Staddon (1993) found that Skinner's theory presents two significant deficiencies: Firstly, he downplayed the significance of processes responsible for generating novel behaviors, which it is term as "behavioral variation." Skinner primarily emphasized reinforcement as the sole determinant for selecting responses, overlooking these critical processes involved in creating new behaviors. Secondly, both Skinner and many other behaviorists of that era endorsed contiguity as a sufficient process for response selection. However, Rescorla and Wagner (1972) later demonstrated, particularly in classical conditioning, that competition is an essential complement to contiguity. They showed that in operant conditioning, both contiguity and competition are imperative for discerning cause-and-effect relationships.
The influential Rescorla-Wagner model  highlights the significance of competition for limited "associative value," essential for assessing predictability. A similar formal argument was presented by Ying Zhang and John Staddon (1991, in press) concerning operant conditioning: the combination of contiguity and competition among action tendencies suffices as an assignment-of-credit mechanism capable of detecting genuine instrumental contingency between a response and its reinforcer. This mechanism delineates the limitations of Skinner's idea of adventitious reinforcement, revealing its efficacy only under stringent conditions – when the reinforcement's strengthening effect is nearly constant across instances and with very short intervals between reinforcers. However, these conditions rarely hold in reality: behavior following reinforcement tends to exhibit high variability, and superstitious behavior diminishes with extremely brief intervals between reinforcements.

Behavior therapy
Behavior therapy is a term referring to different types of therapies that treat mental health disorders. It identifies and helps change people's unhealthy behaviors or destructive behaviors through learning theory and conditioning. Ivan Pavlov's classical conditioning, as well as counterconditioning are the basis for much of clinical behavior therapy, but also includes other techniques, including operant conditioning—or contingency management, and modeling (sometimes called observational learning). A frequently noted behavior therapy is systematic desensitization (graduated exposure therapy), which was first demonstrated by Joseph Wolpe and Arnold Lazarus.

Behavior analysis
Applied behavior analysis (ABA)—also called behavioral engineering—is a scientific discipline that applies the principles of behavior analysis to change behavior. ABA derived from much earlier research in the Journal of the Experimental Analysis of Behavior, which was founded by B.F. Skinner and his colleagues at Harvard University. Nearly a decade after the study "The psychiatric nurse as a behavioral engineer" (1959) was published in that journal, which demonstrated how effective the token economy was in reinforcing more adaptive behavior for hospitalized patients with schizophrenia and intellectual disability, it led to researchers at the University of Kansas to start the Journal of Applied Behavior Analysis in 1968.
Although ABA and behavior modification are similar behavior-change technologies in that the learning environment is modified through respondent and operant conditioning, behavior modification did not initially address the causes of the behavior (particularly, the environmental stimuli that occurred in the past), or investigate solutions that would otherwise prevent the behavior from reoccurring. As the evolution of ABA began to unfold in the mid-1980s, functional behavior assessments (FBAs) were developed to clarify the function of that behavior, so that it is accurately determined which differential reinforcement contingencies will be most effective and less likely for aversive punishments to be administered. In addition, methodological behaviorism was the theory underpinning behavior modification since private events were not conceptualized during the 1970s and early 1980s, which contrasted from the radical behaviorism of behavior analysis. ABA—the term that replaced behavior modification—has emerged into a thriving field.
The independent development of behaviour analysis outside the United States also continues to develop. In the US, the American Psychological Association (APA) features a subdivision for Behavior Analysis, titled APA Division 25: Behavior Analysis, which has been in existence since 1964, and the interests among behavior analysts today are wide-ranging, as indicated in a review of the 30 Special Interest Groups (SIGs) within the Association for Behavior Analysis International (ABAI). Such interests include everything from animal behavior and environmental conservation to classroom instruction (such as direct instruction and precision teaching), verbal behavior, developmental disabilities and autism, clinical psychology (i.e., forensic behavior analysis), behavioral medicine (i.e., behavioral gerontology, AIDS prevention, and fitness training), and consumer behavior analysis.
The field of applied animal behavior—a sub-discipline of ABA that involves training animals—is regulated by the Animal Behavior Society, and those who practice this technique are called applied animal behaviorists. Research on applied animal behavior has been frequently conducted in the Applied Animal Behaviour Science journal since its founding in 1974.
ABA has also been particularly well-established in the area of developmental disabilities since the 1960s, but it was not until the late 1980s that individuals diagnosed with autism spectrum disorders were beginning to grow so rapidly and groundbreaking research was being published that parent advocacy groups started demanding for services throughout the 1990s, which encouraged the formation of the Behavior Analyst Certification Board, a credentialing program that certifies professionally trained behavior analysts on the national level to deliver such services. Nevertheless, the certification is applicable to all human services related to the rather broad field of behavior analysis (other than the treatment for autism), and the ABAI currently has 14 accredited MA and Ph.D. programs for comprehensive study in that field.
Early behavioral interventions (EBIs) based on ABA are empirically validated for teaching children with autism and have been proven as such for over the past five decades. Since the late 1990s and throughout the twenty-first century, early ABA interventions have also been identified as the treatment of choice by the US Surgeon General, American Academy of Pediatrics, and US National Research Council.
Discrete trial training—also called early intensive behavioral intervention—is the traditional EBI technique implemented for thirty to forty hours per week that instructs a child to sit in a chair, imitate fine and gross motor behaviors, as well as learn eye contact and speech, which are taught through shaping, modeling, and prompting, with such prompting being phased out as the child begins mastering each skill. When the child becomes more verbal from discrete trials, the table-based instructions are later discontinued, and another EBI procedure known as incidental teaching is introduced in the natural environment by having the child ask for desired items kept out of their direct access, as well as allowing the child to choose the play activities that will motivate them to engage with their facilitators before teaching the child how to interact with other children their own age.
A related term for incidental teaching, called pivotal response treatment (PRT), refers to EBI procedures that exclusively entail twenty-five hours per week of naturalistic teaching (without initially using discrete trials). Current research is showing that there is a wide array of learning styles and that is the children with receptive language delays who initially require discrete trials to acquire speech.
Organizational behavior management, which applies contingency management procedures to model and reinforce appropriate work behavior for employees in organizations, has developed a particularly strong following within ABA, as evidenced by the formation of the OBM Network and Journal of Organizational Behavior Management, which was rated the third-highest impact journal in applied psychology by ISI JOBM rating.
Modern-day clinical behavior analysis has also witnessed a massive resurgence in research, with the development of relational frame theory (RFT), which is described as an extension of verbal behavior and a "post-Skinnerian account of language and cognition." RFT also forms the empirical basis for acceptance and commitment therapy, a therapeutic approach to counseling often used to manage such conditions as anxiety and obesity that consists of acceptance and commitment, value-based living, cognitive defusion, counterconditioning (mindfulness), and contingency management (positive reinforcement). Another evidence-based counseling technique derived from RFT is the functional analytic psychotherapy known as behavioral activation that relies on the ACL model—awareness, courage, and love—to reinforce more positive moods for those struggling with depression.
Incentive-based contingency management (CM) is the standard of care for adults with substance-use disorders; it has also been shown to be highly effective for other addictions (i.e., obesity and gambling). Although it does not directly address the underlying causes of behavior, incentive-based CM is highly behavior analytic as it targets the function of the client's motivational behavior by relying on a preference assessment, which is an assessment procedure that allows the individual to select the preferred reinforcer (in this case, the monetary value of the voucher, or the use of other incentives, such as prizes). Another evidence-based CM intervention for substance abuse is community reinforcement approach and family training that uses FBAs and counterconditioning techniques—such as behavioral skills training and relapse prevention—to model and reinforce healthier lifestyle choices which promote self-management of abstinence from drugs, alcohol, or cigarette smoking during high-risk exposure when engaging with family members, friends, and co-workers.
While schoolwide positive behavior support consists of conducting assessments and a task analysis plan to differentially reinforce curricular supports that replace students' disruptive behavior in the classroom, pediatric feeding therapy incorporates a liquid chaser and chin feeder to shape proper eating behavior for children with feeding disorders. Habit reversal training, an approach firmly grounded in counterconditioning which uses contingency management procedures to reinforce alternative behavior, is currently the only empirically validated approach for managing tic disorders.
Some studies on exposure (desensitization) therapies—which refer to an array of interventions based on the respondent conditioning procedure known as habituation and typically infuses counterconditioning procedures, such as meditation and breathing exercises—have recently been published in behavior analytic journals since the 1990s, as most other research is conducted from a cognitive-behavior therapy framework. When based on a behavior analytic research standpoint, FBAs are implemented to precisely outline how to employ the flooding form of desensitization (also called direct exposure therapy) for those who are unsuccessful in overcoming their specific phobia through systematic desensitization (also known as graduated exposure therapy). These studies also reveal that systematic desensitization is more effective for children if used in conjunction with shaping, which is further termed contact desensitization, but this comparison has yet to be substantiated with adults.
Other widely published behavior analytic journals include Behavior Modification, The Behavior Analyst, Journal of Positive Behavior Interventions, Journal of Contextual Behavioral Science, The Analysis of Verbal Behavior, Behavior and Philosophy, Behavior and Social Issues, and The Psychological Record.

Cognitive-behavior therapy
Cognitive-behavior therapy (CBT) is a behavior therapy discipline that often overlaps considerably with the clinical behavior analysis subfield of ABA, but differs in that it initially incorporates cognitive restructuring and emotional regulation to alter a person's cognition and emotions. Various forms of CBT have been used to treat physically experienced symptoms that disrupt individuals' livelihood, which often stem from complex mental health disorders. Complications of many trauma-induced disorders result in lack of sleep and nightmares, with cognitive behavior therapy functioning as an intervention found to reduce the average number of PTSD patients suffering from related sleep disturbance.
A popularly noted counseling intervention known as dialectical behavior therapy (DBT) includes the use of a chain analysis, as well as cognitive restructuring, emotional regulation, distress tolerance, counterconditioning (mindfulness), and contingency management (positive reinforcement). DBT is quite similar to acceptance and commitment therapy, but contrasts in that it derives from a CBT framework. Although DBT is most widely researched for and empirically validated to reduce the risk of suicide in psychiatric patients with borderline personality disorder, it can often be applied effectively to other mental health conditions, such as substance abuse, as well as mood and eating disorders. A study on BPD was conducted, confirming DBT as a constructive therapeutic option for emotionally unregulated patients. Before DBT, participants with borderline personality disorder were shown images of highly emotional people and neuron activity in the amygdala was recorded via fMRI; after 1 year of consistent dialectical behavior therapy, participants were re-tested, with fMRI capturing a decrease in amygdala hyperactivity (emotional activation) in response to the applied stimulus, exhibiting increases in emotional regulation capabilities.
Most research on exposure therapies (also called desensitization)—ranging from eye movement desensitization and reprocessing therapy to exposure and response prevention—are conducted through a CBT framework in non-behavior analytic journals, and these enhanced exposure therapies are well-established in the research literature for treating phobic, post-traumatic stress, and other anxiety disorders (such as obsessive-compulsive disorder, or OCD).
Cognitive-based behavioral activation (BA)—the psychotherapeutic approach used for depression—is shown to be highly effective and is widely used in clinical practice. Some large randomized control trials have indicated that cognitive-based BA is as beneficial as antidepressant medications but more efficacious than traditional cognitive therapy. Other commonly used clinical treatments derived from behavioral learning principles that are often implemented through a CBT model include community reinforcement approach and family training, and habit reversal training for substance abuse and tics, respectively.

Related therapies
List of notable behaviorists
See also
Reference in APA 7th edition format
Further reading
External links

Graham, George. "Behaviorism". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
"Behaviorism". Internet Encyclopedia of Philosophy.
In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:

The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).
The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.

Motivation
The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data.
It is an often made fallacy to assume that complex models must have high variance. High variance models are "complex" in some sense, but the reverse needs not be true. In addition, one has to be careful how to define complexity. In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from: The model 
  
    
      
        
          f
          
            a
            ,
            b
          
        
        (
        x
        )
        =
        a
        sin
        ⁡
        (
        b
        x
        )
      
    
    {\displaystyle f_{a,b}(x)=a\sin(bx)}
  
 has only two parameters (
  
    
      
        a
        ,
        b
      
    
    {\displaystyle a,b}
  
) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance.
An analogy can be made to the relationship between accuracy and precision. Accuracy is a description of bias and can intuitively be improved by selecting from only local information. Consequently, a sample will appear accurate (i.e. have low bias) under the aforementioned selection conditions, but may result in underfitting. In other words, test data may not agree as closely with training data, which would indicate imprecision and therefore inflated variance. A graphical example would be a straight line fit to data exhibiting quadratic behavior overall. Precision is a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample space is the ideal condition for any analysis. However, intrinsic constraints (whether physical, theoretical, computational, etc.) will always play a limiting role. The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting). This means that test data would also not agree as closely with the training data, but in this case the reason is inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage.

Bias–variance decomposition of mean squared error
Suppose that we have a training set consisting of a set of points 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  
 and real values 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 associated with each point 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
. We assume that the data is generated by a function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 such as 
  
    
      
        y
        =
        f
        (
        x
        )
        +
        ε
      
    
    {\displaystyle y=f(x)+\varepsilon }
  
, where the noise, 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, has zero mean and variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
.
We want to find a function 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  
, that approximates the true function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 as well as possible, by means of some learning algorithm based on a training dataset (sample) 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  
. We make "as well as possible" precise by measuring the mean squared error between 
  
    
      
        y
      
    
    {\displaystyle y}
  
 and 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  
: we want 
  
    
      
        (
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle (y-{\hat {f}}(x;D))^{2}}
  
 to be minimal, both for 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  
 and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 contain noise 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
; this means we must be prepared to accept an irreducible error in any function we come up with.
Finding an 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
  
 that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
  
 we select, we can decompose its expected error on an unseen sample 
  
    
      
        x
      
    
    {\displaystyle x}
  
 (i.e. conditional to x) as follows:: 34 : 223 

  
    
      
        
          
            E
          
          
            D
            ,
            ε
          
        
        
          
            [
          
        
        
          
            (
          
        
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
        =
        
          
            (
          
        
        
          Bias
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            
              )
            
          
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} _{D,\varepsilon }{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}
  

where

  
    
      
        
          
            
              
                
                  Bias
                  
                    D
                  
                
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                
                  
                    ]
                  
                
              
              
                
                ≜
                
                  
                    E
                  
                  
                    D
                  
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                −
                f
                (
                x
                )
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  
                    E
                  
                  
                    D
                  
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                
                  
                    ]
                  
                
                
                −
                
                
                  
                    E
                  
                  
                    y
                    
                      |
                    
                    x
                  
                
                
                  
                    [
                  
                
                y
                (
                x
                )
                
                  
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}&\triangleq \mathbb {E} _{D}{\big [}{\hat {f}}(x;D)-f(x){\big ]}\\&=\mathbb {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}\,-\,\mathbb {E} _{y|x}{\big [}y(x){\big ]}\end{aligned}}}
  

and 

  
    
      
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        ≜
        
          
            E
          
          
            D
          
        
        
          
            [
          
        
        
          
            (
          
        
        
          
            E
          
          
            D
          
        
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        ]
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
      
    
    {\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}\triangleq \mathbb {E} _{D}{\Big [}{\big (}\mathbb {E} _{D}[{\hat {f}}(x;D)]-{\hat {f}}(x;D){\big )}^{2}{\Big ]}}
  

and

  
    
      
        
          σ
          
            2
          
        
        =
        
          E
          
            y
          
        
        ⁡
        
          
            [
          
        
        
          
            (
          
        
        y
        −
        
          
            
              
                f
                (
                x
                )
              
              ⏟
            
          
          
            
              E
              
                y
                
                  |
                
                x
              
            
            [
            y
            ]
          
        
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
      
    
    {\displaystyle \sigma ^{2}=\operatorname {E} _{y}{\Big [}{\big (}y-\underbrace {f(x)} _{E_{y|x}[y]}{\big )}^{2}{\Big ]}}
  

The expectation ranges over different choices of the training set 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  
, all sampled from the same joint distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
  
 which can for example be done via bootstrapping.
The three terms represent:

the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 using a learning method for linear models, there will be error in the estimates 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 due to this assumption;
the variance of the learning method, or, intuitively, how much the learning method 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 will move around its mean;
the irreducible error 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
.
Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples.: 34 
The more complex the model 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model "move" more to capture the data points, and hence its variance will be larger.

Derivation
The derivation of the bias–variance decomposition for squared error proceeds as follows. For convenience, we drop the 
  
    
      
        D
      
    
    {\displaystyle D}
  
 subscript in the following lines, such that 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        =
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x;D)={\hat {f}}(x)}
  
.
Let us write the mean-squared error of our model:

  
    
      
        
          
            
              
                
                  MSE
                
              
              
                
                ≜
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                y
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                +
                ε
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
              
              
                
                  since 
                
                y
                ≜
                f
                (
                x
                )
                +
                ε
              
            
            
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
                
                +
                
                2
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    )
                  
                
                ε
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                [
                
                  ε
                  
                    2
                  
                
                ]
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{MSE}}&\triangleq \mathbb {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}\\&=\mathbb {E} {\Big [}{\big (}f(x)+\varepsilon -{\hat {f}}(x){\big )}^{2}{\Big ]}&&{\text{since }}y\triangleq f(x)+\varepsilon \\&=\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}^{2}{\Big ]}\,+\,2\ \mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}\varepsilon {\Big ]}\,+\,\mathbb {E} [\varepsilon ^{2}]\end{aligned}}}
  

We can show that the second term of this equation is null:

  
    
      
        
          
            
              
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    )
                  
                
                ε
                
                  
                    ]
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                 
                
                  E
                
                
                  
                    [
                  
                
                ε
                
                  
                    ]
                  
                
              
              
              
                
                  since 
                
                ε
                
                   is independent from 
                
                x
              
            
            
              
              
                
                =
                0
              
              
              
                
                  since 
                
                
                  E
                
                
                  
                    [
                  
                
                ε
                
                  
                    ]
                  
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}\varepsilon {\Big ]}&=\mathbb {E} {\big [}f(x)-{\hat {f}}(x){\big ]}\ \mathbb {E} {\big [}\varepsilon {\big ]}&&{\text{since }}\varepsilon {\text{ is independent from }}x\\&=0&&{\text{since }}\mathbb {E} {\big [}\varepsilon {\big ]}=0\end{aligned}}}
  

Moreover, the third term of this equation is nothing but 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
, the standard deviation of 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
.
Let us now expand the remaining term:

  
    
      
        
          
            
              
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        
                          )
                        
                      
                      
                        2
                      
                    
                    
                      
                        ]
                      
                    
                  
                
                
                +
                
                2
                 
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        )
                      
                    
                    
                      
                        (
                      
                    
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    −
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        )
                      
                    
                    
                      
                        ]
                      
                    
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}^{2}{\Big ]}&=\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}+\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}\\&={\color {Blue}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}^{2}{\Big ]}}\,+\,2\ {\color {PineGreen}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}{\Big ]}}\,+\,\mathbb {E} {\Big [}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}\end{aligned}}}
  

We show that:

  
    
      
        
          
            
              
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        
                          )
                        
                      
                      
                        2
                      
                    
                    
                      
                        ]
                      
                    
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
                
                −
                
                2
                 
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                f
                (
                x
                
                  )
                  
                    2
                  
                
                
                −
                
                2
                 
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\color {Blue}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}^{2}{\Big ]}}&=\mathbb {E} {\big [}f(x)^{2}{\big ]}\,-\,2\ \mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}\,+\,\mathbb {E} {\Big [}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}{\Big ]}\\&=f(x)^{2}\,-\,2\ f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,+\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\\&={\Big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}\end{aligned}}}
  

This last series of equalities comes from the fact that 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 is not a random variable, but a fixed, deterministic function of 
  
    
      
        x
      
    
    {\displaystyle x}
  
. Therefore, 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        )
        
          
            ]
          
        
        =
        f
        (
        x
        )
      
    
    {\displaystyle \mathbb {E} {\big [}f(x){\big ]}=f(x)}
  
. Similarly 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        
          )
          
            2
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        
          )
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} {\big [}f(x)^{2}{\big ]}=f(x)^{2}}
  
, and 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
      
    
    {\displaystyle \mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}=f(x)\ \mathbb {E} {\Big [}\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}=f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}}
  
. Using the same reasoning, we can expand the second term and show that it is null:

  
    
      
        
          
            
              
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        )
                      
                    
                    
                      
                        (
                      
                    
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    −
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        )
                      
                    
                    
                      
                        ]
                      
                    
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                f
                (
                x
                )
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                −
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                 
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
              
            
            
              
              
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\color {PineGreen}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}{\Big ]}}&=\mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,f(x){\hat {f}}(x)\,-\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}+\mathbb {E} {\big [}{\hat {f}}(x){\big ]}\ {\hat {f}}(x){\Big ]}\\&=f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\,+\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\\&=0\end{aligned}}}
  

Eventually, we plug our derivations back into the original equation, and identify each term:

  
    
      
        
          
            
              
                
                  MSE
                
              
              
                
                =
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
                +
                
                  σ
                  
                    2
                  
                
              
            
            
              
              
                
                =
                Bias
                ⁡
                
                  
                    (
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                +
                
                Var
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                +
                
                
                  σ
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{MSE}}&={\Big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}+\mathbb {E} {\Big [}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}+\sigma ^{2}\\&=\operatorname {Bias} {\big (}{\hat {f}}(x){\big )}^{2}\,+\,\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}\,+\,\sigma ^{2}\end{aligned}}}
  

Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over 
  
    
      
        x
        ∼
        P
      
    
    {\displaystyle x\sim P}
  
:

  
    
      
        
          MSE
        
        =
        
          
            E
          
          
            x
          
        
        
          
            {
          
        
        
          Bias
          
            D
          
        
        ⁡
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          ]
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            }
          
        
        +
        
          σ
          
            2
          
        
        .
      
    
    {\displaystyle {\text{MSE}}=\mathbb {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}.}

Approaches
Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,

linear and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.
In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.
In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).
In instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.
In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.: 307 
One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines "strong" learners in a way that reduces their variance.
Model validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off.

k-nearest neighbors
In the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:: 37, 223 

  
    
      
        
          E
        
        
          [
          
            (
            y
            −
            
              
                
                  f
                  ^
                
              
            
            (
            x
            )
            
              )
              
                2
              
            
            ∣
            X
            =
            x
          
          ]
        
        =
        
          
            (
            
              f
              (
              x
              )
              −
              
                
                  1
                  k
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  k
                
              
              f
              (
              
                N
                
                  i
                
              
              (
              x
              )
              )
            
            )
          
          
            2
          
        
        +
        
          
            
              σ
              
                2
              
            
            k
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} \left[(y-{\hat {f}}(x))^{2}\mid X=x\right]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}
  

where 
  
    
      
        
          N
          
            1
          
        
        (
        x
        )
        ,
        …
        ,
        
          N
          
            k
          
        
        (
        x
        )
      
    
    {\displaystyle N_{1}(x),\dots ,N_{k}(x)}
  
 are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under "reasonable assumptions" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.

Applications
In regression
The bias–variance decomposition forms the conceptual basis for regression regularization methods such as LASSO and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.

In classification
The bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition, with the caveat that the variance term becomes dependent on the target label. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected cross-entropy can instead be decomposed to give bias and variance terms with the same semantics but taking a different form.
It has been argued that as training data increases, the variance of learned models will tend to decrease, and hence that as training data quantity increases, error is minimised by methods that learn models with lesser bias, and that conversely, for smaller training data quantities it is ever more important to minimise variance.

In reinforcement learning
Even though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.

In human learning
While widely discussed in the context of machine learning, the bias–variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterized training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalizability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.
Geman et al. argue that the bias–variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of "hard wiring" that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.

See also
References
External links
MLU-Explain: The Bias Variance Tradeoff — An interactive visualization of the bias-variance tradeoff in LOESS Regression and K-Nearest Neighbors.
In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:

The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).
The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.

Motivation
The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data.
It is an often made fallacy to assume that complex models must have high variance. High variance models are "complex" in some sense, but the reverse needs not be true. In addition, one has to be careful how to define complexity. In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from: The model 
  
    
      
        
          f
          
            a
            ,
            b
          
        
        (
        x
        )
        =
        a
        sin
        ⁡
        (
        b
        x
        )
      
    
    {\displaystyle f_{a,b}(x)=a\sin(bx)}
  
 has only two parameters (
  
    
      
        a
        ,
        b
      
    
    {\displaystyle a,b}
  
) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance.
An analogy can be made to the relationship between accuracy and precision. Accuracy is a description of bias and can intuitively be improved by selecting from only local information. Consequently, a sample will appear accurate (i.e. have low bias) under the aforementioned selection conditions, but may result in underfitting. In other words, test data may not agree as closely with training data, which would indicate imprecision and therefore inflated variance. A graphical example would be a straight line fit to data exhibiting quadratic behavior overall. Precision is a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample space is the ideal condition for any analysis. However, intrinsic constraints (whether physical, theoretical, computational, etc.) will always play a limiting role. The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting). This means that test data would also not agree as closely with the training data, but in this case the reason is inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage.

Bias–variance decomposition of mean squared error
Suppose that we have a training set consisting of a set of points 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  
 and real values 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 associated with each point 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  
. We assume that the data is generated by a function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 such as 
  
    
      
        y
        =
        f
        (
        x
        )
        +
        ε
      
    
    {\displaystyle y=f(x)+\varepsilon }
  
, where the noise, 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, has zero mean and variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
.
We want to find a function 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  
, that approximates the true function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 as well as possible, by means of some learning algorithm based on a training dataset (sample) 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  
. We make "as well as possible" precise by measuring the mean squared error between 
  
    
      
        y
      
    
    {\displaystyle y}
  
 and 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  
: we want 
  
    
      
        (
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle (y-{\hat {f}}(x;D))^{2}}
  
 to be minimal, both for 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  
 and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
 contain noise 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
; this means we must be prepared to accept an irreducible error in any function we come up with.
Finding an 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
  
 that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
  
 we select, we can decompose its expected error on an unseen sample 
  
    
      
        x
      
    
    {\displaystyle x}
  
 (i.e. conditional to x) as follows:: 34 : 223 

  
    
      
        
          
            E
          
          
            D
            ,
            ε
          
        
        
          
            [
          
        
        
          
            (
          
        
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
        =
        
          
            (
          
        
        
          Bias
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            
              )
            
          
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} _{D,\varepsilon }{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}
  

where

  
    
      
        
          
            
              
                
                  Bias
                  
                    D
                  
                
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                
                  
                    ]
                  
                
              
              
                
                ≜
                
                  
                    E
                  
                  
                    D
                  
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                −
                f
                (
                x
                )
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  
                    E
                  
                  
                    D
                  
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                ;
                D
                )
                
                  
                    ]
                  
                
                
                −
                
                
                  
                    E
                  
                  
                    y
                    
                      |
                    
                    x
                  
                
                
                  
                    [
                  
                
                y
                (
                x
                )
                
                  
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}&\triangleq \mathbb {E} _{D}{\big [}{\hat {f}}(x;D)-f(x){\big ]}\\&=\mathbb {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}\,-\,\mathbb {E} _{y|x}{\big [}y(x){\big ]}\end{aligned}}}
  

and 

  
    
      
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        ≜
        
          
            E
          
          
            D
          
        
        
          
            [
          
        
        
          
            (
          
        
        
          
            E
          
          
            D
          
        
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        ]
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
      
    
    {\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}\triangleq \mathbb {E} _{D}{\Big [}{\big (}\mathbb {E} _{D}[{\hat {f}}(x;D)]-{\hat {f}}(x;D){\big )}^{2}{\Big ]}}
  

and

  
    
      
        
          σ
          
            2
          
        
        =
        
          E
          
            y
          
        
        ⁡
        
          
            [
          
        
        
          
            (
          
        
        y
        −
        
          
            
              
                f
                (
                x
                )
              
              ⏟
            
          
          
            
              E
              
                y
                
                  |
                
                x
              
            
            [
            y
            ]
          
        
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
      
    
    {\displaystyle \sigma ^{2}=\operatorname {E} _{y}{\Big [}{\big (}y-\underbrace {f(x)} _{E_{y|x}[y]}{\big )}^{2}{\Big ]}}
  

The expectation ranges over different choices of the training set 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  
, all sampled from the same joint distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
  
 which can for example be done via bootstrapping.
The three terms represent:

the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 using a learning method for linear models, there will be error in the estimates 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 due to this assumption;
the variance of the learning method, or, intuitively, how much the learning method 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 will move around its mean;
the irreducible error 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
.
Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples.: 34 
The more complex the model 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
  
 is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model "move" more to capture the data points, and hence its variance will be larger.

Derivation
The derivation of the bias–variance decomposition for squared error proceeds as follows. For convenience, we drop the 
  
    
      
        D
      
    
    {\displaystyle D}
  
 subscript in the following lines, such that 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        =
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x;D)={\hat {f}}(x)}
  
.
Let us write the mean-squared error of our model:

  
    
      
        
          
            
              
                
                  MSE
                
              
              
                
                ≜
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                y
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                +
                ε
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
              
              
                
                  since 
                
                y
                ≜
                f
                (
                x
                )
                +
                ε
              
            
            
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
                
                +
                
                2
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    )
                  
                
                ε
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                [
                
                  ε
                  
                    2
                  
                
                ]
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{MSE}}&\triangleq \mathbb {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}\\&=\mathbb {E} {\Big [}{\big (}f(x)+\varepsilon -{\hat {f}}(x){\big )}^{2}{\Big ]}&&{\text{since }}y\triangleq f(x)+\varepsilon \\&=\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}^{2}{\Big ]}\,+\,2\ \mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}\varepsilon {\Big ]}\,+\,\mathbb {E} [\varepsilon ^{2}]\end{aligned}}}
  

We can show that the second term of this equation is null:

  
    
      
        
          
            
              
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    )
                  
                
                ε
                
                  
                    ]
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                 
                
                  E
                
                
                  
                    [
                  
                
                ε
                
                  
                    ]
                  
                
              
              
              
                
                  since 
                
                ε
                
                   is independent from 
                
                x
              
            
            
              
              
                
                =
                0
              
              
              
                
                  since 
                
                
                  E
                
                
                  
                    [
                  
                
                ε
                
                  
                    ]
                  
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}\varepsilon {\Big ]}&=\mathbb {E} {\big [}f(x)-{\hat {f}}(x){\big ]}\ \mathbb {E} {\big [}\varepsilon {\big ]}&&{\text{since }}\varepsilon {\text{ is independent from }}x\\&=0&&{\text{since }}\mathbb {E} {\big [}\varepsilon {\big ]}=0\end{aligned}}}
  

Moreover, the third term of this equation is nothing but 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
, the standard deviation of 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
.
Let us now expand the remaining term:

  
    
      
        
          
            
              
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        
                          )
                        
                      
                      
                        2
                      
                    
                    
                      
                        ]
                      
                    
                  
                
                
                +
                
                2
                 
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        )
                      
                    
                    
                      
                        (
                      
                    
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    −
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        )
                      
                    
                    
                      
                        ]
                      
                    
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}(x){\big )}^{2}{\Big ]}&=\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}+\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}\\&={\color {Blue}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}^{2}{\Big ]}}\,+\,2\ {\color {PineGreen}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}{\Big ]}}\,+\,\mathbb {E} {\Big [}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}\end{aligned}}}
  

We show that:

  
    
      
        
          
            
              
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        
                          )
                        
                      
                      
                        2
                      
                    
                    
                      
                        ]
                      
                    
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
                
                −
                
                2
                 
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                f
                (
                x
                
                  )
                  
                    2
                  
                
                
                −
                
                2
                 
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\color {Blue}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}^{2}{\Big ]}}&=\mathbb {E} {\big [}f(x)^{2}{\big ]}\,-\,2\ \mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}\,+\,\mathbb {E} {\Big [}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}{\Big ]}\\&=f(x)^{2}\,-\,2\ f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,+\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\\&={\Big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}\end{aligned}}}
  

This last series of equalities comes from the fact that 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
  
 is not a random variable, but a fixed, deterministic function of 
  
    
      
        x
      
    
    {\displaystyle x}
  
. Therefore, 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        )
        
          
            ]
          
        
        =
        f
        (
        x
        )
      
    
    {\displaystyle \mathbb {E} {\big [}f(x){\big ]}=f(x)}
  
. Similarly 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        
          )
          
            2
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        
          )
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} {\big [}f(x)^{2}{\big ]}=f(x)^{2}}
  
, and 
  
    
      
        
          E
        
        
          
            [
          
        
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
        
          
            ]
          
        
        =
        f
        (
        x
        )
         
        
          E
        
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          
            ]
          
        
      
    
    {\displaystyle \mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}=f(x)\ \mathbb {E} {\Big [}\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big ]}=f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}}
  
. Using the same reasoning, we can expand the second term and show that it is null:

  
    
      
        
          
            
              
                
                  
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        (
                      
                    
                    f
                    (
                    x
                    )
                    −
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    
                      
                        )
                      
                    
                    
                      
                        (
                      
                    
                    
                      E
                    
                    
                      
                        [
                      
                    
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        ]
                      
                    
                    −
                    
                      
                        
                          f
                          ^
                        
                      
                    
                    (
                    x
                    )
                    
                      
                        )
                      
                    
                    
                      
                        ]
                      
                    
                  
                
              
              
                
                =
                
                  E
                
                
                  
                    [
                  
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                f
                (
                x
                )
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                −
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                 
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                f
                (
                x
                )
                 
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                −
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
                
                +
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      ]
                    
                  
                  
                    2
                  
                
              
            
            
              
              
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\color {PineGreen}\mathbb {E} {\Big [}{\big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\big )}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}{\Big ]}}&=\mathbb {E} {\Big [}f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,f(x){\hat {f}}(x)\,-\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}+\mathbb {E} {\big [}{\hat {f}}(x){\big ]}\ {\hat {f}}(x){\Big ]}\\&=f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,f(x)\ \mathbb {E} {\big [}{\hat {f}}(x){\big ]}\,-\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\,+\,\mathbb {E} {\big [}{\hat {f}}(x){\big ]}^{2}\\&=0\end{aligned}}}
  

Eventually, we plug our derivations back into the original equation, and identify each term:

  
    
      
        
          
            
              
                
                  MSE
                
              
              
                
                =
                
                  
                    (
                  
                
                f
                (
                x
                )
                −
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                +
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    (
                  
                
                
                  E
                
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                  
                    ]
                  
                
                +
                
                  σ
                  
                    2
                  
                
              
            
            
              
              
                
                =
                Bias
                ⁡
                
                  
                    (
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    
                      )
                    
                  
                  
                    2
                  
                
                
                +
                
                Var
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                (
                x
                )
                
                  
                    ]
                  
                
                
                +
                
                
                  σ
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{MSE}}&={\Big (}f(x)-\mathbb {E} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}+\mathbb {E} {\Big [}{\big (}\mathbb {E} {\big [}{\hat {f}}(x){\big ]}-{\hat {f}}(x){\big )}^{2}{\Big ]}+\sigma ^{2}\\&=\operatorname {Bias} {\big (}{\hat {f}}(x){\big )}^{2}\,+\,\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}\,+\,\sigma ^{2}\end{aligned}}}
  

Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over 
  
    
      
        x
        ∼
        P
      
    
    {\displaystyle x\sim P}
  
:

  
    
      
        
          MSE
        
        =
        
          
            E
          
          
            x
          
        
        
          
            {
          
        
        
          Bias
          
            D
          
        
        ⁡
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          ]
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            }
          
        
        +
        
          σ
          
            2
          
        
        .
      
    
    {\displaystyle {\text{MSE}}=\mathbb {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}.}

Approaches
Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,

linear and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.
In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.
In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).
In instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.
In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.: 307 
One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines "strong" learners in a way that reduces their variance.
Model validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off.

k-nearest neighbors
In the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:: 37, 223 

  
    
      
        
          E
        
        
          [
          
            (
            y
            −
            
              
                
                  f
                  ^
                
              
            
            (
            x
            )
            
              )
              
                2
              
            
            ∣
            X
            =
            x
          
          ]
        
        =
        
          
            (
            
              f
              (
              x
              )
              −
              
                
                  1
                  k
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  k
                
              
              f
              (
              
                N
                
                  i
                
              
              (
              x
              )
              )
            
            )
          
          
            2
          
        
        +
        
          
            
              σ
              
                2
              
            
            k
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} \left[(y-{\hat {f}}(x))^{2}\mid X=x\right]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}
  

where 
  
    
      
        
          N
          
            1
          
        
        (
        x
        )
        ,
        …
        ,
        
          N
          
            k
          
        
        (
        x
        )
      
    
    {\displaystyle N_{1}(x),\dots ,N_{k}(x)}
  
 are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under "reasonable assumptions" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.

Applications
In regression
The bias–variance decomposition forms the conceptual basis for regression regularization methods such as LASSO and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.

In classification
The bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition, with the caveat that the variance term becomes dependent on the target label. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected cross-entropy can instead be decomposed to give bias and variance terms with the same semantics but taking a different form.
It has been argued that as training data increases, the variance of learned models will tend to decrease, and hence that as training data quantity increases, error is minimised by methods that learn models with lesser bias, and that conversely, for smaller training data quantities it is ever more important to minimise variance.

In reinforcement learning
Even though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.

In human learning
While widely discussed in the context of machine learning, the bias–variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterized training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalizability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.
Geman et al. argue that the bias–variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of "hard wiring" that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.

See also
References
External links
MLU-Explain: The Bias Variance Tradeoff — An interactive visualization of the bias-variance tradeoff in LOESS Regression and K-Nearest Neighbors.
The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.

Adoption
The Bibliographic Reference Code (refcode) was originally developed to be used in SIMBAD and the NASA/IPAC Extragalactic Database (NED), but it became a de facto standard and is now used more widely, for example, by the NASA Astrophysics Data System, which coined and prefers the term "bibcode".

Format
The code has a fixed length of 19 characters and has the form

YYYYJJJJJVVVVMPPPPA
where YYYY is the four-digit year of the reference and JJJJJ is a code indicating where the reference was published. In the case of a journal reference, VVVV is the volume number, M indicates the section of the journal where the reference was published (e.g., L for a letters section), PPPP gives the starting page number, and A is the first letter of the last name of the first author. Periods (.) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number. Page numbers greater than 9999 are continued in the M column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a, etc.) and inserted into column M. The remaining four digits are used in the page field.

Examples
Some examples of bibcodes are:

See also
Digital object identifier


== References ==
Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.
Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.
Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. "There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem."
Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.
The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.
Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require "massively parallel software running on tens, hundreds, or even thousands of servers". What qualifies as "big data" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."

Definition
The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.  Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.  Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data "size" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.  Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.
"Volume", "variety", "velocity", and various other "Vs" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the "three Vs", "four Vs", and "five Vs". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.
A 2018 definition states "Big data is where parallel computing tools are needed to handle data", and notes, "This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model."
In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.

Big data vs. business intelligence
The growing maturity of the concept more starkly delineates the difference between "big data" and "business intelligence":

Business intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.
Big data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.

Characteristics
Big data can be described by the following characteristics:

Volume
The quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.
Variety
The type and nature of the data. Earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. Big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.
Velocity
The speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.
Veracity
The truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.
Value
The worth in information that can be achieved by the processing and analysis of large datasets. Value also can be measured by an assessment of the other qualities of big data. Value may also represent the profitability of information that is retrieved from the analysis of big data.
Variability
The characteristic of the changing formats, structure, or sources of big data. Big data can include structured, unstructured, or combinations of structured and unstructured data. Big data analysis may integrate raw data from multiple sources. The processing of raw data may also involve transformations of unstructured data to structured data.
Other possible characteristics of big data are:

Exhaustive
Whether the entire system (i.e., 
  
    
      
        n
      
    
    {\textstyle n}
  
=all) is captured or recorded or not. Big data may or may not include all the available data from sources.
Fine-grained and uniquely lexical
Respectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.
Relational
If the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.
Extensional
If new fields in each element of the data collected can be added or changed easily.
Scalability
If the size of the big data storage system can expand rapidly.

Architecture
Big data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.
Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.
In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.
CERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current "big data" movement.
In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the "map" step). The results are then gathered and delivered (the "reduce" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named "Hadoop". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).
MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.
Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.
The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.

Technologies
A 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:

Techniques for analyzing data, such as A/B testing, machine learning, and natural language processing
Big data technologies, like business intelligence, cloud computing, and databases
Visualization, such as charts, graphs, and other displays of the data
Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.
Additional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.
Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.
DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called "Ayasdi".
The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.

Applications
Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.
Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).
While many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.

Government
The use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.
Civil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.

International development
Research on the effective usage of information and communication technologies for development (also known as "ICT4D") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.  The challenge of "big data for development" is currently evolving toward the application of this data through machine learning, known as "artificial intelligence for development (AI4D).

Benefits
A major practical application of big data for development has been "fighting poverty with data". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:

Thematic coverage: including areas that were previously difficult or impossible to measure
Geographical coverage: providing sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories
Level of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections
Timeliness and timeseries: graphs can be produced within days of being collected

Challenges
At the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:

Representativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.
Generalizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.
Harmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called "data-fusion", the harmonization of different sources.
Data overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively deal with data.

Finance
Big Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions. The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large. Big Data has also been a typical concept within the field of alternative financial service. Some of the major areas involve crowd-funding platforms and crypto currency exchanges.

Healthcare
Big data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. "Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth." Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.
Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.
A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.

Education
A McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including paid programs like The Data Incubator or General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,
product development, branding) that all use different types of data.

Media
To understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.

Targeting of consumers (for advertising by marketers)
Data capture
Data journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.
Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.

Insurance
Health insurance providers are collecting data on social "determinants of health" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.

Internet of things (IoT)
Big data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.
Kevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: "If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best."

Information technology
Especially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.

Survey science
Compared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality. There have been three Big Data Meets Survey Science (BigSurv) conferences in 2018, 2020 (virtual), 2023, and as of 2023 one conference forthcoming in 2025, a special issue in the Social Science Computer Review, a special issue in Journal of the Royal Statistical Society, and a special issue in EP J Data Science, and a book called Big Data Meets Social Sciences edited by Craig Hill and five other Fellows of the American Statistical Association. In 2021, the founding members of BigSurv received the Warren J. Mitofsky Innovators Award from the American Association for Public Opinion Research.

Marketing
Big data is notable in marketing due to the constant “datafication” of everyday consumers of the internet, in which all forms of data are tracked. The datafication of consumers can be defined as  quantifying many of or all human behaviors for the purpose of marketing. The increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially. It is predicted to increase from 44 to 163 zettabytes within the span of five years. The size of big data can often be difficult to navigate for marketers. As a result, adopters of big data may find themselves at a disadvantage. Algorithmic findings can be difficult to achieve with such large datasets. Big data in marketing is a highly lucrative tool that can be used for large corporations, its value being as a result of the possibility of predicting significant trends, interests, or statistical outcomes in a consumer-based manner.
There are three significant factors in the use of big data in marketing:

Big data provides customer behavior pattern spotting for marketers, since all human actions are being quantified into readable numbers for marketers to analyze and use for their research. In addition, big data can also be seen as a customized product recommendation tool. Specifically, since big data is effective in analyzing customers’ purchase behaviors and browsing patterns, this technology can assist companies in promoting specific personalized products to specific customers.
Real-time market responsiveness is important for marketers because of the ability to shift marketing efforts and correct to current trends, which is helpful in maintaining relevance to consumers. This can supply corporations with the information necessary to predict the wants and needs of consumers in advance.
Data-driven market ambidexterity are being highly fueled by big data. New models and algorithms are being developed to make significant predictions about certain economic and social situations.

Case studies
Government
China
The Integrated Joint Operations Platform (IJOP, 一体化联合作战平台) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.
By 2020, China plans to give all its citizens a personal "social credit" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.

India
Big data analysis was tried out for the BJP to win the 2014 Indian General Election.
The Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.

Israel
Personalized diabetic treatments can be created through GlucoMe's big data solution.

United Kingdom
Examples of uses of big data in public services:

Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify and examine the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.
Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as Meals on Wheels. The connection of data allowed the local authority to avoid any weather-related delay.

United States
In 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.
Big data analysis played a large role in Barack Obama's successful 2012 re-election campaign.
The United States Federal Government owns four of the ten most powerful supercomputers in the world.
The Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.

Retail
Walmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.
Windermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.
FICO Card Detection System protects accounts worldwide.
Omnichannel retailing leverages online big data to improve offline experiences.

Science
The Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.
As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world.
The Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.
When the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.
Decoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times less expensive than the reduction in cost predicted by Moore's law.
The NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.
Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any "friction points", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.
23andme's DNA database contains the genetic information of over 1,000,000 people worldwide. The company explores selling the "anonymous aggregated genetic data" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.
Computational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using "virtual sensors" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.

Sports
Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.
Future performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.
In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.

Technology
As of 2013, eBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.
Amazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.
Facebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.
Google was handling roughly 100 billion searches per month as of August 2012.

COVID-19
During the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.
Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.

Research activities
Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.
In March 2012, The White House announced a national "Big Data Initiative" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.
The initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.
The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.
The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.
The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.
The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.
At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.
Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the "future orientation index". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.
Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.
Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.

Sampling big data
A research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.

Critique
Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.

Critiques of the big data paradigm
"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data."  In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by "big judgment", according to an article in the Harvard Business Review.
Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use "abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge".  Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.
In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.  A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", C. D. Broad, 1926) are to be considered.
Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.
Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.

Critiques of the "V" model
The "V" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:

Data completeness: understanding of the non-obvious from data
Data correlation, causation, and predictability: causality as not essential requirement to achieve predictability
Explainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this
Level of automated decision-making: algorithms that support automated decision making and algorithmic self-learning

Critiques of novelty
Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial "big data". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.

Critiques of big data execution
Ulf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a "fad" in scientific research. Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have a bias in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.
In the provocative article "Critical Questions for Big Data", the authors title big data a part of mythology: "large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy". Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth". Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers "speak for themselves" and revolutionize scientific method, is questioned. Catherine Tucker has pointed to "hype" around big data, writing "By itself, big data is unlikely to be valuable." The article explains: "The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm."
Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.
Big data is a buzzword and a "vague term", but at the same time an "obsession" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that "most published research findings are false" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a "significant" result being false grows fast – even more so, when only positive results are published.
Furthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election with varying degrees of success.

Critiques of big data policing and surveillance
Big data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:

Placing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm
Increasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system
Encouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusion
If these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.

See also
References
Bibliography
Hilbert, M (2016), "Big Data for Development: A Review of Promises and Challenges", Development Policy Review, 34 (1): 135–74, doi:10.1111/dpr.12142; free access, Archived 21 April 2021 at the Wayback Machine
Snijders, C.; Matzat, U.; Reips, U.-D. (2012). "'Big Data': Big gaps of knowledge in the field of Internet". International Journal of Internet Science. 7: 1–5. Archived from the original on 23 November 2019. Retrieved 13 April 2013.
Yanase, J; Triantaphyllou, E (2019). "A Systematic Survey of Computer-Aided Diagnosis in Medicine: Past and Present Developments". Expert Systems with Applications. 138: 112821. doi:10.1016/j.eswa.2019.112821. S2CID 199019309.

Further reading
Peter Kinnaird; Inbal Talgam-Cohen, eds. (2012). "Big Data". XRDS: Crossroads, The ACM Magazine for Students. Vol. 19, no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.
Jure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 978-1-10707723-2. OCLC 888463433.
Viktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 978-1-29990302-9. OCLC 828620988.
Press, Gil (9 May 2013). "A Very Short History of Big Data". forbes.com. Jersey City, NJ. Retrieved 17 September 2016.
Stephens-Davidowitz, Seth (2017). Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are. Dey Street Books. ISBN 978-0-06239085-1.
"Big Data: The Management Revolution". Harvard Business Review. October 2012.
O'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0-55341883-5.

External links
 Media related to Big data at Wikimedia Commons
 The dictionary definition of big data at Wiktionary
Binary classification is the task of classifying the elements of a set into one of two groups (each called class). Typical binary classification problems include:

Medical testing to determine if a patient has certain disease or not;
Quality control in industry, deciding whether a specification has been met;
In information retrieval, deciding whether a page should be in the result set of a search or not
In administration, deciding whether someone should be issued with a driving licence or not
In cognition, deciding whether an object is food or not food.
When measuring the accuracy of a binary classifier, the simplest way is to count the errors. But in the real world often one of the two classes is more important, so that the number of both of the different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).

Four outcomes
Given a classification of a specific data set, there are four basic combinations of actual data category and assigned category: true positives TP (correct positive assignments), true negatives TN (correct negative assignments), false positives FP (incorrect positive assignments), and false negatives FN (incorrect negative assignments).

These can be arranged into a 2×2 contingency table, with rows corresponding to actual value – condition positive or condition negative – and columns corresponding to classification value – test outcome positive or test outcome negative.

Evaluation
From tallies of the four basic outcomes, there are many approaches that can be used to measure the accuracy of a classifier or predictor. Different fields have different preferences.

The eight basic ratios
A common approach to evaluation is to begin by computing two ratios of a standard pattern. There are eight basic ratios of this form that one can compute from the contingency table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form "true positive row ratio" or "false negative column ratio". 
There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.
The row ratios are:

true positive rate (TPR) = (TP/(TP+FN)), aka sensitivity or recall.  These are the proportion of the population with the condition for which the test is correct.
with complement the false negative rate (FNR) = (FN/(TP+FN))
true negative rate (TNR) = (TN/(TN+FP), aka specificity (SPC),
with complement false positive rate (FPR) = (FP/(TN+FP)), also called independent of prevalence
The column ratios are:

positive predictive value (PPV, aka precision) (TP/(TP+FP)).  These are the proportion of the population with a given test result for which the test is correct.
with complement the false discovery rate (FDR) (FP/(TP+FP))
negative predictive value (NPV) (TN/(TN+FN))
with complement the false omission rate (FOR) (FN/(TN+FN)), also called dependence on prevalence.
In diagnostic testing, the main ratios used are the true column ratios – true positive rate and true negative rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – positive predictive value and true positive rate – where they are known as precision and recall. 
Cullerne Bown has suggested a flow chart for determining which pair of indicators should be used when. Otherwise, there is no general rule for deciding. There is also no general agreement on how the pair of indicators should be used to decide on concrete questions, such as when to prefer one classifier over another.
One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent.

Other metrics
There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden's J statistic, the uncertainty coefficient, the phi coefficient, and Cohen's kappa.

Statistical binary classification
Statistical classification is a problem studied in machine learning in which the classification is performed on the basis of a classification rule.  It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories.  When there are only two categories the problem is known as statistical binary classification.
Some of the methods commonly used for binary classification are:

Decision trees
Random forests
Bayesian networks
Support vector machines
Neural networks
Logistic regression
Probit model
Genetic Programming
Multi expression programming
Linear genetic programming
Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example, random forests perform better than SVM classifiers for 3D point clouds.

Converting continuous values to binary
Binary classification may be a form of dichotomization in which a continuous function is transformed into a binary variable. Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff.
However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as "positive" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as "positive" as the one of 52 mIU/ml.

See also
Approximate membership query filter
Examples of Bayesian inference
Classification rule
Confusion matrix
Detection theory
Kernel methods
Multiclass classification
Multi-label classification
One-class classification
Prosecutor's fallacy
Receiver operating characteristic
Thresholding (image processing)
Uncertainty coefficient, aka proficiency
Qualitative property
Precision and recall (equivalent classification schema)

References
Bibliography
Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book)
John Shawe-Taylor and Nello Cristianini.  Kernel Methods for Pattern Analysis.  Cambridge University Press, 2004.  ISBN 0-521-81397-2 (Website for the book)
Bernhard Schölkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, Massachusetts, 2002. ISBN 0-262-19475-9