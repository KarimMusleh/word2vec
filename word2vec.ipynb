{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74259fe-0dc6-4b71-89fc-6ad092158fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import torch\n",
    "from torch import optim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "from random import uniform\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd40b81f-7bd8-4a07-a37a-2946970a36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset):\n",
    "    # I may have filtered too hard\n",
    "    # I removed everything between braces because that's just LateX I tried using regex but couldn't get it to work\n",
    "    # Then I removed empty lines and lines that don't start with an upper case (removing this results in a bunch of random letters in th\n",
    "    filtered_dataset = dataset\n",
    "    # print(f'The dataset is size {len(filtered_dataset)} without filtering')\n",
    "    # with open('latex.txt', 'w') as w: # use this to test wether it removes too much or too little\n",
    "    #     w.write(''.join(re.findall(r' {6}\\n {8}.*?(?:\\\\displaystyle|\\\\textstyle).*?\\n', filtered_dataset, flags=re.DOTALL)))\n",
    "    filtered_dataset = re.sub(r' {8}.*?(?:\\\\displaystyle|\\\\textstyle).*?\\n', '', filtered_dataset, flags=re.DOTALL) # We lowercase the d\n",
    "    # print(f'The dataset is size {len(filtered_dataset)} without the LaTeX')\n",
    "    # print(f\"There are currently {len(re.findall(r'displaystyle', filtered_dataset))} LaTeX blocks that have to be manually deleted\")\n",
    "    # filtered_dataset = '\\n'.join([line for line in filtered_dataset.splitlines() if line.strip()]) #  and line[0].isupper() and len(li\n",
    "    # print(len(filter_dataset))\n",
    "    filtered_dataset = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', filtered_dataset, re.DOTALL)\n",
    "    # print(f'The dataset is size {len(filtered_dataset)} without the links')\n",
    "    # I found the regex above here https://regexr.com/37i6s\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc12ace9-c661-4587-8420-8869451f5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str, force_filter=False):\n",
    "    from pathlib import Path\n",
    "    if Path('filtered_' + name).exists() and not force_filter:\n",
    "        return open('filtered_'+name, 'r').read()\n",
    "    wikis = open(name, 'r').read().split('__WIKI__')\n",
    "    wikis = [filter_dataset(wiki) for wiki in wikis]\n",
    "    with open('filtered_'+dataset_name, 'w') as o:\n",
    "        o.write('__WIKI__'.join(wikis))\n",
    "    return wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14307a76-a828-4041-ba5d-d61a553b2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'small_dataset.txt' # CHOOSING WIKIPEDIA WAS A MISTAKE t\n",
    "dataset = load_dataset(dataset_name,force_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8cff5c-4c95-4405-98c0-a640f139bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_norm=1):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(self.embed(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bd2b88-315e-4b79-80ae-9e03105beb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize = RegexpTokenizer(r'\\w+').tokenize\n",
    "tokenized_dataset = [[word_tokenize(sent) for sent in sent_tokenize(wiki)] for wiki in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa1b510-30c1-4da0-922b-aef5df7e93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [[[word.lower() for word in sent] for sent in wiki] for wiki in tokenized_dataset]\n",
    "# I should probably do this earlier but as of now I am not sure where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f994354b-e125-449b-abdf-b59538c3a4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7909b848-3ac9-4616-93de-569a1b8d3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "idx = 0\n",
    "for wiki in tokenized_dataset:\n",
    "    for sent in wiki:\n",
    "        for word in sent:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = idx\n",
    "                idx += 1\n",
    "id2word = {word2id[word]: word for word in word2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5a1d29-dbca-47c4-811c-3c7e69aef3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42396\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4706ffad-4d68-4268-af94-75ac589d4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(ids):\n",
    "    return ' '.join([id2word[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00644657-8adf-4c52-bf83-ac0246f0ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [[[word2id[word] for word in sent] for sent in wiki] for wiki in tokenized_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97ced61-10ab-41a0-a3e2-95c71c3a6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for wiki in ids for sent in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64493cf2-0fc2-4ec0-9ff2-0173d57be0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning ml is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "930ff8c1-c112-435a-a286-7f565f296952",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter([word for sent in sents for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d50ea5-d133-4a68-9411-307c34a62a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(sent, freq, k, t=1):\n",
    "    data = []\n",
    "    n = len(sent)\n",
    "    for i in range(n):\n",
    "        word = sent[i]\n",
    "        j = i - 1\n",
    "        while (i - j) <= k and j >= 0:\n",
    "            # if uniform(0, 1) > (1 - (t/freq[word])): # This function is taken from the second paper by Mikolov et al.\n",
    "            # It's defined in 2.3 Subsampling of Frequent Words.\n",
    "            data.append((word, sent[j]))\n",
    "            j -= 1\n",
    "        j = i + 1\n",
    "        while (j - i) <= k and j < n:\n",
    "            # if uniform(0, 1) > (1 - (t/freq[word])):\n",
    "            data.append((word, sent[j]))\n",
    "            j += 1\n",
    "    return data\n",
    "    \n",
    "def create_training_dataset(sents, freq, k=3):\n",
    "    training_dataset = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        data = create_training_data(sent, freq, k)\n",
    "\n",
    "        training_dataset.extend(data)\n",
    "\n",
    "    return torch.tensor(training_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "137f0109-d828-4f21-b8d3-e51422855c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = create_training_dataset(sents, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "854b4e79-9389-483e-b3ab-870130a9b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(training_dataset, batch_size=64, shuffle=True, generator=gen, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62652e5a-0116-4a7b-a845-e369515fc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, loader): # R is the range from which we take the training samples\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        X = data[:, 0].to(device)\n",
    "        y = data[:, 1].to(device)\n",
    "        # print(X.device, y.device)\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, F.one_hot(y, num_classes=model.embed.num_embeddings).type(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807759e6-d7cb-42d0-b9f8-22cfcd1b82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Skipgram(vocab_size, 300, max_norm=1)\n",
    "model.to(device)\n",
    "train_skipgram(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2072a16-7bef-4c33-af6c-4a8756ecf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'small-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
