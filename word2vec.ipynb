{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74259fe-0dc6-4b71-89fc-6ad092158fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from torch import nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd40b81f-7bd8-4a07-a37a-2946970a36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset):\n",
    "    # I may have filtered too hard\n",
    "    # I removed everything between braces because that's just LateX I tried using regex but couldn't get it to work\n",
    "    # Then I removed empty lines and lines that don't start with an upper case (removing this results in a bunch of random letters in the dataset)\n",
    "    filtered_dataset = dataset\n",
    "    print(f'The dataset is size {len(filtered_dataset)} without filtering')\n",
    "    # with open('latex.txt', 'w') as w: # use this to test wether it removes too much or too little\n",
    "    #     w.write(''.join(re.findall(r' {6}\\n {8}.*?(?:\\\\displaystyle|\\\\textstyle).*?\\n', filtered_dataset, flags=re.DOTALL)))\n",
    "    filtered_dataset = re.sub(r' {8}.*?(?:\\\\displaystyle|\\\\textstyle).*?\\n', '', filtered_dataset, flags=re.DOTALL) # We lowercase the data and remove the LateX\n",
    "    print(f'The dataset is size {len(filtered_dataset)} without the LaTeX')\n",
    "    print(f\"There are currently {len(re.findall(r'displaystyle', filtered_dataset))} LaTeX blocks that have to be manually deleted\")\n",
    "    # filtered_dataset = '\\n'.join([line for line in filtered_dataset.splitlines() if line.strip()]) #  and line[0].isupper() and len(line) > 30\n",
    "    # print(len(filter_dataset))\n",
    "    filtered_dataset = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', filtered_dataset)\n",
    "    print(f'The dataset is size {len(filtered_dataset)} without the links')\n",
    "    # I found the regex above here https://regexr.com/37i6s\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc12ace9-c661-4587-8420-8869451f5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str, force_filter=False):\n",
    "    from pathlib import Path\n",
    "    if Path('filtered_' + name).exists() and not force_filter:\n",
    "        return open('filtered_'+name, 'r').read()\n",
    "    wikis = open(name, 'r').read().split('__WIKI__')\n",
    "    filtered_dataset = filter_dataset()\n",
    "    with open('filtered_'+dataset_name, 'w') as o:\n",
    "        o.write(filtered_dataset)\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "14307a76-a828-4041-ba5d-d61a553b2987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is size 19739132 without filtering\n",
      "The dataset is size 15207373 without the LaTeX\n",
      "There are currently 0 LaTeX blocks that weren't deleted\n",
      "The dataset is size 15202688 without the links\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'dataset.txt' # CHOOSING WIKIPEDIA WAS A MISTAKE t\n",
    "dataset = load_dataset(dataset_name,force_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd8cff5c-4c95-4405-98c0-a640f139bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_norm=1):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43bd2b88-315e-4b79-80ae-9e03105beb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['machine',\n",
       "  'learning',\n",
       "  '(',\n",
       "  'ml',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'field',\n",
       "  'of',\n",
       "  'study',\n",
       "  'in',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'concerned',\n",
       "  'with',\n",
       "  'the',\n",
       "  'development',\n",
       "  'and',\n",
       "  'study',\n",
       "  'of',\n",
       "  'statistical',\n",
       "  'algorithms',\n",
       "  'that',\n",
       "  'can',\n",
       "  'learn',\n",
       "  'from',\n",
       "  'data',\n",
       "  'and',\n",
       "  'generalize',\n",
       "  'to',\n",
       "  'unseen',\n",
       "  'data',\n",
       "  ',',\n",
       "  'and',\n",
       "  'thus',\n",
       "  'perform',\n",
       "  'tasks',\n",
       "  'without',\n",
       "  'explicit',\n",
       "  'instructions',\n",
       "  '.']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(dataset)\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "words[:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
